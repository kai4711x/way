/30/10

We need to find battery usage of gps, and check how well compass / time of movement tracking works


Plan:

SilentGps:

Only turn on gps during no movement.. or when it's been a long time.. that is relative no movement. That is 90% no movement..
Allow user to double tap to explicitally store place
Have find mode where if user goes in opposite direction for too far, will buzz distinctly
Will integrate in maps, but not necessary.
If initially when stopped, cannot get gps, will wait until moving to enable gps again.
When place is recorded, will buzz distinctly

When taken out of the pocket and turned on:

1. compass reading of last few seconds will not be averaged in.
2. Compass direction will considered to be the same
3. When shutoff, 5 seconds later, will restart average compass reading, considering 
 an offset as calculated from prior set
 
Gps timing should be done based on how long it takes to measure. So if it only takes
a couple of seconds, we should be doing gps more often. If it takes two minutes and
times out, we should do it less often. We should also vary the amount we try. So
sometimes we try 2 minutes, sometimes only 1, sometimes only 30 seconds.
 
4/9

When figuring out whether the user is in motion or not, we seperate the data out
into 15 second segments. In each of these segments we calculate the standard deviation
compared to the last 5 minutes or xxxxxxxxxxxx

Maybe take 1 / measurements and calculate standard deviation from this. If it's >close enough
to zero, it would fall outside the range of standard deviation.

The only problem is that if it sits for a long time, then it would be very difficult for the
thing to go back to "standing" standard deviation. We'll need a rolling value. I think it should
work. We have data, too... we can check it out.

4/13

By taking some statistics on our phone, the mean is 4 seconds, and after 30 seconds it will probably not find a measurement.

4/15

When I was going around yesterday, during cloud cover, the situation changed. It took a lot longer than 30 seconds to find 
a reading, but it did find one eventually.

We want a continuous measurement. We can't continually turn on and off the gps. 


---

The thing with our current code is that we have a max percentage of gps use. We will spend all of it if we can't get a reading.
So if we've been taking short 4 second readings all day, and then end up in an army tank, we will hopelessly spend all our saved
battery trying to read in that tank. Yes, we're staying under the percentage, but we're also spending battery fruitlessly

Because so, we need another stop gap, which doesn't allow us to take another gps reading if we've not yet been unsuccessful in 
a reading in the last x minutes.


-----------
-
- GPS Start
-
-
-
-

4/16

Argument for stats:
We only record when we got a measurement, not when we started or stopped GPS. This is because it makes it too complicated,
we get little blips after we have gotten a reading. We may get a reading when GPS is "turned off", etc. Too much crappy data
hacking.

So it is still useful for debugging purposes, since we only care what the device returns. What we send should always be the same.

We may need to revisit this to have a better stats recorder. But for now, we can debug. If we want stats, we will take them from
the GPS strategy. Hell, we may just use print outs. I wonder if we could store stuff as bytes immediately after we process it?
Like have a buffer of data that saves to disk in another thread. A self flushing buffer, so to speak.. It seems better than
the current solution.

Just have a circular buffer you can write to.

Overview

GpsTrailerService - handles service events. Creates, starts, stops GpsTrailerManager.. What about pauses?

GpsReader - turns on and off gps service. notifies when a reading was taken
    Provides: gps readings
    Connections: notifies GpsTrailerManager (which in turns distributes the message). Notified by GpsTrailerStrategy
       directly to start and stop gps
GpsTrailerStrategy - determines when to turn gps on and off. Has thread to notify turn off gps on timeout.
    Needs: gps reading notification, movement and stop notification. 
    Provides: gpsReader control
    Connections: notifies gpsreader directly.
GpsDatabaseHandler - writes to the database.
	Needs: when gps was taken, when movement started and stopped
	Connections: notified by GpsTrailerManager
CalmnessDetector - determines when user is moving and is at rest
    Provides: movement and stopped notification
    Connections: polled by GpsTrailerManager
GpsTrailerManager - creates and starts GpsReader, GpsTrailerStrategy, GpsDatabaseHandler, CalmnessDetector
    Connections: has GpsReader. implements GpsReader.Processor. 
           has calmnessDetector.
           Notifies GpsTrailerStrategy and GpsDatabaseHandler of gps readings and calmnessdetector stuff
           Owns process thread
4/18

above g periods:

We are doing boolean above g periods because if we take the straight integral, we
will have a lot slimmer range, because when at rest, there is still g activity and it is quite high (due to error?)
If we tried to get rid of this by subtracting a rolling average, if we were in constant motion for a long time, it
would skew the results.

An alternative may be to take an integral minus a certain level. So if the value is 8 we subtract 5 (constant) and use
3 as the height of our integral. (if below 5 we take it as zero)

4/19

Incomplete thoughts:

There is a difference between moving after stopping and moving in general.
1. You actually changed the situation and now can read gps (or not read it)
2. you are in a bus, car, train, or whatever, and the situation is the same


Strategy:

No more boost percentage. Instead, an absolute time limit boost. So a boost of like 60 seconds or so.

No more unsuccessful tries, instead, we keep increasing the gps time wanted for each attempt. Also, no
more minimum wait between unsuccessful tries (to keep it simpler)

Also when user interacts with phone (screen is turned on), last 30 seconds of movement (or so) is cancelled out. We might need to
update rest periods in the database, but I think we should do that anyway to capture the current state.


So, to RECAP, when at rest

1. increase gps time by double for failed attempts up to a certain maximum (10 for at rest and 5 for moving)... rest *beforehand* so
we aren't 10 minutes in GPS debt and the user starts to walk outside.
2. We lower the threshold down to 30 seconds whenever the state has changed (considering that we don't count very short rest periods)
   So if we are in a car, and the car stops, we shorten it down to 30 seconds... the thinking is that the state has probably changed
3. After every *successful try* we have a minimum time limit before we try again. (Having gps readings every 10 seconds is not
very useful)
4. After the first successful try, we shorten to 30 seconds. Then, we start bringing the time period back up again. After 
the second and follwing tries, *with no state change*, we use a rolling average. of the time it took.
5. We use all the gps time available for trying if we decide to do a go. So if we have 5 minutes saved up, and the state changes,
we ask for a minimum of 30 seconds, but if we get 5 minutes, we use it. We use it all. (this is because a long stretch of gps 
has a much better chance of reading than several broken up ones)
6. We have an absolute time limit boost of say 60 seconds to use when we really want gps (like after stopping). No boost percentage
(this prevents us from using 10 minutes of boost time on a high maximum)




--

I think we should do self patents. Yes they are not as good as real ones, but we can file them cheaply and 
I think we can do pretty good by looking at actual ones.

And any company would probably buy it regardless if it has some holes. Hell, ones done by lawyers also have holes.


---

4/20

For determining when the screen is in Intent.SCREEN_ON and Intent.SCREEN_OFF

4/21

maps api key

<com.google.android.maps.MapView
                 android:layout_width="fill_parent"
                 android:layout_height="fill_parent"
                 android:apiKey="0IzYTFYC-mK3pftVVjoJnon_DipfF75wHOMn_9g"
                 />

4/22

For compass readings:

1. record compass readings every 10 seconds or so
2. If the map shows two gps readings that are far enough apart (5 pixels or so), then
  a. Walk the trail of compass / movement times from the start of one gps to the end of the other
  b. scale the eucludian distance from the compass trail to the gps points
  c. rotate the compass trail to the gps point 
  d. draw as straight lines through the compass points
Note that every point along the way that we care to draw will be rotated and scaled. Based on the distance,
we can skip some points.

4/23

One idea for the lines that crisscross the screen and both points are off the map is to not draw lines 
between points. Maybe create a buffer of 50 pixels or so, and create an arrow or drip like effect from
one point to the next. If rest time is really weird and it crosses these boundries then, like, whatever,
but that shouldn't happen, right? 

I mean if the user turns off gps trailer and goes to another place, there shouldn't be a line anyway.

Rest time will cross the bounderies if gps isn't working, however.

4/29

For the dials, here is what we have:

a dial
strips

Does each strip need the same tick per pixel? Yes or lining up the strips would be tricky.
For our current application this won't bother it at all.

Each strip needs "wells" where the ticks are, so the finger can move left or right a few pixels
with no change to the dial. We could even speed up the dial before and after the wells.

Now as far as positions and pixels, etc. 
1. Changing font sizes would be nice for the user, a pain, but nice

Now, we have a logrithmic scale that does it all. But it's not generic, but that doesn't matter.
We also have a bunch of strips to choose from.
Each strip has some text and a distance of ticks.. it is linear

So the question becomes this. There are several variables out there that need to be defined
eventually. At construction, we may not know these variables, but look, here is what we need 
to know to define the width and height of each dial:

For width, nothing is needed.
For height, we need to know the number of strips and height of each strip. That is all we need.

So construction should only have the purpose of defining the minimum height of the dial.

On layout, the rest of the parameters should be constructed.

Now, I think that we should have an init() method for everything. Defining stuff in constructors
is sometimes tricky, especially with extensions. 

So, we have an init() method, which declares everything that is needed by the Strip. This
information must not need to contain anything that must be derived based on width and height
of the strip.

Now look at this, the ticks per pixel is dependant on the width because this is a range of data.
Maybe we need to define it as TextRangeStrip or a TextRangeDial. 
The width defines the range, so the ticks per pixel are dependent on the dial, and fixed.
Then the strips can join the dial if they can represent the ticks per pixel. Each strip has
a maximum ticks per pixel that it can handle. A millisecond Strip would have a very low 
maximum, but a year Strip would have a very high maximum.

Now, we can define RangeDial which can handle different types of Strips. So at init time,
the strip can define it's maximum ticks per pixel and be added to the range. The range can
then decide whether to include it? No, we decide that (that is, TimePosDial), and TimePosDial
handles the strips. 

--Dial
Dial(ctx)

protected setTicksPerPixel(ticksPerPixel) {invalidate()}

onLayout()
{ 
}

setStrips() {}


public getTicksPerPixel

(called by android)
draw()
draws strips

---PointDial
PointDial(ctx)

onLayout()
{}

public setTicksPerPixel()

(called by android)
draw()
{super.draw(); draws center position}


---So RangeDial looks like this:

--note that range dial cannot change it's ticksPerPixel (but the PointDial can)
RangeDial(ctx)

onLayout()
{sets windowSizePx and windowSizeTicks to sane amount, calls private dataChange()}

setRanngeSizePx(int windowSizePx)
{sets range size in pixels; calls dataChange()}

setRangeSizeTicks(long windowSizeTicks)
{sets range size in ticks; calls dataChange()} 

private dataChange() {calcs ticksPerPixel; invalidate() on change}

(called by android)
draw()
super.draw(), draws range markers

---Then we have Strip:
Strip()
init(dg, height, maxTicksPerPixel)
{}

(called by the dial itself)
setDial() 

(called by the dial itself)
setWidth()

draw
{calls dg.draw(pos - ((pixelOverlap + (float)width/2) * dial.ticksPerPixel),
				pos + ((pixelOverlap + (float)width/2) * dial.ticksPerPixel))
}

interface Dg
{
   draw(long startTick, long endTick)
}

ticksPerPixel is public


---Then we have TextStrip:
TextStrip()

init(Strip.Dg dg, textPaint)
{
   super.init(...)
}

drawText(long tick, text)
{
draws text at proper pixel
}

static abstract class DgImpl implements Strip.Dg
{ DgImpl(long ticksPerLabelPeriod)

getText(long tick)
{calls drawText()}
}

----
Now for log (time length dial), we will use text strip, since we want to equally space out the items,
and change the logrithmic scale between each dial position.

Then just our final result calculation is kind of tricky.

Once we have it, the DialSet will reset the strips if necessary for the TimePosDial
and redraw.


So we want 0-1 to be the same distance as 1-10
So given a pixel position, what is the answer?
exp(x), right?

exp(k) = ms2-ms1+1
exp(0) = 1

k = log(ms2-ms1+1)




exp(0)*k = s;
exp(1)*k = e;
exp(x1)/s = exp(x2)/e


exp(x2) = ms2
exp(x1) = ms1

log(ms2) = x2
log(ms1) = x1

x1-x2 = k(0-1)
x1-x2 = k(tick)

exp(ticks * (log(ms2) - log(ms1)))


exp(x) = ms
x = k * y + c
y = 1 then exp(x) = ms2
y = 0 then exp(x) = ms1

k + c = log(ms2)
c = log(ms1)

k = log(ms2) - log(ms1)

x = (log(ms2) - log(ms1)) * y + log(ms1)

4/30

Autozoom button - center on location

Better hours, they don't pop up enough
Maybe move from and to to the to bottom of the screen
pinch to zoom?
There can't be nothing on the time pos dial at all ever
We need lines straight through, always, since if we travel far, we always want to know
where we came from, and where we were going.
Very small dots for gps coordinates. Resting time is bigger.
I might like the idea of colors on the dial corresponding to colors on the graph.
Maybe a random pattern of colors, although I don't want it to be ugly if two colors mesh
Click on each spot shows the rest periods near by. If there are too many, we have a 
scroll window. Press and hold to label and a button, also in menu, and in the corner of the 
scroll window? Along with an x.

5/2

We have problems with relationships between components. For example, we need to 
set the range position, and range. We don't have the number of pixs to make 
the range until we layout.

If we could do something like atoms, like:

dependsOnFinalPixels


5/3



purpose:
1. Where was I yesterday?
2. Where was I during a certain time period?
3. When was I at a certain place?
4. Where and when did I go in a certain area?
 
 
 5/4
 
 purpose:
 
Mark places I've been in the last day or so
   -Times I've been at each position
   -direction of trail??
   -
   
memory of where I was for zooming back.. so I start at 1 day, then zoom out to 100 years, when
I zoom back to one day, I'm in the same place.
Starting position is the last gps entry, not now... no point in viewing blank data.



View places I've marked on the map

--- later: Lead to places on the map

ideas:
paste clipboard will save a place, or a bunch of places..
maybe have a url

wider range
alpha based on average speed through area

OPTIONS:
battery usage
color blind option (remove colors from bands)
on all the time

STARTUP:
For the first time, ask if data should be password protected


5/6

 (in Lcom/google/googlenav/map/Map;)
      name          : 'findZoom'

invoke-virtual {v5}, Lcom/google/map/MapState;.getCenterPoint:()Lcom/google/map/MapPoint; // method@07c8

invoke-virtual {v2, v0}, Lcom/google/googlenav/map/Map;.getMaxMapZoomForPoint:(Lcom/google/map/MapPoint;)I // method@05e6

It's simple, we just zoom in into we are past and then zoom out one. The google api is still smooth!

5/7

PROBLEMS:

When there are no gps coordinates to display for a time period, it sucks. We can't find when they start again on the dial,
so we are paging through no mans land. 
IDEA:Display gps dots on dial, so user can see when measurements are there. Maybe alpha dots for dots that are not on screen
and normal dots for dots that are?
Actually, the user is always somewhere, we just have a bug that displays no dots sometime.

When zooming time range close, and there are no gps measurements, we display only a line, it's kind of confusing.
IDEA: Display alpha lines and dots for data outside range, and autozoom to include these, but only in this condition where
there is only a line.


When viewing dots at a far time range, it's hard to zoom the time range in to just display those dots. 
IDEA: What if when zooming the time range, we zoom to the center of what's on screen?
IDEA2: Double click on the screen to adjust the time period to just the dots there.

Sometimes the colors are hard to see, especially the light colors.
Sometimes the text is hard to read.

The dial is too fat, and doesn't display the whole date
IDEA: display the dates on top of the dials and see if that helps
IDEA: put the year, month, and day on the left and don't make it spin.

Should we worry about performance yet?
I need to do partial lines and alpha... hmmm.


IDEA: a push as well as a pull... so when we zoom out, if by changing the range
we would include a point off the screen we adjust so it doesn't happen. In other words
we want to always keep the same points there, zoom out or zoom in. This is to solve
looking at a place and trying to figure out how long you were there.

IDEA: when zoomed out too far and hitting the edge, zoom in if the user is pulling against
the edge, instead of stopping.. It would probably make sense to keep their original zoom
when they back off the end though, or would it?
So when they rotate the range dial, we adjust the position to do what they want, and when
they rotate the position dial, we rotate the range dial to do what they want.
What I mean is if they rotate the range outward and we are at the end, we push the time inwards
and vice versa.

5/8

IDEA: When the user clicks on the item, display the following:
/-------------------------\
| (Name if set, or "Name" light gray)   X (for closing window
| Start - End Time      | |
| Start - End Time      | |
| Start - End Time      |<---- mini scroll bar
\_________________________/
					 \|
					 
So we'll pan so the window can be visible and then pop it up.
This will solve the following:
1. When viewing a location, there might be several times you are there, how to examine each one?
2. How to close the window easily
3. How to type in a name, or edit it

IDEA: When showing the map, display names in alpha baloons, just:
/--------------------\
| Name               |
\____________________/
                 \|


PROBLEM: What about not being able to read text in scroll bars?
PROBLEM: What about not being able to see movements because the colors are too bright?
IDEA: Maybe in map mode draw certain colors, satellite, use others, etc. That will solve the dial color problems as
   well
   
PROBLEM: Is GpsReviewer interfering with GpsTrailerService? I wonder if it's locking the database or something like
that

PROBLEM: What about ugliness?


IDEA: Display dots on dial where most activity is going on
This is to allow the user to figure out what they did last week. If they are not exactly sure of the time, they can
see the dots and see that a lot of activity happened on that day.. The only issue is that what do we actually display?
Dots when they were stationary? Dots when they were in heavy motion? Stationary, definately stationary, although being
at home will show a lot of "activity", then. Maybe too many dots?

IDEA: menu for allowing user to do queries, such as "Between these times when was I at unlabeled places?" 
"Between these times, when was I somewhere I've never been?"
This allows the user obvious ways to answer common questions

IDEA: For strips.. if a value isn't always present on the strip (for example, the year, then get rid of it. 
Show extra data if there is room to display it. So if days are far apart, it's ok to display Tue, Jan 1, 2010



select 'x' from location l1, (select _id, lat, long from location where lat >= ? and lat < ? 
	and lon >= ? and lon < ?) l2 where l1._id = l2_id and abs(l1.lat - ?) + abs(l2.lat - ?)  
	
	
MODE_WRITE_STRATEGY_STATUS  1: Sun May 16 17:00:58 CEST 2010
MODE_WRITE_STRATEGY_STATUS  2: Sun May 16 16:10:56 CEST 2010
MODE_WRITE_STRATEGY_STATUS  9:                30000


MODE_WRITE_STRATEGY_STATUS  1: Sun May 16 17:00:58 CEST 2010
MODE_WRITE_STRATEGY_STATUS  2: Sun May 16 17:07:42 CEST 2010
MODE_WRITE_STRATEGY_STATUS  9:               611508



MODE_WRITE_STRATEGY_STATUS  9:                30000
MODE_WRITE_STRATEGY_STATUS  9:                60000
MODE_WRITE_STRATEGY_STATUS  9:               120000
MODE_WRITE_STRATEGY_STATUS  9:               125835
MODE_WRITE_STRATEGY_STATUS  9:               240000
MODE_WRITE_STRATEGY_STATUS  9:               611508

 0 gpsOn
 1 gpsAttemptStartedMs
 2 gpsAttemptEndedMs
 3 lastGpsReadingMs
 4 lastGpsStatsUpdate
 5 nextSignificantEvent
 6 wantGpsLevel
 7 stoppedStartMs
 8 state
 9 gpsTimeWantedPerTry
10 totalTimeGpsRunningMs
11 totalTimeRunning


5/21/10

We are going to basic, fixed rates. The issue here is that 

Maybe not... i don't know. I'm worried that some environments might take longer to get a gps reading.
But changing the delay... it's like what if it takes an hour to get a reading in one case? We can't slow
everything down to an hour.

On the other hand, we don't want to set us to 30s per short try and gps never gets a reading until 1 minute

We can ignore outliers, detect movement, etc.

But the point is there should be a maximum, and a minimum.

Getting as many gps readings as possible would be the best.

lastShortReadingTime * 1.1 up to a maximum. Divide by 1.1 if a reading was successful to a minimum of
1.5 * successfulReadingTime.

Time to save will be constant, such as 15 seconds per try or something.
Minimum wait time will be a constant, such as 5 minutes

Then long time will be doubled at each unsuccessful long time failure, and reset at any success (including
short time) to successTime * 3

So then we have the following variables:

shortReadingTime
longReadingTime

and the following preferences

minimumWaitTime = 3*60*1000
shortReadingMultiplier = 1.1
longReadingMultiplier = 2
initialLongReadingMultiplerOfShortReading = 2
maximumLongReadingTime = 15 * 60 * 1000
maximumShortReadingTime = 60 * 1000
multiplierToSavePerShortReading = 2
initialShortReadingTime = 10 * 1000

5/22

Alarms and Wakeup locks


The following can be created once and used many times, it seems

        // Create an IntentSender that will launch our service, to be scheduled
        // with the alarm manager.
        mAlarmSender = PendingIntent.getService(AlarmService.this,
                0, new Intent(AlarmService.this, AlarmService_Service.class), 0);

Ok, so we create an Intent, OneShotAlarm in this example extends BroadcastReceiver and that's it

Intent and pending intent can be precreated and saved, I believe

Then just use the alarm.

Then we'll use a handler timer for the "on" time

So maybe we have a class as follows:

ResilientTimer

setAlarm(millseconds, keepCpuOn)

The special timer will either use a wakelocks and a handler, or an alarm based on keepCpuOn

                
            Intent intent = new Intent(AlarmController.this, OneShotAlarm.class);
            PendingIntent sender = PendingIntent.getBroadcast(AlarmController.this,
                    0, intent, 0);

            // We want the alarm to go off 30 seconds from now.
            Calendar calendar = Calendar.getInstance();
            calendar.setTimeInMillis(System.currentTimeMillis());
            calendar.add(Calendar.SECOND, 30);

            // Schedule the alarm!
            AlarmManager am = (AlarmManager)getSystemService(ALARM_SERVICE);
            am.set(AlarmManager.RTC_WAKEUP, calendar.getTimeInMillis(), sender);

The problem is the entire application can die, and needs to be woken up in a good state

5/29

Version 1 Feature list:

1. Map / Mapless screen
  Labels
  * Changable
  * Spread out?
  * Alphaed?
  
  Better display of points and labels
  
  Menu to switch screens, label current location, go back to front screen
  change current time period.
  
2. Enter password dialog. (first time use, create password, or use no password)
3. Front screen?
  Title
  Options:
    Review With Google Maps (grayed out if no internet)
    Review Without Maps
    Settings
  (Stats ... number of points, etc)
4. Settings
   Option for start on phone start
   Percentage of time for GPS to be on
   Password
5.

????

Upload maps

Maybe it would be better to figure out features organically... the only thing is to make sure it's
simple.

5/31/10

Next things:

1. password protect
2. "Find" menu, lets you specify locations and times... works like this
Screen 1
  Find times where I've been (xxx distance) from (location)
  between (xxx time) and (xxx time)

* If there are no locations yet, the first part is omitted
* if location is not filled, then ignore

Screen 2
  Choose time period(s):
  start1 - end1
  start2 - end2
  ...
  
* if multiple periods are chosen, then combine
* if only one period, then skip

Screen 3
  Time period refinement:
  (start) to (end)

* Autozoom to chosen location

3. Time period select overhaul

4. Settings


6/2/10

I want to set only the preferences that matter. This is so when changing data items, I won't have to update preferences
everytime in the db.

There are some preferences that need to be reapplied to the next version, though.

Suppose the changable, user editable preferences are the only ones that are saved, and are always saved.

So then when we write stuff to the db, we are only writing fields, one at a time, and we write only those which
can be changed by the user.

So first time:

GpsReviewerApp does nothing (which uses factory settings).
When settings change, GpsReviewerApp.save() is called, which saves everything to database

Second time:
GpsReviewerApp loads db settings and applies in memory.

If corrupted:
If corrupted, there is a problem loading the data. We should know this by the testRun. If it fails during this time
, reset to factory (do nothing), notify user, and that's that. Try to recover password preference. If that fails, we can
tell the user that we lost all their data. Hmm.. maybe better to hide that fact and throw an error everytime. It 
shouldn't happen, anyway.

This way, every users preferences that are saved to db are the same, but I can change code that doesn't have a visible
preference without mucking with the database everytime. Otherwise, it will take a database upgrade, but that's ok, because
it shouldn't happen that often.

6/3/10

Here is how we do encryption, if we really want:

Public key - private key.

private key is encrypted with AES using password and stored as a preference
public key is not encrypted and stored as a preference

When we reboot and need to take measurements right away, we choose a random key, and 
sign it with the public key, putting it in a special table.
Every gps measurement references this special table to say which key it uses.
We peridoically "clean up" this key table by reencoding the rows to use the one
with the highest encrypted data.

In this way we can avoid using slow RSA when reading the data but still take advantage
of public/private key crytography for when we reboot and we don't know what the key is.

Of course we need to encrypt user location, too. If we want to cache it, we cache a certain area 
of lat and lon. So we cache a rectangle of area. That way we cache without worrying about it.
But I think for now, we just decrypt the whole thing every time.

Geez, it's a pain in the ass without indexes. I wonder if we could include them somehow.
Like have a lat and lon and time index field. You know? We already use id for the time index.
If we could create lat_index and lon_index, we may have something. Then we can index those fields.
Yet that would probably be giving away too much information, since you could sort of display a
map of their locations... it seems pointless though. and no one would care.

I think we'll just do the first method. Just a lot of code, that's all. Crap.

6/4

When starting up gpstrailerreviewer, we may want to reencrypt all data to use one key... or we
 could keep a in memory key table.. probably just reencrypt to use one key

6/6/10

We need to decide whether to implement encryption or not now. This depends on how we want to
actually display the data, which also depends on the speed of our decryption.

Lets get a vision for it.

Ok, so, I think we have to keep to the current system for now. When it comes down to brass
tacks, we need to display *the data we have* in the most clear and concise way possible. 
The data we have is gps readings, which means one point per reading, with a line connecting
them. 

Were going to go for this. Mabye something cosmetic on the side, but for the most part, this
is the way it will be.

It's sometimes difficult to determine direction... how can we fix this? Maybe darker to lighter
colors for the transistion?

Ok, so back to the matter at hand:

1. Encryption: for gpslocationtable, we select all rows and use moveToPosition() (god help us
if this doesn't work) Based on my cursory examination of android source, this should work well.
Android uses a "WindowedCursor" (search for SQLiteCursor for more info) which will only select
some rows at a time.

2. For userlocation, we don't have a id to index sync up. In this case, we need to keep pulling
data until we have the right item(s). In fact, if we want a range, we need to check every single
one every single time. On the other hand, we only want to display so many rows, so the end result
is a small set. So we need search through everything, and cache a certain set based on the number
and the range (and the zoom level probably). This is all if the user produces so many labels we
can't store the indexes, mind you, into memory. This is a pretty small chance and a pretty large
amount of code. So the alternative is to say darn it and load it all into memory. I think
this is the best idea for this table.

6/7

encryption is a nightmare!

So we have individual tables: 
GpsLocation
UserLocation
...

eventually we will probably have more.

Each table has the same fields.
id, user_key_data_id, data

Here are the operations:

1. Encrypt without password (by generating a new user_key_data row)

2. Encrypt with password (by reusing a user_key_data row)

3. Decrypt (with password)

---

but really, here are the real things we are trying to do:

1. insert into row

setup1: generate and initialize key, 
or,
setup2: 
	a. use password to decrypt private key
	b. use private key to get existing key

	a. encode data into blob
	b. encrypt blob
	c. insert blob

2. delete from row
   setup: none

       a. delete row

3. update row
   setup: 
	a. use password to decrypt private key

   a. get row data
   b. use private key to get existing key for row
   c. decrypt row data
   d. decode row data
   e. read value
   f. encode new value into row data
   g. encrypt row data using existing key for row
   h. (in transaction) update row data

4. read row

   setup: 
	a. use password to decrypt private key

   a. get row data
   b. use private key to get existing key for row
   c. decrypt row data
   d. decode row data
   e. read value(s)

Initial setup:
a. Initialize database
   1. create user_data_key table
b. Create private and public keys
c. Create salt
d. Create initial user_data_key

Clean up:
peridiocally reencrypt all data using different keys so we reduce the
total number of keys.

  setup:
	a. use password to decrypt private key

  a. run mega select to find the total number of rows for each key
     (maybe rerun for each different table?)
  b. choose key that has the most data (or maybe just use the first key)
  c. decrypt first row sym
  c. for all other keys, and each encrypted table
     1. for each row
        a. get sym key using private key
        b. decrypt row
        c. encrypt row using sym from first row
	d. update row
  d. delete key (which is no longer in use)

Change password:
  a. get existing and new password
  b. decrypt private key
  c. re-encrypt private key
  d. save private key
       


Each table row needs to be converted from its binary format to 
java readable data and vice versa


--

GpsTrailerCrypt:

holds salt, encrypted private key, public key, initialWorkPerformed

DONE static initializeWithNoPassword()
{
	1. generates new user_key_data row and sets its as the
	   value to use for instance
}

DONE static initialize(String password)
{
    if(!initialWorkPerformed)
	1. create public and private key
	2. create salt
	3. encrypt private key
	4. save preferences
    
    pulls first user_key_data row (or generates it if it doesn't exist)
    and sets it as the default instance
}


DONE static instance
{
	//returns default instance for encrypting new rows
}

DONE static instance(int id)
{
	//returns an instance based on the user_key_data table
	//saves instances to a cache
}

static EncryptedTable getGpsLocationTable()

static EncryptedTable getUserLocationTable()

static cleanUp()

------

EncryptedTable

EncryptedTable(RowFactory rf)

rowFactory

readRow(EncryptedRow er, Cursor c)

updateRow(encryptedRow)

deleteRow(id)

insertRow(encryptedRow)

----

EncryptedRow:
table
data
userDataId
id
encode(col, data

---

GpsLocationRowFactory
allocateEncryptedRow()
encodeInt(row, col, data)
encodeLong(row, col, data)
encodeDouble(row, col, data)
encodeFloat(row, col, data)
encodeString(row, col, data)
data = decode(row, col)

UserLocationRowFactory
allocateEncryptedRow()
encode(row, col, data)
data = decode(row, col) #note that for name changes, the length may change
		        #this is ok, since there are so few rows





6/11

It's way too slow to be usable as is.

We need to cache data in memory.

Of course, when zooming out, there is too many points to cache.
So we can lessen the number of points. However, that doesn't
work if they zoom in while displaying a large time period.

We can cache points by area to combat that. Ugh, but that won't
work either, because we can't index based on lat/lon

And due to criss crossing lines in front of the screen, we 
can't just ignore points that are out of range.

So the thing is that certain times cross into certain areas

When you are viewing an area, it's not absolute. You are
viewing a collection of *time periods*

X crossed this location at Y, then at Z
So two time periods are being viewed, Y and Z. Everything else
can be thrown out.

So we zoom in on a particular area. In reality, you are zooming
into a time period. 

So let's say we are zoomed out completely. Then we zoom in,
let's say on a line.

--

Ok, so each area has time period ranges in it (which we might
as well use id ranges instead).

If we can figure out which time periods to display, we won't
have to worry about too much data (unless they were there
for a long time (their home for instance))

So if we made a cheat sheet of area vs time periods, i mean
id ranges, we could use this to determine the extent of data
that we needed to display and load what was necessary. 
Combined with a cache, we should be ok. In fact, what we should
do is have a background process that loads the data, the the
overlay simply displays the bitmap created by the background 
process, in order to ensure speed and smoothness of movement.
In addition, that should help panning a lot. I think maps 
probably works similarly itself.

So how do we do this and keep memory and time under control, etc.

And what to do about hot points? (home, etc.)

Remember that we can do an encrypted cache, too.

There are some similarities between hot points and zooming out really
far. In both cases, the points are too close together to be meaningful.

What if we had a distance trail, with point ranges along the way?
Maybe not even a trail, but a graph, so if a point was returned to
again and again, we can use one point with several id ranges.

The other thing to consider is that after a point is laid down, a
place visited, etc, it never changes. The way it was represented is
the way it always will be. (except the color.. darn gosh)

Darn gosh

The thing is that the user is unlikely to scroll over a big area
when zoomed in.

So for far away zooming, we can store the whole line in memory

And near zooming, we can store a land area, that is not too far
away.

We'll use tiles of data. Lines won't work when zooming in

So each tile, we'll need an id range, and a time range, too...
or something, because we need the color. Maybe just a time
range, then. 

So lets imagine we have infinity storage per block, what 
do we do?

To be clear, we'll have to do basically the same thing, but
just with less data. So instead 1000 points, there will
just be 100 or something like that.

The other thing, is that we can cache a lot of it.

In memory, as pixel locations will be a hell of a lot faster.
Or as a bitmap will be even faster. 

So cache can help different operations differently.

Panning:
bitmap cache
reduced point cache

Time period shifting:
reduced point cache

Zooming:



---

A thicker line would reduce the need for pixel point accuracy as well.

Another issue is that we can't let the bad guys glean data from the
cache by the size of each item, etc.

If we all store it in one table, we should be ok, but then, we have to
read the entire table to figure out where things are.

We definitely need to precompute it though. Even 3000 points takes a
long time to read. 

So let's say we precompute it, and we go through each point, one at
a time, and determine where it is in relation to the last point, and
which point is more important based on it's size. And based on that
we compute the cache (or caches).

From a global zoomed out view, we shouldn't have much trouble, 
but when we zoom in, we need to find the points being a certain
rectangular area quickly. And not just the points, but the lines
that cross that area as well. And this can be tricky. We might
have to actually add points to the cache that aren't in the original
data set in order to do this.

In which case, that leads us to separating the data in regions.
And each "point" needs a time range rather than a time point,
so that we colorize it properly.

We can totally drop the original points if we cache it well enough,
this will even work for selecting points for user location data.

Of course, we can only simplify so much. We can't really put 
an upper limit to the number of points that occupy a space, but
we should be able to reduce it enough so it fits in memory.. hmm
that is the issue. I suppose we will just have an upper limit and
if we go past that, we just won't draw everything... or something,
it will wack out at that level in someway, but it will be 1% of
users that will be bothered, so who cares?

So we have a cubic space of lat, lon, and time period, and we 
work from there.

So then, how do we cache it? 3 tables:

time based id ... zoom_level, salt, encrypted data (or pointer?)
lat based id ... zoom_level, salt, encrypted pointer
lon based id ... zoom_level, salt, encrypted pointer

Darn gosh those encrypted pointers, shucks... because it 
means we'll be fighting the database for getting the data we
need. 

There could be a ton of data in lat, lon, or time based intervals
and no way to reduce it down without going through it all.
But the alternative doesn't work either.

The best we could do is find the smallest dimension and iterate
over that, filtering out the other two.

In general, that shouldn't be too bad. We can take slices to
expand the box in any of the three dimensions, decrypting into
memory. 

I suppose that zoom level is a fourth dimension, and we could
make this variable rather than constant. It might be good,
because then points don't shift to a new position when zooming
in.

But how do we figure the zoom level?

That should be fine, we'd do it the normal way. Just pick
points for a "top zoom level", then iterate down. It's just
that zoom levels below the top display their points and the
higher zoom level as well.
Then the cache from different zoom levels could help out,
possibly.

I wonder if we could incorporate some lat/lon to pixel
conversion in the cache. I doubt it... we'll probably need
to stick to lat/lon for now.

Another option is to just have one cache table, and store the
three indexes, time, lat, and lon in memory. I wonder if that
would be too much data, though.

Probably too much.

So the only reason to have data in the cache table is because
of faux points. It might be easier to just store these in 
the main table and label them as such.
But then again, here we go again with the tenenous relationships...
the zoom level itself would tell a black hat approximately how far
they traveled at many unknown time periods.

There is no way around this, unless we have separate cache rows
and don't do the zoom inclusive thing. Maybe that's what we'll do.

Then the hacker could tell the approximate distance they traveled over
the lifetime of the app, but not much more.

So, separate zoom levels it is. We can throw it all into gps_location_time,
 I suppose, but it might be better having a separate cache table. 
In this way, we don't have to worry about cache only fields (such as zoom 
level) and cache points (faux data) being an issue.

We can also steal data from multiple points... altitude from one point,
lat and lon from another. hmm.. maybe that isn't a good idea.
but it would be nice, because otherwise we can't find the extreme range
of alt from a high zoom level which people may be interested in.
Although that would be confusing. It would be better not to do that, and
just look at the gps points themselves. Then we can also keep them
time ordered, so that to find points between a particular range wouldn't
be that hard. So if we did want to find the extreme alt range, we could
in a reasonable period of time..

---

So we have multiple caches, we have a cache table for each

The cache table looks like this

create table gps_loc_cache
{
	id integer primary key,
	min_pixels_per_item float not null,
	user_data_key_fk integer not null,
	salt integer not null,
	data blob not null
}

create table gps_loc_cache_index_table
{
	id integer primary key,
	min_pixels_per_item float not null,
	type integer not null,
	user_data_key_fk integer not null,
	salt integer not null,
	data integer not null
}

The id will be sorted by latitude.

Then, we need the in memory cache object.

So we just have Cache, and in it we
store a 3 dimensional box of the currently viewable area.

So what we do is deal with each dimensional change as one 
operation.

The cache will have an absolute max number of points, but will
fail gracefully, and ignore points after it is full.

So we use tree maps on lat, lon, and time.

So there are two operations,
create the initial box,

and then expand (or contract) dimensions.
Contract will simply be to change the required cached area
Expand will run a query against the index, and then against the
data that it represents (except for the 0 index -- currently lat)

Then we'll delete items based on their count from a particular
dimension. Which ever has more fat, I guess, or maybe the earliest one
modified. I think that is best.

We should try to make it easy to add more indexes, I guess..
Or maybe make the underlying table switchable... hmm.
I don't think we need to worry about it for now. Abstraction
isn't too hard.

Random access files... better than the current db situation by far.
I'm thinking of switching to them straight away, since working with
additional tables is going be additional work that we'll probably
throw away anyway.

The only issue that I see is that the file could be corrupted.
I think that we will use RandomAccessFile for writing as well,
since it has built in writeFloat, writeDouble() etc, thank goodness.

So then it's easy. The first location in the file points to the last completed record.

Ooh, the only thing is user location also needs to be encrypted.

So maybe we'll have two pointers. The first will be to the end of the file.
The second will be to the first and last *deleted* record. 
That record will point to the next deleted record. And so on..... no that
will be a mess because when we reclaim a record, we might write over the data
We could have a separate file with .. no when writing an int, there is
no guarantee that the first int will even be set completely. The first
byte may be the only thing set.

What if we have two index files that we switch back and forth from.

So, we have EncryptedTableFile

Here is how sqlite does it:

** This routine does the first phase of a two-phase commit.  This routine
** causes a rollback journal to be created (if it does not already exist)
** and populated with enough information so that if a power loss occurs
** the database can be restored to its original state by playing back
** the journal.  Then the contents of the journal are flushed out to
** the disk.  After the journal is safely on oxide, the changes to the
** database are written into the database file and flushed to oxide.
** At the end of this call, the rollback journal still exists on the
** disk and we are still holding all locks, so the transaction has not
** committed.  See sqlite3BtreeCommit() for the second phase of the
** commit process.

So we need to do something similar

For insert
Create another file that specifies the end of the current file.
Flush?
insert
remove file. That should work because a remove will be atomic.

The question is the stable storage requirement of a two phase commit (according to wikipedia) Do we have this?

It appears the sqlite solves this by having two journals.. (from the comments, not sure about this)... a master journal and another journal that has
the name of the master journal in it.

Gosh goodness, here is the sqlite format:
**
** + 4 bytes: PAGER_MJ_PGNO.
** + N bytes: length of master journal name.
** + 4 bytes: N
** + 4 bytes: Master journal name checksum.
** + 8 bytes: aJournalMagic[].
**

A lot of stuff here.

However there is also this

** If zMaster is a NULL pointer (occurs for a single database transaction), 
** this call is a no-op.

So maybe this double journal file isn't necessary

So it looks like it does something like write a journal, then sync the journal
to the disc and update the first int to an all clear flag. Then it does it's
changes.

So I guess we can do the same thing. 

That is, create a file which has 0x00000000 at the beginning and 
the second long indicates the current end of the data file.
Then we write 0xFFFFFFFF at the beginning, flush the journal. Write
the next record, and delete (or write 0x00000000 at the beginning)

For deleted records, we can have another file that specifies which records
are deleted.
In this case, we'll need to write a type and a value, and the same 0x00000000
thing.

Basically a transaction log.

So delete will be exactly:

0x0000000
0x0200
(end of deleted records file)

Then, when we add it to the table of deleted entries, we...
set the top part of this file back to 0x0000000

The update will be exactly:

0x0000000
0x0000
(end of records file)
0x0100
(end of deleted records file)

Darn doggit, this is a lot of work. And just for user location...
I wonder if we shouldn't just keep user location the same as it is.


Because without it, we can just add gps locations with no issue.
Which makes me wonder if we should do this at all... I suppose it
will be faster. But by how much...

Right now Sqllitecursor takes 26 % of the time. We can probably reduce
this... we'll have to try selecting one row at a time, I guess...
Hmm... don't know...

6/12

I'm debating whether to keep working with the database or go flat file, man.

If I go flat file, I don't have to worry about the window filling thingy,
no worries about reading a huge dataset. OTOH, I can reduce the size of the
window for performance.

The flat file has tons of work involved, though:

journal recovery
If I do user_location, then have to support update and delete

Even without user_location, I need to support the zoom level thingy.
And doing that means creating an index. Which further complicates the
journals.

And testing is going to tough, and if I do a sloppy job of it, it can
come back to get me later.

The benifit is no more windows of data.. which is good i guess. I'm not sure
if it matters too much though, because I can reduce it with smaller queries
of data.

6/13

We are going to have one cache file and repeat the data for each "index". This way we don't have to jump between two tables... besides, the amount of data
that we would be saving by creating a real index would be minimal.

6/14

Rules:

ids must be sequential (so that getting the next row is easy when searching)
zoom levels are not shared

6/15

When caching points, we won't cache based on the maximum number of points
in a window for the following reasons:

1. A window could be displaying infinity time, so we would have to choose
the number of points based on that. However, the window might be displaying
just an hour or two, then the number of points would be severely reduced
unnecessarily.
2. difficult to calculate, and earlier points would have a precendence over
later points

6/19

There is a problem. We can't order by lat and lon without pulling 
every point into memory because they are encrypted.

Note that it's not a problem that there are the same number of points 
no matter what dimension, since there is no way to correlate each row

We should separate GpsTrailerCache into the current and GpsTrailerCacheCreator
The current system depends on the each cache point having a sequential index.
We'll have to stop that.

Afterwards, we'll need to create cache points using a long id, and separate
it out so some bits are for points that land inbetween and others are for 
points that are not inbetween.

This worries me, since a place that is often used could easily over extend
this range (I think).

A linked list might work.. but it would screw up the binary search.

I wonder if that would be acceptable. It would mean we would no longer
need 3 copies of each cache point.

We could keep pulling in random data and decrypt it until we were "close 
enough" to the area we want.
Since most operations are just slight movements, normally we wouldn't
have to search far. We could place one index (time probably) as a
sequential one, and the others as linked lists.

We'd still need the cache rows for "inbetween" points.. especially if we 
want to maintain the "time" index.

We should have an upper and lower hierarchy of links. The down link will
point to the first lower item. In that way, we don't need to pull data
at random. We start at the top level and work our way down.

We'd still have separate points according to zoom level. This is so we
have room for all the pointers.

So basically we would have the following:

CacheRow:

id
zoom_ppmd - this is not encrypted since we want to be able to go back and forth
with id and find the next or previous row for the zoom_ppmd
--
kind (inbetween or normal)
latm
lonm
time
gps_loc_fk
latm_back_fk
latm_forw_fk
lonm_back_fk
lonm_forw_fk
zoom_up_fk
zoom_down_fk (points to random(?) cache location below)



---

ok, so now we realize that for the cache, we no longer need to deal in boxes... it's useless, because
we already know the blank areas due to the absense of links (except in the time direction, hmm... well, it's
not encrypted so at least there is that)

So ignoring time for the moment, we can now pull in and drop points willy nilly.
However, there still we be a chosen box. Now think, we are absolutely sure the chosen box is covered.
But, anything outside of it could have gaps. Even if items have gaps, this doesn't mean that the 
new chosen box isn't covered, however. 

Also, just because we have something cached, doesn't mean it's going to help much. If we don't know
what's between point a with latm 1 and point b with latm 4, we will have to search inside that area
to check the lonm and the time.

We can have the chosen box, the outer limits (a square area where we know whats what) and the wild west
where there might be holes in the data. But it's so complicated. I wonder if there is a better way to do this

I guess this is the method.. darn gosh.. tough.

So here we go.

We have chosen box, and the ghost box.

For the ghost box:

1. We can't bite into the chosen box
2. We want to remove the least amount of area possible
3. We want to be far away from the chosen box as possible

For the wild west:

1. We remove from here first.
2. We find each point by looking at the edges of the treemaps

So here is the method:

First delete from the wild west if there are points available.
If not, shorten the ghost box to one minus the edge of the wild west
basd on points 1, and 2 (obviously chosing a different zoom level first
is possible)

6/21

But the ghost box could go more than one direction. It could look kind of like a plus
But we don't want to make it too complicated.

Maybe we shouldn't worry about the cache right now. We'll just pull in points, forget
about the ghost box, and add it back if necessary. It's a perf anyways.

So then we have the chosen box, which pulls in points. We cache the points,
and then remove the ones that are far away from the chosen box.
We cache based on id only, then. 

So then we have:

??? [] zlIdToRow 

Now we need to look up points based on id, and purge
the cache.

To purge it, we could use a treemap based on accesses,
so:

??? lastAccessToRow

Now, the question is, who should get the 0 index, lat, lon or time?

Right now it's time. So what is the advantage of having it? and the disadvantage?

The advantage is to find a particular value is faster, and so is getting a set of
values. You can also tell how many values are between two points.
No. we have to use time. Otherwise we will be inserting rows which would be
incredibly messy.

The main thing we should be trying to do is increase the size of the box. Hey,
if we can cover all points, that would be awesome. Free panning, baby.

---

Wait this is complicated again

Lets start over, once again... dog goneit

When we search the cache for rows, we will want to have an iterator between
two times, lat and lon positions.

Maybe we should make that into an array list?

Darn it.

Why is this so hard?

So ideally, we would want to say,
getPoints(zl,t1,lat1,lon1,t2,lat2,lon2)

If we just create an array of this data, and coordinates,
we're cool. We would have to use a sorted map.

Then, when we want to expand the box, we what? we go
through the list until we are missing a link. 
i.e. The next id in the time dimension isn't found.

Then we find it. If it's in the dimensions we want,
then good. If not, then bad. But, we can keep track
of the minimum that we find (that we don't know about)
In other words, here is what we are guaranteed.

We are guaranteed that all points within the box are there.
So while we go through the box, if we find points outside

....

No no no, the issue is that we don't know in which direction to go.
Because if we go through time, then we may find points in lat and
lon that are way outside of the box.

We could go in lat, and find points outside in time and lon.
We could go in lon and find points outside in lat and time.

I mean it's doable. But how much time do we want to spend?
How much work do we want to do?

We need to decide which direction to go, which may not be
too easy. It depends on the length of each dimension.
Then we need to walk down it, and expand the cache if we can in
whatever other direction we can try to expand it in. It's very
frustrating.

Like if we walk down lat... 
we will get points that may or may not be in the box. We handle
those points. It kind of sucks.

But I really can't see an alternative. We walk down the side
that is what? Has the most area? So suppose we are dealing
with a very narrow time, but thick lat and lon. Then what?
We say we go down time, since most of the points we find
in the other dimensions will be there. And if we walked down
lat, we'd go very far, and have a lot of misses, because time
is so narrow.

But comparing dimensions is worthless unless we know the min 
and max of each, so we can compute the point density. We can
find that given an initial chosen box, though. So there we go.
And then again, it is complex. 

But we handled EncryptedRows, so I think we can handle this.
Dog gosh, it's hard, though.

So we have a 
createInitial()... we'll use density based on first and last
points to decide which way to go.

Then we have
getData(int zoomLevel, long [] dim6)

Why don't we just have several boxes, like 10? 
Then we just subtract out the boxes that we have,
and find what's left. We will use the thick dimension 
idea to choose the direction.


Why don't we use a tile system?
Each cache point has a tile id. The tile id start and end
positions in the data can be cached.

So then we either load the entire tile or we don't. Keep in
mind they will be three dimensional. Maybe quite big for the 
lowest zoom level. 

but even if we don't keep every tile in memory, we can still
do it using a binary search. Then a window will simply request
a box that will be converted to tiles. The ones we have we keep
the ones we don't we find.

Since it'll be sparse, we'll do some efficiency stuff for that.

So either we have the tile or we don't. No more wild west. 

The only thing is that new points will have to be added in
the right place. So we have the difficult position of having to
push a lot of points over to add a new one.
And we can't use a linked list, since we can't do a binary search
then.

This might sound crazy, but what about this:

First point for a tile, we add it in order.
Then we have a linked list to the next point.
Same issue, only smaller.

Or what if we had a new table, called Tile
which pointed to the first point in each case.

Except that whenever a tile changed, you knew
the person visited that spot. So we change
random tiles, I guess.

What if we Tile wasn't a table, but a file?
Same issue.

You could actually tell where the person 
was, too. Whether they stuck to the same
tile or went somewhere else. You could
tell that two rows were related because
they'd both be modified at the same time.
Linked lists will always have that issue.

Encryption sucks. I'm so tempted to 
just get rid of it.

So even if we keep the linked list method,
it would be possible to tell that these
points are close together in either time,
lat, or lon by which ones were modified.

So then we go back to the tile method 
with insertion. But we can't do that,
because the index will vary in density
depending on which tile you are in.

So, we're up a creek.

---

Each microdegree is .11 meters
We take points at a maximum of every 3 minutes
That's 20 an hour.

So let's start with a cache size of 11 meters per side
by 1 hour. So 1 hour is a maximum of 20 points.

Let's see, so that is 
100000 * 100000 * 24 * 365

which equals 

87,600,000,000,000 or 87.6 terrabytes.

Huge.

So let's change that to 110 meters per side and 1 day
which is 
10000 * 10000 * 365

36,500,000,000 or 36.5 gigabytes

So then let's go
1100 per side and 2 days
(lat and lon are the 180 and 360)
1000 * 1000 * 365 * 180 * 360

So we could do 10 km per side

It's too much data

So I think we are left with time tiles with linked lists
to other tiles.

tiles should all have max and min lat and lon.
This is so we know whether we need to pull the tile in at all.
Otherwise we are always pulling in a tile even if none of its points
are in range.

Keep in mind that at a particular time, a user can only be in one tile.

Scenarios:

1. Far zoom out, large time period. We will load all the tiles.
Of course, this is ok, because if we are zoomed out far, the tiles
will have few points.

2. Near zoom, large time period.

Note that having a large time period makes ordering by time ineffective.

I wonder if we can simply cache all tiles and say the heck with it.

If we had a tile per day, that would be a maximum of 480 points.
Then the most tiles we could have would be 365 per year
so that would be around 3650 per 10 years.
There would probably be a lot less than 480 per day, however.
So 30 years would be 36500 tiles.
Each tile would need min and max lat, lon and time start.
So that is 4*2+4*2+8 = 16 bytes
16 * 36500 = 584000 or half a meg. Not too bad.
Even 10 years from now, we won't have to worry about a measly half meg
and then it'll be a quarter.

Heck I don't think we even need to worry about it today.

So there we have it. Now the other thing, is do we want to make tiles 
different time sizes, would that be a problem?

I don't think so.

So that should solve the issue. We probably won't average more than 40-50 points a day.

Could we / should we have tiles of tiles?

I think that tiles and our zoom levels could be combined.

6/23/2010

So lets see. The danger is that although the zoom levels will provide
good information for display, they may not be appropriate for caching.
However, if we do combine them, its a smaller cache.

The thing is though, that the zoom levels are quite arbitrary and
can be adjusted. There are a minimum number of zoom levels, but not
a maximum.

It also means that we need to always store the top most level.

Although the top most level could even be higher than what we 
would normally display. So I don't think this is much of an issue.

So the question becomes, how do we decide what to make the zoom levels at.

Now look, we need a minimum and a maximum. Hold on,
so remember, we work like this

Tile
min lat, max lat
min lon, max lon
lat, lon
speed mps,
gps_loc_fk
first_down_fk
sibling_fk

(we could also have the following so we don't need to load all the siblings)
up_fk
lat_left_fk, lon_left_fk, lat_right_fk, lon_right_fk


So the issue now is, how to make sure we have rectangular areas.

First, do we need them? What if we allowed tiles to overlap?

The basic strategy for finding a window of space is:
1. From highest level, find points that overlaps area (if none, then you're done)
2. If zoom level of window good enough for zoom level of tile(s)? If yes, then display them
3. Load all of the sub tiles of overlapping tiles. For the ones that overlap repeat from step 1 (aggregate and display them all)

So if we had overlapping tiles (which to think about it, I think we'll need, they have to be optimized for display purposes) ..

Actually our algorithm (as long as we use rectangular distance (ie. at most 
x pixels in x, or at most x pixels in y), willl cause non overlapping rectangles
and squares. So we are fine there.

Now lets consider hot spots:
So let's talk about the home scenario (where the user continually comes to their house every day for 10 years and there are a ton of points there)

Oh we are forgetting that time is also needed as well. So we will have overlapping areas (unless we start updating the tiles)

The problem is that we will have several distinct times for these areas (that the user was there).

So even if we have a few giant top tiles, if the user keeps going back and forth between them, we could have an unlimited number of cache points.

Like say going to work and back every day.

Darn.

Maybe we could cache in the time dimension, too.
So we have a tile per time period, granted. 
Then we have timezoom, which will cover multiple time periods
and create a new row with a fuzziness and a center.

Because if the user zooms in on a time period, we need to know where
the doggoneit they went.

And it is true that it looks messy as heck anyway if they zoom out in
time and view a hotspot.

So, if we can help it, I don't want to make each tile have to overlap
exactly if they are closeby and have separate time periods.

So maybe when we zoom out in time, we can do the same as when we zoom
out in size.. in that if points are near enough, they become one tile.
So the tiles will expand, but just a little bit.

The weird thing is that the time overlap doesn't apply when the
prior and next points are known. In other words, if it took you
5 days and to walk from sf past your home in mv to san jose, then
it should just be a line.

But if you are zoomed in on your home across all time, it should be
blurry.

True, but if you zoom in on the 5 days, during those 5 days, you
would have only visited your home once.

So it matters how many tiles are contained in the time period you mention,
and that is all. So if you mention 5 days and you were only there once,
then it won't be blurry at all, and very sharp. 

Also if you were there many times but over a short time period, it wouldn't
be "blurry" either. I suppose there are two dimensions here:
location
time (as far as color goes)

So let's do two things.

One is that somehow we will gradiant depending on the times involved. 
Depending on the center, we will blur towards the edge, or do something like 
that.

So each tile will have a time range, as well as a point range.
And a time_down_fk and a time_up_fk as well.

So the idea for making the cache is as follows:
(remember that this will be done often. So that we don't have to worry
about inefficiencies with large number of points for these steps)

1. Start with the points.
2. Start with a minimum tile size (based on maximum ui zoom)
3. Start collecting tiles without concern for time, i.e. each new time
 period gets a new tile. (time is granular enough that we don't need to 
 worry about initial aggregation). 
4. Go through the tiles again, with a further out zoom level. We will have
  to copy the tiles where there is only one useful point. (i.e. if there
  is a subtile with no tiles close by enough to be aggregated, we have
  to create a copy for the next level)
5. Continue until we reach the top zoom level. 
6. 


-- the output will be
tiles for each zoom level will be complete, and will be selectable together.

The issue still is, what if they go back and forth from work, and one upper 
level tile contains all the lower level ones.
Then we need to go through a large number of tiles with no intervening
separation. And there might not be a way to separate it, because if they are
the same spots, each time, then, yo!

I wonder if we could say for sure that the tiles will be created in order.
In that case, we could find the tiles that we want. So if we want 5 days
of tiles betwee May 3rd and May 8th, then we can use binary search between
start and end tiles for a particular time. No, it won't work because the
user might move out of the area. No because if they did, there would be
another upper level tile that would cut that out. So we can use binary
search and be guaranteed that all the tiles under the larger tile are
in order and are within it's bounds. So then we don't even need sibling
links. We can just have a start_down_fk and an end_down_fk and that's it.

So if we want infinity time over a spot? Or a month?
We need to do the next part:

Zoom by time.

Here we want to create fuzzy areas. If a user goes to a place multiple times
we want to be able to fuzzy it.

So the question becomes, when do two points overlap? When their areas overlap,
how about their centers? I think we have to think in terms of centers,
but we can always change this. 

So the idea here is to combine by time, but just like with lat and lon,
we keep track of the range of times for each of the points.

So, for instance, lets say we are zoomed all the way out and we
have the time level set to infinity.

We need to find all the points and give them an infinity time level.
Let them overlap as we decide. 

Then, let's say we lower it down to a month. So at the end, we might
need to *combine* tiles based on when they are. So if we have time levels
of 1 month, and we are viewing apr 15th to may 15th and we have two tiles
in about the same position, from apr 1st to may 1st and may 1st to june 1st,
then we need to combine the two and take the difference.

Oh, so maybe it's not just combining time periods willy nilly, but we need
to combine the tiles that are in similar locations and are close together
in time by a certain requirement as well.

Because, in this case, we actually combine tiles that are close together 
in time and space

So in other words there is a time granularity... or is there?
Can we just always combine based on space granularity? 
So we then combine tiles if they are close together always in the same
map.

Lets see, the time granularity is really denoted by the color that we
are using. In otherwords, if two points are both orange, then they can
be represented by the same tile. 

And that all depends on the time range *on the screen at the time*. Can
we find that out before we get into this time caching issues?

Yes, because that's easy to find, using binary search.

So after doing a binary search to find start and end times on the screen,
using timelevel of  0, we then can determine the color granularity. 
So then we can choose an appropriate time granularity.
From there we choose an appropriate time level and 

So let's say we start in the upper left corner of zoom and time.
That will definately have a small range of points.
Then we want to navigate down to where we need to go

(high to low)
(and a third "area" going out into your face)
    time ------------------------>
zoom
  |  
  |  
  |  
  |  
  |  
  |  
  |  
  |  
  |  
  v

As any of these increase, the number of points won't be so bad. 
And as zoom increases, area necessarily decreases. This is fixed.
Time can be anything, so we're left with a two-d plane:

          time ------------------------>
zoom area
  |    ^
  |    |
  |    |
  |    |
  |    |
  |    |
  |    |
  |    |
  |    |
  |    |
  v    |


But note this, in the database of cached points, there is no area,
so the number of points is going to be a lot bigger with more granular
time and/or zoom

Ok, so lets look at the stay at home person 

In this case, there will be a lot of tiles in time, but little in area.
So we get something like this
          time ------------------------>
    time gran. <------------------------
zoom area
  |    ^  few                       many(few on screen)
  |    |  few                       many(few on screen)
  |    |  few                       many(few on screen)
  |    |  few                       many(few on screen)
  |    |  few                       many(few on screen)
  |    |  few                       many(few on screen)
  |    |  few                       many(few on screen)
  |    |  few                       many(few on screen)
  v    |  few                       many(few on screen)

Do we need to support all levels? Always yes. We can always zoom in,
and since the area of the screen is not relavant to the database size
we will have to support all

Now lets look at the home to work and back geek.
          time ------------------------>
    time gran. <------------------------

zoom area
  |    ^  1                         many(few on screen)
  |    |  1                         many(few on screen)
  |    |  1                         many(few on screen)
  |    |  1                         many(few on screen)
  |    |  1                         many(few on screen)
  |    |  1                         many(few on screen)
  |    |  2                         many(few on screen)
  V    |  2                         many (few on screen)

There are more points when the area is zoomed in far, but
the times are split between home and work.

Now lets look at the traveler, going willy nilly everywhere
without stopping

          time ------------------------>
    time gran. <------------------------

zoom area
  |    ^  few                       few
  |    |  
  |    |  
  |    |  
  |    |  
  |    |  
  |    |  
  V    |  many(few on screen)       many(few on screen) 

but those examples are just database. 
Now we need to do screen:

          time ------------------------>
    time gran. <------------------------

zoom area
  |    ^  
  |    |  
  |    |  
  |    |  
  |    |  
  |    |  
  |    |  
  V    |  

So here are our examples

homebody
home to work
nomad

(multiplied by)

1. zoomed out, all time
2. zoomed out, specific time
3. zoomed in, all time
4. zoomed in, specific time

Our plan:

To start with no data beforehand:
1. We start from the top left corner, choose
tiles based on area, and time
2. We move in area first then time (it really
   makes no difference) to find where we want to
   settle. Since each time we move towards the right
   or down we are reducing the time period or area
   covered, then we won't run into issues.
   Actually another reason is that if we move in area
   first, we can then determine the time covered by 
   that area, and further reduce the granuals

The issue now is, how do we determine what links to
what when caching by time?



6/24

We can create links by using variable length data. Since
the data is all stored in a blob, this isn't a problem.

I wonder if we even need links. Or we need a count
of links. 

Now the other thing is the volume of data.

We are creating a cache row every time there is not
a continuous time period. is this too much?

I was thinking yesterday of combining these somehow,
but lets come up with a definition first.

Tile
id,
--(encrypted)
min lat, max lat
min lon, max lon,
start_time, 
end_time,
speed mps,
first_pos_down_fk,
first_time_down_fk, 
sibling_pos_fk,
sibling_time_fk,
gps_loc_fk

I don't think this causes a problem with colors, because
if there is a large gap and it is significant enough for
you to care, you should zoom in and combine.
Combining is not currently being done by the cache. 

So you have to ask, are two points close enough to combine?
If so, do so. Otherwise do not.

I'm still worried that there will be too many points, but
it may not be a problem. 

Ok, so how do we build this?
1.  We create the level 0 point
2.  Read a gps point
3.  Start at 0 point
4.  Am at bottom level? 
    Yes - go to 9
    No - continue
5.  Get all the children in time
6.  Is the point handled by any of them?
    Yes - add it to the child and subroutine to 4 with the child, then go to 9
    No - continue
7.  Can any of the children be modified to add the point?
    Yes - add it to the child and subroutine to 4 with the child, then go to 9
    No - continue
8.  Create new child to hold point, subroutine to 4 with child, then continue
9.  Get all children in space
10. Am at bottom level? 
    Yes - go to 1
    No - continue
11. Get all the children in time
12. Is the point handled by any of them?
    Yes - add it to the child and subroutine to 4 with the child, then go to 9
    No - continue
13. Can any of the children be modified to add the point?
    Yes - add it to the child and subroutine to 4 with the child, then go to 9
    No - continue
14. Create new child to hold point, subroutine to 4 with child, then continue


   8t, 8sq
 100t, 8sq
  10t, 8sq
   1t, 8sq   1t, 100sq

1000t, 1000sq   1000t, 100sq   1000t, 10sq   1000t, 1sq
 100t, 1000sq
  10t, 1000sq
   1t, 1000sq


ok, i can do this, pseudo code be darned! (The psuedo code above doesn't work)


GpsTrailerCacheCreator will do the stuff.

6/25/10

The db does appear to be quite big, with around 30 cache rows being added
per gps location row.

Darn, it is a lot. 
But the weird thing is this... suppose they are in the same area,
then why do we need to create another row? I mean, can't we be like,
the user just stayed in this area for that long of a period of time,
regardless if it's 10,000 hours? Of course, lower zoom levels, they
might have moved. But if they didn't, then they didn't.

So as long as it's continuous, we should never create a new row.
Yes, of course.


So the time cache

The question is, given a specific start and end time, s and e,
does the point handle it?

The question is, was the user "there"?

Now if the user was at his house all day on July 23rd but
left from 6pm - 6:05pm to chase his dog that ran out in the 
street, then, when we are looking at months, and zoomed out
to the neighborhood and we ask, what are the points for
July 23rd at 6pm through August 23rd at 6pm? What happens?

So let's see, there is the pos dimension,
and we have
Neighborhood point contains:
	     House point
	     Chasing dog points
So under infinite time, we ask what's on the screen, and
the cache returns
Neigh point:
     House point
     Chasing dog points

So then we zoom down to 1 month and we still display:
House point (which continues on even during the 5 minutes
  when the user wasn't there)
Chasing dog points

Now let's say the user was chasing the dog for 15 days,
and we look inside this area (when the user was chasing
the dog)

We would have:
House point (incorrectly)
Chasing dog point

In addition, we have no way of knowing the house point
rainbow, so we would display the user being there the
whole time and chasing the dog at the same time.

So even if we zoom out to where the house should be
visible, the rainbow for the house would be incorrect.

And that's why we bother keeping a time cache at all.
To denote the rainbow. The rainbow is like subpoints,
and we don't need to be entirely accurate with them.

So, yes, we can "cross the gap" at the appropriate
time level. So in that case, if we had a time diameter
of 1 minute, instead of 60 points being created for an
hour, we'd only create one if the user stayed in that
area for an hour. Then we'd only pull back one, and
only display one with the rainbow cut properly according
to the viewing time. If there are other points, of course
they are displayed. The only thing is with too course a
viewing time, it will appear the user is in two places
at one time.

This does *not* hold true with space. The difference
is that space is 2d. See, with time if there is p1
and p2 and p3 and p4, we know they are all in a straight
line (since it's 1d) so we don't have to worry about 
displaying an arc. But with space, they may move around in
a square, or a circle or a happy face, we just don't know.
I suppose it might be possible to choose one dimension
and lengthen the cache point according to that one dimension
or at least hold two dots instead of 1. I mean, as long
as the user stayed within a certain distance along a line
between point 1 and point 2, we could figure out that 
they were there. Huh, so that would mean we are actually
caching the lines, and not the points. And that would
work perfect for large distances. We load the tile,
are they in the line from p1 to p2? Yes, then display
it.

Great, that also shortens the cache immensly. I guess
we'll do both, since we need to worry about the 
inbetween points anyway. Hey that gets rid of the
KIND byte, awesome. That reduces it by a lot.

There is one thing that bothers me which is what if
they zoom in between the tiles. If we create a tile
between every point, then we are cool. Otherwise...

Yea, because if it's arbritrary, we are sure to miss
it. Yes, we definately need a tile per every two
points. So we can sort of link them together. And
the notion is that the points are along it. However,
that brings up the worrying notion that the points
may vary in time thickness. So I suppose it's like
a 3d line. As long as it is close enough to the 3d
line, we can keep it in the cache, but if it varies
we have to throw it out. Of course we also could
specify different types of caches if we want, where
it's not a 3d line but something else, like a
square wave sort of thing (ie. moves quickly and
then stops at the end), but this is pie in the sky.

So we have a link to the past for each item, and
that should work fine. This is because we store
the latest point, so the line goes from the previous
cache point to the current one.

However, how does that work for time blur?

Maybe we could do something like an id per item.
No it wouldn't work. We could use the variable length
stuff to do sibling pointers, though. That should work..
We could even squish it down by just having the relative
position ... ah too much. Just sibling pointers and it shouldn't
be hard, so we can wait on it.

So our new gps loc cache will have the following:

LATM,
LONM,
TIME,
SPEED_MPS,
FIRST_POS_DOWN_FK,
FIRST_TIME_DOWN_FK,
SIBLING_FK

So with time blur, having the points in a line only
work when standing still, ie. a straight up and down
line.

But with an actual distance, it doesn't work, because
we don't know where to put the standing still points
and we don't know where to put the colors.

So if there is too much time blur, and by that, I mean
any at all, we really can't do anything besides spot
in lat and lon.

Well, let's think about it, we're talking about caching
a line. That is the line is in sharp contrast, and along
that line we allow a leeway according to time blur, lat
and lon. But the point is that even if we have a large
time blur, we can't go backwards. Sure but, if you think
the new lat and lon, and new time create a line. As long
as you are close enough to the point on that line you are
fine. However, if you have infinite time, ie a large time
blur, but very tight lat and lon, it won't work, because
the colors have to be right. So a new potential point
has to be perfectly on the line, but it's time can vary 
a lot. It works fine.

On the other hand, with very sharp time and a blurry
lat and lon, we must keep the time but can vary the lat
and lon. Also works.

Now, we need to consider speed_mps. Speed_mps is the average
speed the user was traveling. Now think of how that is 
computed currently, by the speed of the last point and the 
current point. Now think of a user flying on an airplane
ride and going 1000 miles in 18 hours (a slow airplane)
Then he gets out and waits around for about 30 minutes.

If we zoom out in time and zoom out in position, what
happens? The 30 minutes of wait time disappears,
because we add it to the cache row as being on the line.
And the line travels pretty fast. However, at this 
range, big points would be days. I think it works.

I think all we have to do is calculate the speed of the next point by 
a) either the length of the current tile / time, or 
b) (the length of the current tile + the length of the next tile) / time of both

6/26

Ok, so we need the min and max of coordinates to figure out
how far a box can "move". However, this is an issue because
we need the average error.

I think that we won't be able to figure out the average error
if we move around the center easily.

Maybe we should just keep the center fixed.
We will still need to adjust the center, though. Unless you
want the center to be at the center of the cache. But I don't
think that is very good. 
Maybe what we can do is take an average of the size as time 
goes along.

So we start with a zero by zero size and then when we get the 
next point, we increase it, and the next point more so, and
so on. Or maybe the current point to the last point 
or maybe the current point to the current center.
It won't be perfect, but at least it won't be too bad.

So for this we'll need:
average diff squared
center lat, center lon


Consider this,

suppose we have 3 close together points in time and space,
and of course we create a line for that. Then we get some
wacked out point far in the future. nevermind

So should we move the center?

So to move the center... in general

first compute the 3d center of all the points of the tile. 
Then compute the point after a standardized distance for
each of the lines. Then shoot a vector through that point.
In other words, average the vectors for all points in
the future of the 3d center (or in the past, they should
be the same) no. won't work

First compute the minimum 3d non lopsided box that contains
all the points. Then figure which corners to use (???)
Then for the bottom left corner, compute the line from it
through all the points. Take the highest x and y slope 
(considering z is up and down). Do the same for the upper
right corner and choose the most up and down one. That
will define the lopsidedness.

I think that it will always be a problem, without going back
to the 3d points, we can only adjust the size, the radius
of the lopsided box upwards. We can't adjust the angle without
going back to the points.

However, that brings up a point. When there are two adjacent
measurements, there is no gap. Cause if we started on 
a plane trip, and turned on the gps when we started
flying and got a point when we started going straight and 50 miles
afterwards (and then the captain told us to turn off all electronic
devices because they're causing interference) and then we had
a point at our destination, this could all be a single cache tile.

Just something to keep in mind.

So we can't adjust the angle but we can adjust the radius in
*any direction* upwards. So in that case, we just need to
project the new point on the plane at t=0 or some thing like
that.

Then we adjust the radius to contain it.

So we need:

MIN_LATM, 
MIN_LONM,
MAX_LATM,
MAX_LONM,
LATM_PER_MS, //slope
LONM_PER_MS, 
TOTAL_DIST_FROM_CENTER, //our inaccurate method for recording fuzziness
NUM_POINTS, // total number of points added
END_TIME, // the stop time, note this may change if we add more points.
FIRST_POS_DOWN_FK,
FIRST_TIME_DOWN_FK,
SIBLING_FK


//the following is the extent of the actual points in the cache, not its
//maximum range
public static final Column MIN_LATM = new Column("MIN_LATM",Integer.class);  
public static final Column MIN_LONM = new Column("MIN_LONM",Integer.class); 
public static final Column MAX_LATM = new Column("MAX_LATM",Integer.class); 
public static final Column MAX_LONM = new Column("MAX_LONM",Integer.class); 
public static final Column LATM_PER_MS = new Column("LATM_PER_MS",Integer.class);  //slope
public static final Column LONM_PER_MS = new Column("LONM_PER_MS",Integer.class);  
public static final Column TOTAL_DIST_FROM_CENTER = new Column("TOTAL_DIST_FROM_CENTER",Integer.class);  //our inaccurate method for recording fuzziness
public static final Column NUM_POINTS = new Column("NUM_POINTS",Integer.class);  // total number of points added
public static final Column END_TIME = new Column("END_TIME",Integer.class);  // the stop time, note this may change if we add more points.
public static final Column FIRST_POS_DOWN_FK = new Column("FIRST_POS_DOWN_FK",Integer.class); 
public static final Column FIRST_TIME_DOWN_FK = new Column("FIRST_TIME_DOWN_FK",Integer.class); 
public static final Column SIBLING_FK = new Column("SIBLING_FK",Integer.class); 

We don't have a point intersecting the lopsided box, we have a segment (a capped line).
I suppose we can look for the intersection of the lopsided box through the line
and if it's within the time slop, we're good.

Ok, so we have a point in space. we convert that to a line, now we need to see 
where it intersects this lopsided box. Since we are alreay sure the point is 
within the bounds of it, we can turn the lopsided box into a thick line.
So now we have a lat vs lon type of thing with a min k and max k. However we have
the infinity problem so we can't compute the lat / lon directly for both
min and max.

So we have a point with a lat and lon. Then we have the box lat/lon plus min/max k
We can reverse these incase of infinity.
So we just need to compute k given lat and lon and our two slopes, and compare the 
two.

So lets see. There is also the case where both slopes are zero.

So there is time vs lat and time vs lon
And we have a line that strikes like lightening down through time and a specific lat and lon
So lets say we figure out the intersection between lats, like what time will the min lat
hit and what time will the max lat hit
and same thing with lon
Then we find the intersection of the range of both, and we have the answer.
However lets say the slope of lat is 0. Then there is either no time or all times,
depending on the lat point.
So if we look at the slope and its near zero, then ..

What if we just take the slopes and compare them? Given min lat and min lon, compute
the slope to the lat, and to the lon, and do the same for max lat and max lon.
If min_slope < min to point slope and max to point slope < max slope we are in.

No, that's good for a point, but won't work for a time range

What we can do is take the min and max point on the segment, take the slopes.
If both slopes are on the same side as the min slope, then the min slope doesn't
cross. Same with the max. Now if the max slopes are all less and the min slopes
are all more, it's smack in the center. 

There, thats the solution and it will work with everything.

Shoot, the only thing is the max time... you know, the time skip?
What we need to do is find the min time we actually cross the box,
and if that is too high, we exit.

Actually though, we have a beginning time to the point.. right?
And .. wait, am I thinking of this correctly?
With infinite time, we want all the points, right?
Let's say we have a day of time slop, what does that mean?

I mean, if you had a very tight lat and lon slop, and a zero slope,
the time would't mean anything. I mean you'd just ignore it
when computing lat and lon. 

Now lets say it's 12:00 July 1st and we have a very tight lat and lon position.
If we have infinity time slop and a point also at 12:00 July 1st right outside
the lat and lon, with a tiny tiny slope, do we really want to pick it up? 
No, of course not, because of the law of gradual change.

Great, so we were thinking about it wrong. That should simplify things.
We have a point again. We need to figure out where it intersects the 
lopsided box, with the minimum time. Hold on, we have a time start and
time end. So we have an end time of the lopsided box, and a begin
time of the point. If they cross the slop, then great. 
The point is that the time slop is really a time jump. Do we accept 
points this far in the future. Regardless of whether we can include it
based on lat and lon, we only want the point if it within a certain
distance in time. So that simplifies that.

So given all that, we simply check the begin time to see if it covers
the time slop. If not exit.
Otherwise, we go on and find where the point exists given the slope.
If it's inside, we are golden. So, we can again go back to the easy
method of finding the time slice and examining the lat and lon. Whew.

6/27

What about the problem where we have a tile inside another tile, but
they go off in different directions so eventually, the child tile
won't be used again because it moves to position that is outside
of where the parent is?

Do we need/ should we force the slope of the children to match that of the
parent?

We can't let the child "escape" the parent, I think...
The thing is that when we ask for the nodes inside a lopsided box, they
better be all in the lopsided box. Because if a child starts extending
into another box, we're kind of messed.

So the thing is though, that since we have a lopsided box, we 
now have a start and end lat, lon with points extending out of it.
I mean, it's no problem if we create a max limit in lat and lon so
we can actually use it. But another thing is that we have to understand
that if we have a point in Buenos Aires and the next one in Rome, we 
must know about the one in Rome, even if we are zoomed in Buenos Aires
so we can draw the line correctly. And vice versa.

Which is conflicting. We both need and don't want points outside the box.

Remember that even in the case of super time slop, we still want this.

So lets take a look

We have cachepoint 1
and lets say we have a bunch of points in BA, under cache point 2
then we hit it with a point to rome. Cache point 2 says no fricking way
and so we create cache point 3. But, wait, cache 3 extends into cache
2's territory. But that's ok. We load them both. However, then
we have cache 3 children. What happens there? Cache 3 children play around
in rome, so we don't load them because they don't extend in to cache 2
territory. And we still have the big cache 3 which extends into both.
So the point is it is necessary that caches overlap.

So is there a problem? I don't think so, because the italy to rome would
never be a child of cache 2 because it couldn't contain it. A cache
3 sibling would be created to handle it and be placed from BA to Rome.
So suppose the user one day goes back to BA and starts playing around.
Lets say they go to Rome and then come back to BA. Would cache 3 hold it?
Absolutely not! Because cache 3 is a one direction entity. So it would say
no fricken way, I'm a Rome guy now, I used to be a BA guy, but I moved there
and can keep going to russia or something (along the line) but not back. 
So we would create cache 4, and yes we should.
Then when we query, we would get cache 2, cache 3, and cache 4, and we need 
to. 

Of course, let's say they came back really fast. What would happen? Cache 3
wouldn't take it, because it's now a Rome guy. But cache 2, considering it's
not jumping off in some direction, would take it. However, the intersting thing
is that if they jumped back, a cache 4 would be created for that. Which is
good, because we need to know they jumped back at X time. Wait, would it?
No, I don't think so, if the jump was fast enough, it would get sucked up
by cache 2 and a cache 4 would not be created. And remember, the idea of the
time jump is that anywhere within it, the colors are the same. So we 
have going away, but not coming back then? Is that a caveat we can live with?

It seems like a bug. This whole thing is bizzare. It's hard to see how it makes
sense.

The other option (maybe) is to get rid of slope, and then we need to have
inbetween points again..

It seems like if we want to keep going this way, we need to consider both
the start and the end points to decide whether something can be added to a 
cache. But then lets say the user went back and forth to work a lot,
what would happen? We would create hundreds of cache points that would 
*always* be needed to be displayed. One per day.

Darn.

But I thought that we had some other way of connecting time slop caches.
Based on being next to each other. Or some linkage or something. I don't
think caches should be a replacement for actual links. Also think,
suppose that cache 2(home) and cache 4(work) already existed and had
more than a couple days time slop. What would happen is that cache 2
would take home, and cache 4 would take work. And that's the way it should
be. Cache 3 is an anomally, really. Cache 2 and cache 3 would have hundreds
of points. Cache 3 only one. (cache 3 goes from home to work). After the
work guy has been established, cache 4 becomes kind of irrellavant.

But look, what is likely to happen in work and home scenario is there
is really work, home and road.
We start at home, goof around, take the road with one or more straight
shots, then go to work and goof around. Then go back. With a large amount
of time slop, each of these are constant big caches. But eventually
we get to the point, under a day or so, that each time a new cache
needs to be created. And that's true for a day all the way down to the
minimum cache level. So we have repeating data. No good.

So I believe the key was the idea that if they are in the same area after
the time period expired we don't need to create another row. Which works
when they are resting. But if they keep tracing over the same ground day
after day, we are messed, because if they choose infinite time we will
get their daily pattern, over and over again. Because we can keep track
of *time ranges* but not repeating time instances. I mean, it will only go
up to a maximum, since there are only so many trips you can do in a month
(where the time blurness is not enough to cache the data) 
but I think it's high.  It all depends on the time slop used, however.
I mean the user isn't probably going to notice the slight difference
between colors. 

But that brings up a point, if we have a connection can we combine
back and forth? I mean, we won't be able to tell the difference between
the user going away and towards home for a big time slop, so maybe
we can let the slope "reflect" back the way it came? I don't see why not.
It's a perf thing, but we might want to do that.
(hey so is the whole cache!)

So I think the point of all this is that a child can be the child of
two parents?

Which makes the sibling link kind of messed.

Right, I mean that's what we have to do. What if we just duped the child?

Wait one second.. we have this problem... it's a known problem, that 
we don't like time blur elements. But this is not that. This can
occur even with no time blur. With no time blur, it's easy, because
there is always a trail through time. So we can place the item in
the upper tile from the past and link to it from the future. 
So the last sibling link will link to the past. It furthers the
trail outside of the parent. But what if it crosses multiple boundaries?
We have to deal with the fact that the parents can be smaller than the 
children. 
Like if you are staring at the atlantic ocean, there will be no upper
level cache to handle it. However, wait doesn't the upper level cache
grow? Yes it does.
I think I was down this path before. The child can never escape the parent.
Parents can overlap, yes, but a child always has a parent covering it's entire
space. 

So in that case, we will always have a parent, and we don't need to link
a child sibling. 

I'd like to come up with some facts about this system, so I don't get confused
in the future, because it is a very hard thing to wrap my mind around.

Lets see:

FACT: Retracing the same steps is a little bit of an issue, but relaxing time blur will fix this (also search for "reflect")
FACT: linking between two time blurs right next to each other may not be complete
FACT: Children are always covered by their parents in space-time. Period! The children will never receive a point that their parents cannot handle.

The only outstanding issue is the problem where the large cache gets a 
crappy slope because of two nearby points after a short time period. 

Maybe we should lessen the slope for space blur items. I'd like to adjust
the slope, actually. But I think I thought this wouldn't work. Is it true?
The only issue I see is 1) overlap, 2) gaps forming.

No.. I don't think the gap issue is real. I can't see how a gap would form,
since we always need to keep the same points. We are just adjusting what
would happen in the future.

Gosh this is complex!

I'm worried I'm overlooking something. 

So if we look at the children, can we adjust the slope?
We are basically asking for what we need to ask for when displaying.
What points do we display for a particular time box, in 3d space-time?

That's the big point, as long as we can contain the same points that
we did before, we definitely won't be hurting anything, right?
Yes, because even if we create gaps, they won't be important, because
there will always be another cache covering the line between points
from inside and outside the cache.

So the thing is, we just go through the immediate children, and as long
as we still cover them, we change change our slope to whatever we like 
to handle a new point. In this way, we won't have to worry about crazy
slope angles on large area caches

So given that, what do we need to do.

We need to find the best slope to contain all the points, but we don't
need to be anal, since the slope isn't even an issue. In fact, we don't
really even need to record it. It's not important (except to define the
current area)

But wait, we don't even need it, really. We can define the current area
as a 3d box. Right? or no? 

The ui will ask for a 3d area of time and space with a time blur.

FACT: Time blur levels are simply entirely different caches with different rules for which cache box handles a particular time.

So we will be given a min and max corner. We need to find all intersecting
caches. So yes a cache that is a lopsided box can slide under the 3d box the
UI wants and doesn't need to be loaded. Is this important? I'm not sure.

____________
|          |  <-- ui box
|          |
|          |
|          |    ----  
|          |   /   /
------------  /   /
             /   /   <-- cache
            /   /
           /   /
          /   /
         /   /
        -----     
        

So in this case, knowing the slope helps a lot in lowering the caches we have
to deal with.

So, yes we will have to deal with slope, and record it.

Now back to the fun stuff. How to choose the ideal slope for the minimum size,
given that we have access to all the immediate children (which are also sloped
boxes)

Oh gosh, that sounds complicated.

I wonder if we can get away with no slope?
Well, the problem is it ruins one cache for two points that are far
apart like a trip from BA to Rome if we don't allow it.

We can ignore it for some purposes, and keep it for others, I suppose.

FACT: The slope is only meant as a kind of compression so that we can 
display a lot of points easily. It allows us to increase the number
of points handled by the cache tile (especially for BA to Rome)

So considering that, we could possibly replace these sloped boxes with 
3d cubes that are picky about what points they accept. They just overlap
with each other sometimes.

So the only reason to go to the children is to be able to display the
point as part of the upper level, (or to improve the display, make it 
sharper or something... not sure we care too much)

In which case, we almost need to add it to the children first in some
reversible transaction. Then we can go to the parent and see if we can
handle the children as they are. But sloped boxes, my gosh what a pain.

Can we use a min max sort of thing? I mean it won't be perfect, we
may not get the smallest area, but think about it.. well wait.

This seems like a classic problem. How to fit a line to data. Only
we are doing it in 3d.

In the future, we may want to do curved least squares of higher dimensions.

--

So this page shows how to compute it for 2d, we'll just do it two
times for both dimensions, lat and lon, it's perfect.

http://en.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares

We just now have to think about that the items inside are sloped boxes
as well. 

And by doing this, we are just reducing the number of caches on our
level by accepting a point that would otherwise take another cache
to generate. 

FACT: The idea behind sloped boxes to reduce the number of caches
on the current level. The more points we can accept, the less caches
we have to deal with.

Ok so we have an equation to deal with points. What about boxes?
Well every sloped box is facing the same way, so we are really talking
about lines. And this is to determine the best slope, not the extent
of the box. I mean hmm. I was thinking in terms of points before.

So lets make it easier and reduce the inner cache tiles to boxes
that contain the sloped box.

In that case, we can just grab the center of the box. And we can
use the borders of the box to determine if we have fit.
The question I have is whether we can simply use both the min and
max squares to determine if we have fit.

But wait, let's suppose we have some very long stretched out
sub tiles. Let's say we have two of them. Then we would be
pointing the big box in the exactly wrong dimension if we didn't
take into account the ends.
I think that, yes we can just use the ends of the inner boxes to
determine the minimum size necessary for the big box.
Also, since they are a square, we can use the center point of
each square to determine the best line for the big box.
Actually I'm not sure about that. But otherwise we need to do
math and that is a bad thing.

We could use both... I think we'll just deal with the fact
that our line fitting is not perfect.

The thing is that we may not be able to deal with all the
children in memory at once. I suppose that is ok.
I mean there could be a lot of children. If you think
about it, only certain children change, I wonder...

We can do everything for the slope with the four sums
sum(x), sum(x^2), sum(y), sum(xy)
So if we keep these in the parent, we won't need
to go through the children in order to recalculate
the slope. In fact, with these, we won't need the
slope cached anymore. We just need a height for the 
3d box. 

So we currently have:

MIN_LATM, 
MIN_LONM,
HEIGHT_LATM,
WIDTH_LONM,
LATM_PER_MS, 
LONM_PER_MS, 
TOTAL_DIST_FROM_CENTER_SQR, 
NUM_POINTS, 
END_TIME, 
FIRST_POS_DOWN_FK,
FIRST_TIME_DOWN_FK,
SIBLING_FK

we'd change it to:

MIN_LATM, 
MIN_LONM,
HEIGHT_LATM,
WIDTH_LONM,
MIN_TIME,
MAX_TIME,
SUM_LATM,
SUM_LATM_SQR,
SUM_LONM,
SUM_LONM_SQR,
SUM_TIME,
SUM_TIME_X_LATM,
SUM_TIME_X_LONM,
NUM_POINTS, 
FIRST_POS_DOWN_FK,
FIRST_TIME_DOWN_FK,
SIBLING_FK

So we increasing the size of each cache point by 1.5...

Hmm the thing is though that we are using first down
and sibling, so we have to go through each cache point anyway
in the ui, so if there are soooo many points we'll have a 
problem, we need to deal with it.

So, lets see, are there too many points per cache tile ever?

I mean let's consider the commute (home, road, work)

In that case we can have infinity sub points. Yes, because
the high time slop cache (parent) can go on forever as long
as the jump isn't too big. But the children will keep 
increasing because their parameters are under the jump.
Darn.
We could have a maximum number of points per super tile
and just refuse to add points beyond this. That
may be a good idea. Otherwise, unless we do some
sort of sorted map, we won't be able to pick and choose.
And anyway, we'd have to store all this extra data to
determine the slope. I think we may want to cache the
slope anyway in some form. This is because we don't want
to have to hit the children in the UI just to display the
parent.

So then, what we want is:

MIN_LATM, 
MIN_LONM,
HEIGHT_LATM,
WIDTH_LONM,
LATM_PER_MS, 
LONM_PER_MS, 
TOTAL_DIST_FROM_CENTER_SQR, 
NUM_POINTS, 
END_TIME, 
FIRST_POS_DOWN_FK,
FIRST_TIME_DOWN_FK,
SIBLING_FK

But no start time?
No, it doesn't make sense.
We need a start to display it properly. We really can't
get it from anywhere else because the previous cache may
be under another parent.

So we need to add start time to this.


And now that we can limit the number of children, we can
also addPoints at once, rather than separately.

FACT: There is a hard limit to the number of children 
a parent cache can have

I think we had a rule before where we always added to the earliest point
first, to prevent overlap. But I think that we can prevent
overlap anyway.

Huh, we don't really want the best fit, actually. We want the
one which can contain the points in the smallest area.
So if there are 100 points all in the same spot and 1 outlyer
we need to split the middle.

which means that this may be harder or a lot simplier. Think
if we found the absolute edges considering the upper and lower
square of the lopsided boxes of the children caches, then 
we found the center, let's say. 

Then let's find the min slope of each point to the 4 corners
(each corner of each top and bottom square of the lopsided
boxes). But there are two slopes?
Right, and we need the minimum of both. 

No no no,

the size of the box may extend beyond the edge point of the sub
boxes.

After some consideration we have determined that it is difficult
to do this. Consider a circle with a slight dent.. Somehow we'd
have to find this dent and in fact, it may not even be the smallest
area. 

So we will just take the least squares, call it the slope and be done
with it.

I wonder if we should cache the lines instead of the points.

In this way, consider, home, work
cache 2 - home
cache 3 - home-to-work
cache 4 - work
cache 5 - work-to-home (since start and end both must fit)
cache 6 - home

That would probably be best because it makes most logical
sense.

what if we separated everything into plates.

What if we went like this. Start at the first point
It is considered the "pivot"
Then we go through the rest (consider the top slope at first)

We find the highest and lowest slope starting from the "pivot" (1), 
and find the distance we would need for that slope.


Maybe the least squared thing isn't so bad, if we have a bunch
of points in more or less a line, then we try to fit another point
that doesn't fall along the line, it might be better to reject it
so it looks better on the gui. Size of the distance slop may not
be the only consideration when determining if a point can fit in
the cache box. 

FACT: Consider:

                                                                   
       * <- point to be added                                     
                                                                   
                                                                   
                                                                   
                                                                   
                                                                   
                                                                   
                                                                   
                                                                  
                                                                   
                                                                   
       
    ***                                                         
 ***                                                               


Yes, we could flatten the slope to include this last point,
but it doesn't really follow the data.


What about first time fk.

Remember that these are completely different levels and have no
bearing between each other. In fact, beyond the zeroth level,
why do we even have a time fk link?
So in other words, why don't we have an index saying this
is the time level and wash our hands of the whole time thing?

We may want to do that... it could simplify other things

ugh, so now we have the issue that the bottom cache level
doesn't have lopsided boxes inside of it but actual
points. How do we even store these? I mean we could
have a sibling link in the actual gps location, I guess.
Bastardizing that form.. I suppose.
No we can't, because there are multiple caches (one
per time level)

So in reality, we need to have the bottom cache level
store a single point. And it will be 5 x the data.
Darn.

But wait, is there another way? Is there a way out of this?
Not if we change the slope. Anytime the slope changes, guess
what? The whole cache needs to be recalculated. But hold
on, suppose we didn't need to recalculate the cache. 

See the thing is, ok we choose a time level, and that
is all well and good. But then we choose a zoom level.
Now, we can always choose an up close and personal
zoom level, so we will always need the up close and 
personal zoom, regardless if the slope changes or not.
Which means we are always going down to the zero'th 
level. We are always adding a new point for the zero'th
level, and we are never "swallowing" the data. 
When a point is accepted warmly to a cache, it means
that another cache doesn't have to be added *for that
level*. However, for another level, we are going through
that same procedure again. But even at the bottom level,
it is true that we can "swallow the point".

But let's think about this another way... the bottom level
have no children, so let's just say the slope can't change
at the bottom level. It would be very odd for the bottom
level to change it's slope, anyway. It would need 
near perfect points.

Ok so there is a problem with limiting points. If we limit
points with infinity time, we will get infinity duplicates.
Not a problem except that we have to go through all of them
to find out what we need. Furthermore, this will generate
even more data that we would have to look at in the UI level.

Ok, lets determine what we mean by infinity, because it matters
a lot. We can display infinity time, but we never have infinite
cache, right?

This is a hard issue. We can always have the situation where
1) at home we spend 14 hours
2) on the road we spend 1 hour x 2
3) at work we spend 8

So for home we have a time gap of 10, for work a time gap of 16 and
for the road, a time gap of 22.

The problem then is that for a given time level, lets say with a
time jump of 18 hours, we keep on creating road caches, but we 
keep the same home and work cache. So from the top level,
we now have infinte road caches to go through.

But isn't this always a problem? I mean suppose we have zero 
gap, we still have infinite road caches to go through because
we got rid of the time link.

But I think I'm missing something. I mean the time link.
No I'm not, because we only have one sibling. 

But wait, zero gap is not a problem, now is it. The reason
is that the parent will have split out the road caches to
go through, ideally it's like this:

--|---
  | a
b | t1
i |
g |
( |
t |---
1,| a
2,| t2
3 |
) |
  |
  |---
  | a
  | t3
  |
  |
--|---

The problem is that if they travel back and forth over the
same ground, we get infinite time caches over where they
traveled. 

We can't just split up big according to time, because we
may want infinite time. 

Now here is the thing, the reason we have these guys here
is because of the rainbow. So as soon as the rainbow
gets too compressed, we should increase the time jump enough
to handle it.

But think, if we define the upper and lower bounds ... ok 
wait.

So we start at infinite time. Or maybe at a certain time limit
.. look  we have the road, if we run into the situation where
the time gap is such... Ah, so the problem is not that
we will have infinite roads, it's that we are considering there
being a maximum time jump. One that we don't cross. Because
as we increase the amount of time, we also must necessarily 
increase the time zone levels. Unlike zoom, time keeps getting
bigger. So let's take a look at the ideal case again

FACT: the way it should look:

--|---
b | a1
i | t1
g |---
1 | a2
( | t2
t |---
1,| a3
2,| t3
3 |
) |
--|---------
b | a1|a1|a1
i | t4|t5|t6
g |---------
2 | a2
( | t5
t |---------
4,| a3
5,| t6
6 |
) |
--|----------

So in essence, we still have a point count. The thing is that
we don't want to have a maximum time zoom that is too low.
Otherwise, that is where infinity tiles come into play.

Now I wonder about the time sorting stuff again. Well
no it won't work, because we'd have all tiles in the
proper direction but there is nothing to prevent 
t1 being in a1 and t2 being in a2 and then t3
going back to a1 (when we're trying to view a1)
So there would be gaps. 



Ok so what we need to do is see if the parent can
read the data, and if so go to its child. If not
we create a parent to add. We do this for each
time level.

7/1

So, we have two scenarios, 

1. zoom
2. pan

For panning, we need to keep the same points and
add new ones. It's true that if we run out of cache
we'll just display *some* of the total needed points
so if we run out of cache, so be it.

Well sort of.. I suppose that what, we'll have a list
of cache points that we need to display. We can keep
them hardcoded in memory, or we can tell the cache that
this is what we want to display. 

So what do we do? Tell the cache, we want to display "X"
and ask the cache for the points? Does the cache store
them separately (memory hog) or just return them when asked?
Maybe we can ask for only cache entries? So we can call
it twice? Once to actually pull the data and the second to
just grab from the cache? Sounds silly. 

But the thing is, we don't want the thread that draws the
graph to block on the db. Right. I mean lets think, we
need to redraw. We have

1. screen
2. overlay buffer

should we have
3. overlay buffer2 <--for drawing

Maybe we are overthinking this and should just do the least
possible for now to see how it looks. Then we could even stick
it back on our phone for once.

So in that case, we could just throw everything into the drawer
just to see how it works (or doesn't.) It's code we can reuse,
because, Isabel, we need to draw.

I think that we need get the data twice. 

7/3 

For the autozoom button

We need to keep going down until is reasonable. What does that mean?
It means that the times of the items are within a reasonable distance
of the total time. Now, there is no go up, we do the following
Go down. Get extent and max/min time. If it's with a reasonable distance
We are ok.. Wait, it's more than that.
Everytime we go down, we see if each row is a reasonable distance. If
any row exceeds that (and we're not at the bottom, we keep going down)
.

Now eventually we are going to have to worry about splitting cache rows.
Because we can't display anything before a certain time. Hmm.. maybe it's
like this, we take the first row, then split it.
Then determine the lat / lon area. If it's small enough for the granualarity,
we go down to the next level.

So then we go down more.

So in other words, we find the extent of one zoom level. If it's too granular
we go down and find the next extent.
In other words, let's say we are at zoom level one and the time limit makes

so lets see,
we start at zoom level 1. Go through all siblings. Find the extent 
*based on the time*.

  
Darn, we have a problem. If we are looking at a really tight

7/4

Ok, so we want multiple versions of the app, because we want one working one
while we debug another. The issue is that we need to change the package every
time. If we also have a free version vs a normal version, we are going to
hit issues, too. I think we can deal with that later, however.

So to do this, we could change the package of all of the classes to gps2.
That would be good, because then we could "graduate" things to gps
It's also good to have practice doing this.

7/6

So the problem is that some wacky stuff is happening.

1. Everything from argentina is not being displayed.
2. It's really slow when zoomed out.

It's really hard to figure out what is going on because the
data is encrypted. Maybe I should just print it all out in a table.
Just decrypt every row and print it out. Then I can see the links...
And figure out where the heck are the argentina rows.


Ok.. so for the cache... let's see. we have x additions which 
should lead to x subtractions. So after 1024 additions we
should get the subtraction.. right?
If a point was added to the cache, then only after 1024 additions
should we have to worry about it being removed. So anything less
than that...

53634

I wonder if we can have dynamic zoom levels. I think I thought of this
before. But the idea is to create an extra zoom level if there are too
many points to help split them up.

ok, so we have the following scenarios

true:
s1 < s2 and e1 < e2 
s1 < s2 and e1 > e2
1 straddle

so if s2 is -179 and s1 is 179, then what?

7/7

Now, we have to deal with these weird items that are there, like to africa???
Maybe we should print it out... We need to find the point it's based on.

That may be a pain. Let's print the stuff out, then we can find the id while we are 
thinking. So, anyway we need to find the point. What if we print the x,y coords
and the id? Then we can see the data. So if we print the spacetime box,
the items? But what about them disappearing when they shouldn't. What if we
print out the ids of what we are displaying in an easy format to find which
ones are missing. Then we analyse the particular ones that are missing.

TODO 2:
So there are rows such as these with a bizarre minLon:

D/F5OOHACK( 6567): GpsLocCacheRow(startTimeMs=2010-06-20 00:34:29.264,endTimeMs=2010-07-05 21:19:34.787,minLatStart=       22.5742454529,minLatEnd=       31.7837924957,minLonStart=    -2147.4836425781,minLonEnd=       35.9893417358,slopeLatm=       -0.0027345908,slopeLonm=        0.3628682196,id=85385,ftdfk=-2147483648,fpdfk=85511,sfk=77237)

TODO 2:
Ok, there are some cases where the map overlaps on the screen, which makes the start and stop lan wrap the world. We will need to solve that issue by allowing all lats and lons.
         
TODO 2: We are still displaying the whole stbox even when we have a smaller time period in spacetimebox than the cacherow (ie. cache row goes from 1 AD to 2000AD and we 
are only displaying 1997)

853850 is the one, and Yulin is the place.


end center lonm: 113290176

getMaxLonm() is minLatm + heightlatm = 28763214 + 3020578 = 31783792
endTime = 1277138595429
startTime = 1276965269264
delta time = 1277138595429 - 1276965269264 = 173326165
slope is 0.4424011

getMaxLatm is 0.4424011 * 173326165 + 31783792 = 108463478.054781

getCenterLonm is 

midpoint is 30273503, 


Yea, fixed.

Ok so we have the issue of how many points will cause problems. So we need to add fake points. Maybe what we should do
is alter the Cache Creator to creat the last 5000 points over and over again, adjusting the locations kind of randomly within
20 feet or so and see what happens... Maybe todays would be good, because we remember it and it involves travel.

Ok, I did that (if there aren't any bugs). Now what? Lets see. The issue is that
over infinity time, there will be infinity points, lets read our notes.

Ok, so we need to figure out if this is going to work or not.

I suppose that in every area, there is only so many points. 
In other words, under infinity time, it will be ok. Because, we will just
fill it up.

Under non-infinity time, as long as the gap isn't too big, big will be just
like infinity time. But the point is that a might be smaller.

So wait, let's say we are looking at 10 years. Then the gap disappears.
But lets say we are looking at a month. What if big is too big?
Well, let's say that we have a point limit. Then big can't get too big.
But there is still an issue, because biggest will get too big. So it
depends on zoom levels. 

Because, look. Let's say one month. Of course there is only one root.
So what happens?
We have big, that would jump over the gap. So then we have a child of big
that *doesn't* jump over it.

So what happens? We add only so many points to big, and create big2.
Now big2 only has so many points, so we add big3, and so forth and so forth
until we have 1000 big points. Then what do we do? Create a bbig2 to hold
the big. So if we get to the top.. to row 0, guess what? We can have more than
1000 bigs in it. Because we'd have overlapping bigs, because of the point count
limitation. Now where this all starts depends on the jump situation. 
So it depends on how long the persons commute is. Now what is the problem, then?
Suppose the commute is 50 miles, what does that do to us?

50 miles, lets say 100 km instead. So thats zoom level 3. And remember, it's 
diameter. So a zoom level 3 should be able to hold everything. 
So every day the person does their commute. So after 1000 days (around 3 years)
we just overcome the limit. Well, wait. Zoom level 4 must be 10 km. 
So that means we go over 5 of them every day. 10 actually.
So after 100 days, we hit the limit. Which means that there will be 10 zoom level
3's in zoom level 2.
 
No problem.

Now I always wondered, should we have dynamic zoom levels? What about dynamic time
levels?

It sounds like an elephant swatting a fly. I mean we have plenty of good data. We
are creating the one thing I am worried about to the extreme, so if that works, 
we are good.

So lets get to the big thing, then. 

Displaying this stuff.

So there is one issue rumbling around my head.

No we figured it out. But lets come up with a plan...

So let's see. 

We have a thread that produces a new background.
We have the current one. Now, the point here is that
it's reasonably fast to draw right now. I don't want to rely on that,
but... I mean we need a cache of what were drawing.
Then we build a new one. And the reason is incase they zoom, we can
display something cool. Or we could scale the bitmap directly, but
it seems dumb. And what if they zoom out. It's almost as if we could
make some marker that processing isn't finished. Like a grayish screen
Or something, so the user can know when to start processing. 
But I don't want the thing to flash like crazy. It could slowly turn
gray, that may work. So slip into grayness and slip back.

Just make the whole screen darker would be best to avoid complaints
by colorblind people. 

So we make the screen turn darker and than lighter, fine. That we can
do later, because it doesn't affect the main functionality.
So for dots containing multiple colors, we are thinking of a spiral
gradiant effect. I don't even know if that is possible. It probably won't be
cheap, and it will be too small to see, I would think, unless there is plenty
of alpha, which there would be. Well think of it as pie slices. We probably
can do pie slices, I would think. If we have enough colors in the pie slices,
they will look like gradiants. besides, we have time. we'll use a background.

Now we have two things that are represented by size, the speed of the user and
the fuzziness of the spot. Well, actually no, because regardless of the time zoom
the fuzziness of the spot remains the same. And that should be at an imperceptable
level. So we don't really need alpha for it. Although, yes we do, because we are
going to create one fuzzy spot for a large number of cache points. 

So the issue there is that for fuzzy spots, they will be big and fat, because
a large amount of time was spent there. But then big and fat usually means that
the user "stopped" there. However, there is a difference between fuzzy big and
fat and non fuzzy. Now, we don't need to simplify it on the screen, we just need
to add alpha. Because points show through each other. Which makes me wonder. 
Because we would rather have an organic way of displaying it then pie slices.

But because we don't know where the points overlap, we don't know the fuzziness
needed for each point. But if we went through and counted the overlaps, we would.
So in other words, we could set the alpha according to how many overlaps the point
had, and that would take care of the fuzziness. Now if a point by itself represented
many times, well... now think, that a gps cache point is actually a line. So the 
line represents many time points, and has a direction (in most, but not all cases)
So we need to do a good job for both. We could have a spiral jitter. I mean we keep
going in circles while we follow the line. Or I mean that we go in a circle until
we reach where we started. But that would look funny for actual travel since the
line would be slightly curved. And even if we got that right, it would look like
some kind of weird ribbon. 

Would it really be so horrible to have thick lines.. it doesn't matter, because
we can have a zero direction point. So what if after a certain distance, we 
create an "inbetween" point. Then we could follow the logic. Only the issue there
is we'd still need pie. Well, we could say that if a point has enough *distance*
we create inbetween points. But otherwise, we use pie. Then we still use alpha, too.
A big enough mess that it may work. The only problem is yellow alphed with blue
is green! But I don't not want to use alpha, because it would be super ugly, like
party confetti.


1. screen becomes gray during "thinking" moments
2. most likely use a background buffer
3. pie slices for multiple times.


7/8


id=85385 d> 85511 d> 96489 (nmn) -> 95363 (nmn) -> 94345 (nmn) -> id=93914 (nmn) 
-> id=93666 (nmn) -> 


Here is the problem for the crazy slope:
GpsLocationRow(timeMs=2010-06-11 01:35:26.582,latm=       31.9474277496,lonm=       35.9350509644,id=3625)
GpsLocationRow(timeMs=2010-06-11 01:35:26.655,latm=       31.9474277496,lonm=       35.9350509644,id=3626)

7/9

Ok so we have a graph.. I wonder if we have overflow.

D/GpsTrailer( 2952): c GpsLocCacheRow(id=      1663,dt=  292714,st=14052816,mlatm=   -3559,mlonm=   -1868)
D/GpsTrailer( 2952): c GpsLocCacheRow(id=      1636,dt=  356434,st=13696382,mlatm=    -426,mlonm=   -3541)
D/GpsTrailer( 2952): c GpsLocCacheRow(id=      1564,dt=  715412,st=12980970,mlatm=     928,mlonm=   -1304)
D/GpsTrailer( 2952): c GpsLocCacheRow(id=      1511,dt=  600527,st=12380443,mlatm=    1756,mlonm=    3440)
D/GpsTrailer( 2952): c GpsLocCacheRow(id=      1465,dt=12380443,st=       0,mlatm=    2102,mlonm=    1040)


D/GpsTrailer( 3848): cx GpsLocCacheRow(startTimeMs=2010-06-21 21:26:56.287,endTimeMs=2010-06-21 21:31:49.001,minLatStart=       22.2955360413,minLatEnd=       22.2955360413,minLonStart=      114.1735687256,minLonEnd=      114.1735687256,slopeLatm=        0.0065627201,slopeLonm=       -0.0058282143,id=1663,ftdfk=-2147483648,fpdfk=1662,sfk=1636)
D/GpsTrailer( 3848): cx GpsLocCacheRow(startTimeMs=2010-06-21 21:20:59.853,endTimeMs=2010-06-21 21:26:56.287,minLatStart=       22.2986679077,minLatEnd=       22.2986679077,minLonStart=      114.1718902588,minLonEnd=      114.1718902588,slopeLatm=       -0.0087898457,slopeLonm=        0.0046937163,id=1636,ftdfk=-2147483648,fpdfk=1635,sfk=1564)
D/GpsTrailer( 3848): cx GpsLocCacheRow(startTimeMs=2010-06-21 21:09:04.441,endTimeMs=2010-06-21 21:20:59.853,minLatStart=       22.3000221252,minLatEnd=       22.3004226685,minLonStart=      114.1741256714,minLonEnd=      114.1748657227,slopeLatm=       -0.0019683011,slopeLonm=       -0.0041560587,id=1564,ftdfk=-2147483648,fpdfk=1609,sfk=1511)
D/GpsTrailer( 3848): cx GpsLocCacheRow(startTimeMs=2010-06-21 20:59:03.914,endTimeMs=2010-06-21 21:09:04.441,minLatStart=       22.3008499146,minLatEnd=       22.3010597229,minLonStart=      114.1788711548,minLonEnd=      114.1790542603,slopeLatm=       -0.0013616333,slopeLonm=       -0.0079346085,id=1511,ftdfk=-2147483648,fpdfk=1545,sfk=1465)
D/GpsTrailer( 3848): cx GpsLocCacheRow(startTimeMs=2010-06-21 17:32:43.471,endTimeMs=2010-06-21 20:59:03.914,minLatStart=       22.3011951447,minLatEnd=       22.3013153076,minLonStart=      114.1764755249,minLonEnd=      114.1768875122,slopeLatm=       -0.0000294600,slopeLonm=        0.0001698983,id=1465,ftdfk=-2147483648,fpdfk=1484,sfk=-2147483648)

GpsLocationRow(timeMs=2010-06-21 21:31:49.001,latm=       22.2974567413,lonm=      114.1718597412,id=5069)
GpsLocationRow(timeMs=2010-06-21 21:54:04.833,latm=       22.2981357574,lonm=      114.1725234985,id=5070)

114171858 

Log.d(GpsTrailer.TAG, "c "+child.toString(getLong(START_TIME), getInt(MIN_LATM), getInt(MIN_LONM)));


So we still have disappearing points...


The problem appears to be that points not inside the super cache are labeled to it.


GpsLocCacheRow(startTimeMs=1970-01-01 08:00:00.000,endTimeMs=292278994-08-17 15:12:55.807,minLatStart=    -2147.4836425781,minLatEnd=       -0.0000010000,minLonStart=    -2147.4836425781,minLonEnd=       -0.0000010000,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=3,ftdfk=4,fpdfk=84916,sfk=-2147483648)
GpsLocCacheRow(startTimeMs=2010-05-10 23:30:34.950,endTimeMs=2010-06-14 03:46:44.484,minLatStart=       39.6527786255,minLatEnd=       41.7707481384,minLonStart=        4.0589118004,minLonEnd=       12.3493518829,slopeLatm=       -0.0034637568,slopeLonm=        0.0093739880,id=18770,ftdfk=-2147483648,fpdfk=65481,sfk=18194)
GpsLocCacheRow(startTimeMs=2010-05-10 23:30:34.950,endTimeMs=2010-05-24 21:12:15.141,minLatStart=       41.6975402832,minLatEnd=       41.7707481384,minLonStart=       12.2401781082,minLonEnd=       12.3493518829,slopeLatm=       -0.0000328160,slopeLonm=       -0.0000584053,id=18769,ftdfk=-2147483648,fpdfk=25387,sfk=-2147483648)
** GpsLocCacheRow(startTimeMs=2010-05-14 02:36:31.950,endTimeMs=2010-05-14 03:13:14.418,minLatStart=       41.8128204346,minLatEnd=       41.8449783325,minLonStart=       12.4021415710,minLonEnd=       12.4718561172,slopeLatm=       -0.0519050546,slopeLonm=       -0.0899907127,id=21223,ftdfk=-2147483648,fpdfk=21292,sfk=20028)
GpsLocCacheRow(startTimeMs=2010-05-14 02:36:31.950,endTimeMs=2010-05-14 02:56:57.714,minLatStart=       41.8347854614,minLatEnd=       41.8396949768,minLonStart=       12.4629821777,minLonEnd=       12.4630117416,slopeLatm=       -0.0738322586,slopeLonm=       -0.1396246552,id=21222,ftdfk=-2147483648,fpdfk=21262,sfk=-2147483648)
GpsLocCacheRow(startTimeMs=2010-05-14 02:36:31.950,endTimeMs=2010-05-14 02:46:47.324,minLatStart=       41.8396759033,minLatEnd=       41.8396759033,minLonStart=       12.4629812241,minLonEnd=       12.4629812241,slopeLatm=       -0.0817844123,slopeLonm=       -0.1395736635,id=21221,ftdfk=-2147483648,fpdfk=21220,sfk=-2147483648)
GpsLocCacheRow(startTimeMs=2010-05-14 02:36:31.950,endTimeMs=2010-05-14 02:46:47.324,minLatStart=       41.8396759033,minLatEnd=       41.8396759033,minLonStart=       12.4629812241,minLonEnd=       12.4629812241,slopeLatm=       -0.0817844123,slopeLonm=       -0.1395736635,id=21220,ftdfk=-2147483648,fpdfk=21219,sfk=-2147483648)

So it appears that we screwed up when we created 21223..



SpaceTimeBox(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2013-04-05 21:19:34.786,latStart=       41.7296791077,latEnd=       41.8277282715,lonStart=       12.3521671295,lonEnd=       12.4340867996)
GpsLocCacheRow(startTimeMs=2010-05-10 23:30:34.950,endTimeMs=2010-05-24 21:12:15.141,minLatStart=       41.6975402832,minLatEnd=       41.7707481384,minLonStart=       12.2401781082,minLonEnd=       12.3493518829,slopeLatm=       -0.0000328160,slopeLonm=       -0.0000584053,id=18769,ftdfk=-2147483648,fpdfk=25387,sfk=-2147483648)


GpsLocationRow(timeMs=2010-04-20 05:33:26.470,latm=      -25.5982513428,lonm=      -54.5704498291,id=1)
GpsLocationRow(timeMs=2010-04-20 05:41:52.286,latm=      -25.5981407166,lonm=      -54.5706710815,id=2)


GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=      -25.5982513428,minLatEnd=      -25.5982513428,
minLonStart=      -54.5704498291,minLonEnd=      -54.5704498291,slopeLatm=        0.0002234014,slopeLonm=       -0.0004448258,id=0,ftdfk=-2147483648,fpdfk=10,sfk=-2147483648)
GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=      -25.5982513428,minLatEnd=      -25.5982513428,
minLonStart=      -54.5704498291,minLonEnd=      -54.5704498291,slopeLatm=        0.0002234014,slopeLonm=       -0.0004448258,id=10,ftdfk=-2147483648,fpdfk=-2147483648,sfk=-2147483648)


-5.4570672E7

-54570449

D/GpsTrailer( 7321): inserted GpsLocCacheRow(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:34.358,minLatStart=      -25.5951881409,minLatEnd=      -25.5951881409,minLonStart=      -54.5747375488,minLonEnd=      -54.5747375488,slopeLatm=       -0.0025539817,slopeLonm=       -0.0008227026,id=102,ftdfk=-2147483648,fpdfk=101,sfk=-2147483648)
E/GpsTrailer( 7321): Bad child data!!! GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:15:34.358,minLatStart=      -25.5990753174,minLatEnd=      -25.5954570770,minLonStart=      -54.5716438293,minLonEnd=      -54.5694198608,slopeLatm=        0.0002198765,slopeLonm=       -0.0025273715,id=13,ftdfk=-2147483648,fpdfk=102,sfk=-2147483648) child: GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-20 05:53:50.120,minLatStart=      -25.5981407166,minLatEnd=      -25.5981407166,minLonStart=      -54.5706710815,minLonEnd=      -54.5706710815,slopeLatm=        0.0041095852,slopeLonm=       -0.0056572966,id=75,ftdfk=-2147483648,fpdfk=74,sfk=12)



7/12

Ok, so the cache has been filling up and blowing over. It's a problem... we could increase it again I suppose.. Let's do that for now and see the results while we think.

So we don't know what the issue is exactly right now.. We could have too many children in the cache.

Now there are two ways of approaching it... and I'm thinking it may be better to create a diagnostic tool to examine
an existing database. This way when we actually start adding points for real, we will be able to see what is going on.

Now the thing is this, how do we control such a tool? I think we keep it simple and just use adb and lesses like we have
been. 

In which case, what do we want to report?

For each zoom level, we will want the max and min number of children and average number.

Now, what about doing it while it's running. It should be callable from the create cache process.

Yet we still should be able to run it separately.

One issue that we've been facing is that we really only have one "command" as such. Just "run"

Maybe we should have a menu with different options. We could add this functionality to 
GpsReviewStart.. Just a simple menu that does different stuff.

Crap, we ran out of memory... at 10240 * 10 caches
well that gives us an idea


Yea, we definitely need more visibility.

So lets say we have a menu on GpsReviewerStart.

Now, the thing is though that until we produce the cache,
we are unsure of what we have..

There is another issue that I haven't mentioned yet. We don't have testing facilities..
and I wonder. First, we were able to create all the cache rows before and it worked ok
Although some were being added to their parents incorrectly, so that may have been
the issue there.

Second, we really don't know if this stuff is working correctly or not. 

I wonder if a visual thing might be better.

Ok, so here is the thing. We'd like to have a menu, but we also want it
automatiable.. So selecting stuff off the menu can be pre-done in some 
hardcoded script... so if we are debugging, we don't need to select 
crap on the phone everytime.

So lets say we add menu items to GpsTrailerReviewerStart... we'd add
the following:

1. Redo cache building
2. Map reviewer
3. Mapless reviewer
4. Cache statistics

7/12/10

/home/tim/gpstrailer10.db3 contains good data without fake points

CacheViewer ... so what's the perfect cache?

Each cache level should have a certain number of items below it..
but wait, wasn't the whole idea to display a point or points for
a given resolution? So if a resolution has a 1000 points, then
they need to be displayed, full stop. True, true, but regardless
if we are displaying a tree of data, or just an item we are fine.

The thing is though, if it's good enough.. it's good enough..
yea but we sit there and go through 20000 items for one additional
point. 

So lets see about that, at least.. we need to. Or should we start
drawing fixing.. I think we should draw fix, actually...
Cause it's more important, we know we need to do that.

Hmm, but the thing is, to actually start using gpstrailer2 we need
to be able to record points and we can't do that until we get
this issue straightened, because we can't go through 20,000 points
just to add one. True.. let's take a look at the data, I guess.

No, I still think we should work on display for now. We have a lot
of ideas that we aren't sure are going to work.. And what we do
with this affects the cache greatly. So let's not optimize it yet
.

Ok


Now there are two ideas for displaying. One is to do random
display... that is if a person comes back to the same spot over
and over, we display the latest stuff over the previous stuff.

Another method is like confetti where we display all the colors 
together. I don't like that, it's confusing and displays little
useful information. I think the covering is better.

We could have a 3d display later that displays stuff at an angle
and creates mountains of data according to how long you've been
there.

Ok, fine, then that's easy. So then I'm thinking about an outline
or something to make the data stand out more. We could do an outline
we would just need to paint, and then paint under it with an outline
of every point and line so the border of the underneath outline extends
a little further than the original.
What about an a while alphaish outline? So the background fades away
a little underneath the points and lines.. That sounds promising.

So that makes it simple and clear.. I don't really see anything else
to do... ok lets see:

1. screen becomes gray during "thinking" moments
2. most likely use a background buffer
3. just draw over older data.. data must be sorted of course then

Ok, so we need to draw in an offscreen buffer, twice
So maybe two offscreen buffers? One with the white alpha stuff
and the other without it? How hard is it to do the white alpha
as a gradiant. Oooh that sounds hard. What if we just do a
blank white constant alpha first, cause it's easy.

Great, so then what? 

We have a thread that draws everything as alpha constant slightly
bigger, then draws it normally.

So we have OverlayDrawer which does this drawing.
The cache we kind of want to share, so we'll have cache.getInstance()
makes sense.

Ok, so the OverlayDrawer will ask the cache to do whatever so it can
draw. When should it draw? When things change. Now we can gray out
the data if necessary.. I think we need two bitmaps, so we can
pan the dead data. 

Ok, so overlay drawer has two background images, and a getImage()
Then we need to notify it to redraw. We'll probably just ping it
to tell it.

Overlay drawer will then just redraw.

It's overlay's responsibility then to place the image that
overlay drawer draws.

Seems simple enough

So, problems:

there is a delay, a drag.. this is because while we are
drawing the points, the location of where those points 
need to be changes, so half the display is one place the
other half is in another
we need to make a bitmap mask


For the delay, the drag, we need to calculate the point positions...
or really what we need to do is calculate the positions of the points
in the main looper thread (the draw part) and then draw them in the
other thread... 

Ok, so the trick is that we have no idea what the map view is going to
say for the location of these things. The other issue is that when
we pan, if we pan the image too, which, yes we have to do, then
we are relying on a rectangular coordinate system. So we have to
use a min and max position and a latm and lonm to pixelness.
So that is what we need to calculate in the looper thread. If
it doesn't work, of course we need to go even further up the
chain and put it in the overlay draw method. I think that if
we can do it easily we will go here.

So we have overlay, we have the stbox...
so what we do is update the stbox when we draw...
the thing that worries me is that we shouldn't have to redraw
before we are ready...

So we have two methods.. updateStBoxArea() and updateStBoxTime()

updateStBoxArea()  

updateStBoxTime() won't necessary be called by the drawer

So the drawer is going to take the stBox and use it.

The update method is going to create a new st box and update the 
stbox..

Wait, so the stbox defines all we need to know from the display
to draw our points..

So, the thing is that we want to make a copy when displaying
so we don't have "newStBox", we create a local copy whenever
we draw.

So we do the following... newStBox

Problems:

1. when zooming it flips around like crazy... remove it or do scaling
2. lat and lon calculation does not work when zoomed out far due
to the curvature of the earth I suppose... only a problem in latitude
(y direction)
3. Still ugly
4. A lot of jumping around during zoom and when fully loaded

So the zoom buttons do the same as using
getController().zoomIn(). They have the jump bug. The jump bug seems
to occur when it can't load data. It's easy to create, just turn off
wifi and zoom in and out.

So before we zoom in and after, it says the mapcenter is the same
(according to the overlay draw method)


E/HACK    ( 3622): mapcenter is -27420502,-55900027
E/HACK    ( 3622): mapcenter calc 2 is -27419437,-55900241
D/HACK    ( 3622): mapcenter corner -27412160,-55907065
D/HACK    ( 3622): drawing mapcenter -27420502,-55900027
D/HACK    ( 3622): drawing mapcenter corner -27409875,-55907386
E/HACK    ( 3622): drawing mapcenter calc 2 is -27417055,-55900627

Ok, so what we found out is both the mapcenter and the projection
of the center changes. They both become different however, I think.

Scrolling by what is and what was does not seem to do anything great,
the cursor just bounces to a random location.

So now we need to scroll to the new spot, so I think I'll try
animateTo

From before zoom to after zoom, mapcenter stays the same

So we have, after two seconds of delay:

previous mapCenter  
previous calc mapCenter

now mapCenter
now calc mapCenter

pmc = X
pcmc = X
nmc = X
ncmc = Y

After 2 seconds of delay:
animateTo seems to do nothing. 
I'm going to try setCenter(x) next
doesn't do anything.

so then, I guess there is setCenter(y) which seems totally wrong, but lets see
what happens.

doing this causes it to jump the wrong direction. We can try the difference backwards
,but I think we'll get similar results to setPixel

Same issue with setPixel().. works the first time , but a second zoom will cause
it to jump to a random location


E/HACK    (13591): prev mapcenter is -27433370,-55886257
E/HACK    (13591): prev mapcenter calc is -27433370,-55886257
E/HACK    (13591): next mapcenter is -27433370,-55886257
E/HACK    (13591): next mapcenter calc is -27440912,-55878447
E/HACK    (13591): prev mapcenter is -27425828,-55894067
E/HACK    (13591): prev mapcenter calc is -27425828,-55894067
E/HACK    (13591): next mapcenter is -27425828,-55894067
E/HACK    (13591): next mapcenter calc is -27437139,-55882352

ok so after z2 we got this stuff

pmc = X
pcmc = X
nmc = X
ncmc = Y

pmc = W
pcmc = W
nmc = W
ncmc = V


E/HACK    (14627): prev mapcenter is -27419240,-55900210
E/HACK    (14627): prev mapcenter calc is -27419240,-55900210
E/HACK    (14627): next mapcenter is -27419240,-55900210
E/HACK    (14627): next mapcenter calc is -27414592,-55900853
E/HACK    (14627):  map offset is -4648,643
E/HACK    (14627):  map move to is -27423888,-55899567

E/HACK    (14627): prev mapcenter is -27423888,-55899567
E/HACK    (14627): prev mapcenter calc is -27423888,-55899567
E/HACK    (14627): next mapcenter is -27423888,-55899567
E/HACK    (14627): next mapcenter calc is -27416913,-55900532
E/HACK    (14627):  map offset is -11623,1608
E/HACK    (14627):  map move to is -27435511,-55897959


great that fixed it, but we still have a problem with zoom out,
and we got to fix the delay time.

And we have to test after loading map data, to see what happens
then.

It jumps to the wrong position

E/HACK    (14856): prev mapcenter is -27929184,-55967571
E/HACK    (14856): prev mapcenter calc is -27929184,-55967571
E/HACK    (14856): next mapcenter is -27929184,-55967571
E/HACK    (14856): next mapcenter calc is -27929184,-55967571
E/HACK    (14856):  map offset is -139437,-54428
E/HACK    (14856):  map move to is -28068621,-56021999


Note that X and Y are the same now.

No, it's still not working..

let's see. we draw based on the calculated, I would suppose.
So the goal is to get the calculated to the chosen

I mean we could base it from the chosen...

The thing is that we know what the mapcenter is supposed to be

So it kind of works when we keep hitting the offset, keep changing
it.. So it makes sense that it is hitting from the same value.. we
are trying to offset for that. So in other words, we want to keep
the same value everytime, lets try that, I suppose.

Man this stuff makes no sense...

So let's try this. Lets try setting it and seeing what it changes to.

So in other words, right before we draw, we do this.

get the calculated center, use the pixel offset to set it..

No, it just keeps moving..

Ok, so here it is...

the mapcenter is wrong. But that is the only way to communicate with the 
map api. The issue is that we tell it scroll one direction to offset,
but it seems that the way we tell it to scroll doesn't always send it
to the right place. 

So the thing is, can we keep trying... Just to see what happens?


Darn, scrolling doesn't do anything.. I suppose we could return without
drawing anything... No, then we have the runaway scrolling...

Besides, the issue is that what we are actually displaying is Y,
and what we want to display is X. If we change X to something else 
in hopes that Y becomes what X used to be, we forget what X used to
be. And also, it doesn't behave right anyway.

So scrolling doesn't work.. because when we change X to change Y,
they will still never meet.

But the interesting thing is that the offset seems to work. Like
when we scroll, the calc position becomes exactly what the 
wanted position was.

So think about that... we have this offset that gets created that
we can account for. We know the extent of the offset, based on
the difference between X and Y. 

But the main issue is that we corrected for certain amount of the
offset already. But then the offset changes and that correction
has to change as well.

So in other words, as soon as the offset changes we need to do
something... The thing is we have X, that is the original position
Then we move to X' to offset for it. Then when we zoom in again,
we are actually at X, although we zoom into X'. Except now the
offset will change again. And if the map is displayed, the
offset disappears, or something happens.

Darn, what a shame. 

What are the extent of the methods, is there anything that might
fix this?

void 	stopAnimation(boolean jumpToFinish)
          Stops any animation that may be in progress, and conditionally update the map center to whatever offset the partial animation had achieved.

 void 	stopPanning()
          Resets the pan state to make the map stationary.

TODO: look at this method... how funny I didn't see it before:
 void 	zoomToSpan(int latSpanE6, int lonSpanE6)
          Attempts to adjust the zoom of the map so that the given span of latitude and longitude will be displayed.

 so stopAnimation doesn't fix the bug
neither does stopPanning... doggoneit

What if we just use zoomToSpan? would that fix it?

zoomToSpan sometimes runs into a different issue where 
it doesn't zoom at all.. Then it will zoom. and it also has the jumping around bug.

What if we zoom in, check for an offset, say darn that and zoom out again
and then try zooming in again, or something.. don't know..

If only there was a reset switch. Maybe we got to look at the decompiled code..

7/15

Ok, so here's the deal.

I believe the calculated center is always correct.

So the thing is that before the zoom and after it, we want the same calculated center.

So what we do is just compare one to the other, compute the difference and wham! scroll by
that amount.


scrollBy messes everything up... even the google label becomes offset.. it's really insane.


Unfortunately, nothing seems to work.. I guess we'll give up.

here is where the code is mostly saved /tmp/bak07151151.tar.bz2

Ok, so lets move on.. now the question is should we file a bug? Well, it's going to be a little
difficult to convince someone.

On one hand, it may be worth it, but on the other hand, it will be a lot of work that may turn
into nothing. Very frustrating... I think that we will ignore it for now.. It's a crazy problem

This may sound crazy, but what if we simulate touch events? To drag... I mean that works.
Sure, but....

Yea, that might work.. do I want to do that now? I really want to take a break from this
problem..

We need to get the latest version of the phone os, too..

To see if it fixes it. But we want to keep root, right?

---

So, so far the visuals are running slick, we never seem to be examining too many points, and 
everything seems to be cool, except that adding a point sometimes takes many seconds.

Also, why does the number of nodes not seem to change between zero and months?

We have revisited, especially buenos aires.

I don't want to do a slow tired test.. we need something faster...

We can easily create GpsLocationRows.. we need to do this.

Let's start with some simple scenarios. The other thing that keeps nagging at me, why don't
we change the constant zoom levels? What about the constant time levels?

I wonder how to approach this... a bunch of nodes... hmm.. this is so hard to test..


Well if I think about it, I can see how it can go through 10,000 nodes. There are 9 time
levels, that's 1000 each. But we have a cache of 40,000.. well that worked. I don't know
it still seems like a lot. Oh wait, I think I see. Look, we try to add to the children first,
then the parent. In *every* case. So we go all the way down to level 9. So we start at level
0, yet we don't know if we can add it until we've gone through all the children first..
See, thats why it was so fast beforehand. So maybe we got to test twice to get the cache
to start adding rows faster.

Like a pre test


7/16

well that killed it.

Now the question is what are we going to do?

For 3 months of data, it had to go through 6000 points, it takes forever
for the home/work only person.

We are doing analysis right now and it's taking forever as well.
It is 300 megs of data, though.

There are 3.2 million rows in the cache table.

Huh, well that's not too bad.. I wonder what the problem is.


Here is time level 8:

D/CacheView.viewStats(  956): view stats
D/CacheView.viewStats(  956): timeLevel=8
D/CacheView.viewStats(  956): 0 ZoomLevelStats(min=        68,max=        68,avg=       68.0000000000,count=         1)
D/CacheView.viewStats(  956): 1 ZoomLevelStats(min=         1,max=         8,avg=        1.3676470518,count=        68)
D/CacheView.viewStats(  956): 2 ZoomLevelStats(min=         1,max=        39,avg=       25.9247303009,count=        93)
D/CacheView.viewStats(  956): 3 ZoomLevelStats(min=         1,max=        34,avg=        9.3376188278,count=      2411)
D/CacheView.viewStats(  956): 4 ZoomLevelStats(min=         1,max=        23,avg=        3.8599030972,count=     22513)
D/CacheView.viewStats(  956): 5 ZoomLevelStats(min=         1,max=       142,avg=        1.3633915186,count=     86898)
D/CacheView.viewStats(  956): 6 ZoomLevelStats(min=         1,max=        36,avg=        1.0909551382,count=    118476)
D/CacheView.viewStats(  956): 7 ZoomLevelStats(min=         0,max=         0,avg=        0.0000000000,count=    129252)

We'll see about the other time levels as well.

D/CacheView.viewStats( 1009): timeLevel=7
D/CacheView.viewStats( 1009): 0 ZoomLevelStats(min=        68,max=        68,avg=       68.0000000000,count=         1)
D/CacheView.viewStats( 1009): 1 ZoomLevelStats(min=         1,max=         8,avg=        1.3676470518,count=        68)
D/CacheView.viewStats( 1009): 2 ZoomLevelStats(min=         1,max=        39,avg=       23.8064517975,count=        93)
D/CacheView.viewStats( 1009): 3 ZoomLevelStats(min=         1,max=        36,avg=        9.9385728836,count=      2214)
D/CacheView.viewStats( 1009): 4 ZoomLevelStats(min=         1,max=        22,avg=        3.9467370510,count=     22004)
D/CacheView.viewStats( 1009): 5 ZoomLevelStats(min=         1,max=       140,avg=        1.3638824224,count=     86844)
D/CacheView.viewStats( 1009): 6 ZoomLevelStats(min=         1,max=        36,avg=        1.0912406445,count=    118445)
D/CacheView.viewStats( 1009): 7 ZoomLevelStats(min=         0,max=         0,avg=        0.0000000000,count=    129252)
D/CacheView.viewStats( 1009): timeLevel=6
D/CacheView.viewStats( 1009): 0 ZoomLevelStats(min=        68,max=        68,avg=       68.0000000000,count=         1)
D/CacheView.viewStats( 1009): 1 ZoomLevelStats(min=         1,max=         8,avg=        1.3676470518,count=        68)
D/CacheView.viewStats( 1009): 2 ZoomLevelStats(min=         1,max=        33,avg=       21.0752696991,count=        93)
D/CacheView.viewStats( 1009): 3 ZoomLevelStats(min=         1,max=        33,avg=       10.6770410538,count=      1960)
D/CacheView.viewStats( 1009): 4 ZoomLevelStats(min=         1,max=        22,avg=        4.1610836983,count=     20927)
D/CacheView.viewStats( 1009): 5 ZoomLevelStats(min=         1,max=       134,avg=        1.3607413769,count=     87079)
D/CacheView.viewStats( 1009): 6 ZoomLevelStats(min=         1,max=        36,avg=        1.0908077955,count=    118492)
D/CacheView.viewStats( 1009): 7 ZoomLevelStats(min=         0,max=         0,avg=        0.0000000000,count=    129252)
D/CacheView.viewStats( 1009): timeLevel=5
D/CacheView.viewStats( 1009): 0 ZoomLevelStats(min=        68,max=        68,avg=       68.0000000000,count=         1)
D/CacheView.viewStats( 1009): 1 ZoomLevelStats(min=         1,max=         8,avg=        1.3676470518,count=        68)
D/CacheView.viewStats( 1009): 2 ZoomLevelStats(min=         1,max=        33,avg=       20.8602142334,count=        93)
D/CacheView.viewStats( 1009): 3 ZoomLevelStats(min=         1,max=       140,avg=       10.7242269516,count=      1940)
D/CacheView.viewStats( 1009): 4 ZoomLevelStats(min=         1,max=        25,avg=        4.1912040710,count=     20805)
D/CacheView.viewStats( 1009): 5 ZoomLevelStats(min=         1,max=       196,avg=        1.3590105772,count=     87198)
D/CacheView.viewStats( 1009): 6 ZoomLevelStats(min=         1,max=        36,avg=        1.0907065868,count=    118503)
D/CacheView.viewStats( 1009): 7 ZoomLevelStats(min=         0,max=         0,avg=        0.0000000000,count=    129252)
D/CacheView.viewStats( 1009): timeLevel=4
D/CacheView.viewStats( 1009): 0 ZoomLevelStats(min=        68,max=        68,avg=       68.0000000000,count=         1)
D/CacheView.viewStats( 1009): 1 ZoomLevelStats(min=         1,max=         8,avg=        1.3676470518,count=        68)
D/CacheView.viewStats( 1009): 2 ZoomLevelStats(min=         1,max=        28,avg=       17.6666660309,count=        93)
D/CacheView.viewStats( 1009): 3 ZoomLevelStats(min=         1,max=       127,avg=       12.5228242874,count=      1643)
D/CacheView.viewStats( 1009): 4 ZoomLevelStats(min=         1,max=        52,avg=        4.2350425720,count=     20575)
D/CacheView.viewStats( 1009): 5 ZoomLevelStats(min=         1,max=       271,avg=        1.3596676588,count=     87136)
D/CacheView.viewStats( 1009): 6 ZoomLevelStats(min=         1,max=        36,avg=        1.0909551382,count=    118476)
D/CacheView.viewStats( 1009): 7 ZoomLevelStats(min=         0,max=         0,avg=        0.0000000000,count=    129252)
D/CacheView.viewStats( 1009): timeLevel=3
D/CacheView.viewStats( 1009): 0 ZoomLevelStats(min=        68,max=        68,avg=       68.0000000000,count=         1)
D/CacheView.viewStats( 1009): 1 ZoomLevelStats(min=         1,max=         8,avg=        1.3529411554,count=        68)
D/CacheView.viewStats( 1009): 2 ZoomLevelStats(min=         1,max=        27,avg=       17.0543479919,count=        92)
D/CacheView.viewStats( 1009): 3 ZoomLevelStats(min=         1,max=       129,avg=       13.0694713593,count=      1569)
D/CacheView.viewStats( 1009): 4 ZoomLevelStats(min=         1,max=        60,avg=        4.2505121231,count=     20506)
D/CacheView.viewStats( 1009): 5 ZoomLevelStats(min=         1,max=       349,avg=        1.3593579531,count=     87161)
D/CacheView.viewStats( 1009): 6 ZoomLevelStats(min=         1,max=        36,avg=        1.0908906460,count=    118483)
D/CacheView.viewStats( 1009): 7 ZoomLevelStats(min=         0,max=         0,avg=        0.0000000000,count=    129252)
D/CacheView.viewStats( 1009): timeLevel=2
D/CacheView.viewStats( 1009): 0 ZoomLevelStats(min=        68,max=        68,avg=       68.0000000000,count=         1)
D/CacheView.viewStats( 1009): 1 ZoomLevelStats(min=         1,max=         8,avg=        1.3529411554,count=        68)
D/CacheView.viewStats( 1009): 2 ZoomLevelStats(min=         1,max=        27,avg=       17.0434780121,count=        92)
D/CacheView.viewStats( 1009): 3 ZoomLevelStats(min=         1,max=       129,avg=       13.0752553940,count=      1568)
D/CacheView.viewStats( 1009): 4 ZoomLevelStats(min=         1,max=        60,avg=        4.2513413429,count=     20502)
D/CacheView.viewStats( 1009): 5 ZoomLevelStats(min=         1,max=       349,avg=        1.3593924046,count=     87161)
D/CacheView.viewStats( 1009): 6 ZoomLevelStats(min=         1,max=        36,avg=        1.0908631086,count=    118486)
D/CacheView.viewStats( 1009): 7 ZoomLevelStats(min=         0,max=         0,avg=        0.0000000000,count=    129252)
D/CacheView.viewStats( 1009): timeLevel=1
D/CacheView.viewStats( 1009): 0 ZoomLevelStats(min=        67,max=        67,avg=       67.0000000000,count=         1)
D/CacheView.viewStats( 1009): 1 ZoomLevelStats(min=         1,max=         8,avg=        1.3731343746,count=        67)
D/CacheView.viewStats( 1009): 2 ZoomLevelStats(min=         1,max=        28,avg=       17.3260860443,count=        92)
D/CacheView.viewStats( 1009): 3 ZoomLevelStats(min=         1,max=       123,avg=       12.8663740158,count=      1594)
D/CacheView.viewStats( 1009): 4 ZoomLevelStats(min=         1,max=        47,avg=        4.2508654594,count=     20509)
D/CacheView.viewStats( 1009): 5 ZoomLevelStats(min=         1,max=       349,avg=        1.3588969707,count=     87181)
D/CacheView.viewStats( 1009): 6 ZoomLevelStats(min=         1,max=        36,avg=        1.0910103321,count=    118470)
D/CacheView.viewStats( 1009): 7 ZoomLevelStats(min=         0,max=         0,avg=        0.0000000000,count=    129252)
D/CacheView.viewStats( 8286): timeLevel=0
D/CacheView.viewStats( 8286): 0 ZoomLevelStats(min=        67,max=        67,avg=       67.0000000000,count=         1)
D/CacheView.viewStats( 8286): 1 ZoomLevelStats(min=         1,max=         8,avg=        1.3731343746,count=        67)
D/CacheView.viewStats( 8286): 2 ZoomLevelStats(min=         1,max=        28,avg=       17.3260860443,count=        92)
D/CacheView.viewStats( 8286): 3 ZoomLevelStats(min=         1,max=       123,avg=       12.8663740158,count=      1594)
D/CacheView.viewStats( 8286): 4 ZoomLevelStats(min=         1,max=        47,avg=        4.2508654594,count=     20509)
D/CacheView.viewStats( 8286): 5 ZoomLevelStats(min=         1,max=       349,avg=        1.3588969707,count=     87181)
D/CacheView.viewStats( 8286): 6 ZoomLevelStats(min=         1,max=        36,avg=        1.0910103321,count=    118470)
D/CacheView.viewStats( 8286): 7 ZoomLevelStats(min=         0,max=         0,avg=        0.0000000000,count=    129252)

7/17/10

So the question is, if we had all the memory in the world, what would fix this problem?

One of the things is that .. hold on, I need to investigate some more.

Ok.. darn we forgot that the old code would be wiped when we upgraded the OS.

So the question is, do we want to separate the versions again? Or what do we want to do?
We could have two packages.... it kind of makes sense, more freedom. In fact with gps reviewer
start, we could just simply call the other application. No big deal. Then just move things over
to the old package when we want to use it.

However, the only thing I don't like is that we will miss out on the latest and greatest.

But we can simply do a diff. I suppose... except for the package names.. we could create
a perl program to copy from one package to another. In fact that is what we can do right 
away to start out. We will create the program which will copy everything under rareventure.gpstrailer2
to rareventure.gpstrailer3, and then we'll do our work in gpstrailer3.

Of course, this also means that the gps service will shutdown every time we reload, 
so we'll have to make the reviewer start start it up.


I think a better way would be able to install a production and a development version.
It's good, but remember that they would share db, unless we did something special.

Oh what a pain!

So given the results that we have... what do we do?

I guess we should test first... we may not need to do anything. Yet... I just wonder,
because our time levels and zoom levels are quite arbitrary. Right now, each time
level is separate, also they don't change much between each other..

I mean I don't want to rewrite with no apparant benefit. But it seems so arbritrary
that there could stand to be a lot of benefit if I just take a look.

So there are two questions. The first is that are the time levels right.. There are
so many.. it seems like a waste of space, but that really isn't a big deal.

The zoom levels, however... are much more important. Before I had the inkling that
we should just have the screen zoom levels dictate the zoom levels of the cache,
but I don't know..

The thing is that the more zoom levels there are, the less extreneous points we
have to go through. But the more blurry points we have to get through before
we get to the ones we care about.

As far as the time levels go... we need a maximum level of "X". So I suppose
it doesnt matter if we have a 1 second time level or 1 month, as long as it's
less than the minimum.

Let's see, with hundreds of subpoints, then we may need to go through thousands
of out of bounds points to find the borders. 

TODO 2: I think that when changing the time period, we should start from the 
first position *on screen*

Well, so the first thing is to find out exactly what is happening.

I mean we could go down to the point where each point can just have two subpoints,
etc.

With that, I mean think, we'd have to reorganize a lot. I suppose we'd just dump on
top or in the middle. It would be like a small single point at first. Then we'd get
another and create a new parent for both, so with 2 points there would be 3 guys.
Then we'd do it again and again. I wonder... it seems so simple. I wonder...
We just go down far enough where we are like, doggoneit, small enough.

It seems a lot better than choosing these things.

The only question then is the 

Finally some darn data!

With 2 point granularity, 3 months over fake data:
D/GpsTrailer( 4221): iterStasts=pointsGoneThrough=9900,pointsReturned=7821
D/GpsTrailer( 4221): zoomLevel=5

With 10 point granularity:
D/GpsTrailer( 4315): iterStasts=pointsGoneThrough=1254,pointsReturned=1066
D/GpsTrailer( 4315): zoomLevel=4


D/GpsTrailer( 4315): iterStasts=pointsGoneThrough=3291,pointsReturned=2902
D/GpsTrailer( 4315): zoomLevel=4

Once we get to zoomlevel 5 it takes like 20 seconds to draw:

D/GpsTrailer( 4315): zoomLevel=5
D/GpsTrailer( 4315): iterStasts=pointsGoneThrough=11236,pointsReturned=7806

It's fine when we reduce the time level down far enough

D/GpsTrailer( 4315): zoomLevel=5
D/GpsTrailer( 4315): iterStasts=pointsGoneThrough=420,pointsReturned=182

So the thing is, I think, is that when we lengthen the time period, we have
to reduce the effective zoom granularity.. and this is based on two things:
1. time (of course)
2. and coherence of display. I

But the thing is that even if we have a huge time period, what's on the
screen matters. What's nearby the point. So in other words we should have
different time levels and granularity based on what is nearby...

Right now, the points between time levels have no relation to each other.
What if that was changed.

What if we started with a point, right. Then we created multiple parents
in the two dimensions of area and time. And we multiply the two to get the
limit.

Well wait, we need to be able to determine whether to create a parent or not.
Because say we have X at time v and location a and Y at time w and location b.

Then we need to decide, is it useful to create a parent for these points?
Now the other thing that bothers me a little is that we have siblings and only
one child group.. However we can fix that because we can easily have variable length
data... so pay no heed.

So consider that we can have as many parents as we'd like. What is the criteria for 
forming a parent?

Well that depends on the usefulness of it. 
Wait wait wait. How will we find these points?
I mean there has to be a container for each point, so we can lower it down. Think
of it as a bunch of areas and we choose one, and then go in there and go in there
and go in there. So this is possible.
But think, we have multiple parents.. We have multiple parents today.. too.

But that's ok. Think of it like glass windows, they all go to the same underlying
points, but are just different ways of getting there.

So here is the thing, whenever we expand a box, we need to check to see what
parents the new box has. We need to go through the existing cache with a box,
covering the size and associate if we have not.

Now the other question is, do we need to cover all the children? Yes, because
if we are checking outside of the area of the major boxes, we will miss the children
that extend out of it.

So yes, we need to extend parents over children. Now we need to decide, how do we 
decide what parents to create?

Remember, we are always going from parent to child. Also think, if we have overlapping
children... what do we do? How do we choose which child? I mean we have a lopsided box.
Is this even appropriate? What if instead we make it a cube? or a box, rather, and forget
the lopsidedness?
What does the lopsidedness really buy us?

It prevents us from covering a huge area just because the user decided to fly from
Argentina to Italy. But think, the user is in only one location at a time, they can't
go backwards. So the points in the box will by definition only be the two points for
the flight. They will only increase if we decide to continue the box to become bigger
and bigger.

But you can say, what if we have infinite time? If so, we still would not want a box
to carry that much area and those many children.

But here is the point in fact, we are trying to reduce the size of each box to a 
certain maximum, which depends on the screen size. We need to cut the screen into a
series of boxes, each with the same lat and lon and time. Well sort of, anyway we have
a maximum size of box we can support. Under that, we need to find the child, or if
there is no child, then the box itself. But wait, What if the user is traveling a
far distance, and nearly in a straight line.. We can grab the points along the way
and make a lopsided box that covers a minimum area. That is what the lopsided box
buys us, the predictable behavior of the user. If they are going in a constant speed
a constant direction, we can represent that by a single point, rather than many.

And do we really care about the time a point is there? Well the time itself, no, but
the time jump, yes. Remember we are talking about time jump.

So the box is a box of area and time jump. But what are we really talking about?
Just how well the points are represented by the simulation. Nothing else. It seems
so strange, though, Like we are combining two concepts. One is that of which point
belongs to which simulation. We need to find the proper simulation, see. 
Then we have the simulation itself. 

Which annoys me. I don't want two tables, see.
But in fact that is what I have kind of. So lets relax a little. Lets say we use boxes
and sims.

So the point is, in the argentina to italy flight, we have a huge box of time and 
distance point to one or more sims with parameters.

Ok, and then we have inner boxes that may point to others. Sometimes boxes won't 
even have sims. Sometimes that is all they have. We could have multiple types of sims,
then. Cloudy time fuzzy sims, and straight in focus traveling sims. Waiting sims, too,
maybe?

So think though, the boxes themselves have to have different time jump levels.
A box with infinite time jump could contain a ton of sims for short time jump.

I have a question, do the time jumps always stay the same/get shorter?

I don't know if they have to... I think they would have to.

So in that case, what, well what can we do?

I mean even though we really don't want it because it would be ugly, we have
to consider the best being all the points displayed. We also want minimum time.
Or in effect a maximum number of points to search through.

I guess what is bugging me about the current situation is that, 
a) the points we have now are too many for the back and forth, heck even over 3 months.
I mean we can adjust it, but, you know..

I mean its not effective. It doesn't matter what time jump we create if we only visit
a spot of 10 ft by 10 ft only once or twice. 

so currently we create 9 copies of that one spot. We could have one, with x number of
parents. But I wonder if I even care? I mean, yea, 300 meg of cache after 3 years is
a lot. It takes long to produce and to go through. But it isn't that big of a deal.

But, remember the current thing doesn't work. True, but can we keep the same model
of timezones for the new system?

Lets pretend that we do... even though they suck. It's so much data, and it's so much
of a copy. Waa! Well wait, ok, so we zoom down in distance, down and down. I know
why I created them, btw, is because I figured you can always change from 0 to infinite 
time.

It's not really related to distance. So if you were fast enough, you could jump from
one place to another, and at 5:00 pm, July 2nd, you did travel from SF to New York,
and at 5:01 you did travel back.

7/18

Ok, so this is what we do. We just split in two in the longest dimension when
a box contains too many points. Now the thing is that we are wondering if we should
just consider time jump another dimension. I think that makes sense.. but let me think.

Ok lets say there is this small area that has big time jumps and many points.

So let's say we do this thing where we split up based on time after we reach the 10
point limit or whatever.

So we keep adding points until we get to 10 and then we split it up, and have a parent
with only two points, and we keep going. So that means we end up with a lot of split up
based on time. 

So now we want to see it at infinity time. What do we get? The top point.
If we want a deep time level, we keep going down until we get there.

Seems like it would work.

So now lets think.. so were just considering it another dimension.. so there really
isn't a problem. And if they overlap, they overlap.. No big deal.

Ok lets imagine a city of points. So we keep getting data, and we finally split it
up based on area. Then the next day comes and we get some more points and stick
them in the appropriate places. I mean it should work.

The thing is, lets say we spend 1000 days in this city. What happens?

If we have 10 points by area and we want to add another with a long time distance,
we would split with time. (so maybe we should use the center of the min and max always?)

so then when we get the next point, we'd add it to the next area point. And that would
happen for awhile until we split with time again. So I guess the point is that it doesn't
matter how much we split with area, if we split zero or many times, it would kind of
look the same from the time perspective.

Except that it would be less points, and we could drill down to more if necessary.

But the thing is that the higher the area is, the less we split through time, too.

So if we are looking at the city level, and lets say with infinite time. Well,
after 1000 days, we have basically a long cylinder, So if we just split up by the
longest dimension.. we are asking for a long cylinder as well. 

So... yes but sometimes a box can not split up by time. It can grow and grow and grow.
Because the time jump isn't high. So the point is, why split it up? But we should,
because, well should we?

We have these little tiny points, well eventually they would split up, right? 
Well what does a tiny little point mean? I mean a long silver strand hair? I
mean we split it based on whether it fits the simulation or not. So it means
a really really tight simulation. Because if it came at the wrong time, it
wouldn't be displayable based on the sim. So you get the small point, and it's 
good enough. Yea, it must be. So there you go. But.... the issue is this.
We have infinite time and we show a corner of the city. Now if there is not
enough time jump we will get a area centered split up. So it would be fine.
Now lets say there is a signficant time jump, like a daily thing. Then we
get the same thing, right, because the time will pyrimid up and the area would
also pyrimid up. But wait, lets say we zoom in really really tight with infinite
time. Here comes the problem. Because we end up with 1000s of cubes split up 
because of time jump... but now let's see

There are two scenarios, one where the user visited the point once or twice, and
the other, 1000s of times.

Now for where the user visits once or twice, lets say around that area we visited
it many many times. So there still could potentially be a lot of time instances
that would have to be gone through to see if they overlap the area. well wait
so there is a big guy that does overlap. But then it has a lot of time split ups, right?
Well lets say that this point happened in the middle of the scene. Right, so first
we have a smaller big thingy, then at one level a cube grows to cover this point
and everything above it does as well. Now we add a bunch of other points on top
of that. So what happens, yes, we have one block in the middle which is longer than
all the others. So yea, it would be a problem. Because we'd cut by time. 

So the thing is that we need two dimensions. Otherwise, we'll just have a blurry
thing, and we can't do it...

So we need two dimensions.

So we probably shouldn't mix them. So can we just split in one or the other
dimension? Yes the rows might be bigger, but there would be a heck less of them.

So in other words, we go to the specific area, and then go down by time. The opposite,
and the reason why is because it will lower the amount of rows, because the time will
be one in a lot of cases.

But how do we do this?

We are a certain point, with a certain area. Then we have to split it up by time.
So we keep splitting up by time and keeping the same area? So we would split it
up and recreate new sub items? I mean with all the sub items in area would
need to do the same thing. We could separate it just on time, and no longer
go towards area. It's crazy that it's the 

So I think the idea is to separate on time first, then on distance.
But we don't duplicate rows. only if they change do we duplicate them.

Now why is it that going in area first is better. Well remember this isn't just
area, but area/time. I mean we split up based on time, too. Just not time jumps.


So lets take a look, suppose we have the home city of 1000 days of activity.
If the time jump is small enough we will be splitting into a bunch of nodes,
but of course that will be ok, because we will be looking at a small enough time.

So wait, what are we talking about?

Node:
MIN_LATM, MIN_LONM, 
HEIGHT_LATM, 
WIDTH_LONM, 
LATM_PER_MS, 
LONM_PER_MS, 
START_TIME, 
END_TIME, 
MAX_TIME_JUMP,
CHILD1_FK,
CHILD2_FK

The question I have is max time jump... like what do we want to do for that?
Because we want to make it close enough that it will handle a lot of possibilities.
I suppose it's distinct levels... but like how do we start? Do we keep the ids
somewhere else. Do we precreate them?

I think that we can put them in preferences... yea it makes sense, because we
have the private key there, too. If we reset to factory preferences, we need to
keep the private key around.

So how do we do this? Lets have a certain increase in time, like 1.3 or something.
So we'll start at what, 1 millesecond? So how many distinct levels is that? 93

Not so bad. Of course we'd probably start at 5 minutes, actually, so that makes it 29
, well 30 including 5

So we'll load 93 numbers from the preferences that will initially all point to 0. Then we'll figure out the max time jump and create duplicates, etc.

Ok so how do we do this.

We'll load cache and it will grab the preferences automatically, right?

CacheCreator will set them up, and grab them automatically.. It will be
a pref in the cache. It will be what, a list of integers, sure why not.

Then cache creator will do it's thing.

Oh god, let's hope this won't suck.

So we sure about 2 children per node, right? The reason I like it is because
it allows us to decide how far to progress without loading too much. But
then again, I mean it's only 2.

So lets see, suppose we have 1000 points. It would be 1000 + 500 + 250 + 125 + 63 +
32 + 16 + 8 + 4 + 2 + 1 or just a little more than double.
Now, it depends on the depth. Cause the tree might not be weighted properly, but
lets look at 10 children
1000 + 100 + 10 + 1
Or about 111%. So we save a lot of space that way. But then, like you said,
we have less choice of what we are displaying. So let's go with 2. I think it's
crazy, but it just might work (the best).

Ok, so we have 30 or so starting places. What we can do is have a counter.. that
will work ok. It's only 30

x = timeJumpStart
y = timeJumpGranularity
z = num steps (result)
w = timeJumpEnd
x * (y ^ z) = w
log(y) * z = log(w/x)
z = log(w/x) / log(y)

Now that I think of this, why don't I just create 30 top level rows in 
the cache table? They would be the first 30, no parents. We could mark them
somehow, too, if we really wanted to. But anyway, if it's the first 30, we'd
just know by that that they are the root guys. They could even point to each
other, I suppose. No but this is better, it's just that I think people might 
complain if they see these guys. Because it does give some information, and
I really don't want people to whine. Bad press.. you know?

We could even have a top level cache row, with different info.. so no worries
about changing the columns around.

Hmm...

So why don't we just use another table? It's not that hard. Yea, maybe we'll just
do that. It's the same, pretty much. And it's more expandable for the future.
Ok

Ok, so let's think abou this.

We have one giant box. It accepts the new point, lets say. Then we have to add it
to one of it's children. So we add it to which one? The one that it fits in, I guess.
If it's outside the children, we add it to the one that accepts it. But there is no
difference, it just raises the what... I mean currently we have zoom level. Zoom level
controls the slop. If there is not enough slop, we don't accept it.
But in this case, the zoom level is variable. So do we raise it when we want to 
accept a point? I mean, keep in mind that we can only have two points, so by setting
a limit on the number of points, we are, in effect, setting a minimum zoom level.

We don't want zoom levels to be unbalanced, I think the thinking goes. I wonder
how time fits in this?

So lets start at the beginning. We have a single box with a single point in it.
Then we get another point. There are two scenarios, it fits or it doesn't

It fits:
So it fits with the current box. But what does that mean? Since the box doesn't
have any children, it must be set to the *minimum zoom level*. Now, we might consider
a time slop, too (which is different than time jump), but it amounts to the same thing,
a cylinder around the point. So the bottom box has a minimum zoom level.

The point gets added, but nothing much happens.

It doesn't fit:

So now we have 2 points. So it's obvious, keep the existing one. Create a new one with
the new point. Then create a parent. Set the zoom level appropriately.

Ok. now lets go through the 3 node tree. 

1. It doesn't fit the top:

We can create a new node to hold them both, or we can change the zoom level of the 
current top. Now what do we got here if we just always create a new node?
You have to realize that the old node won't overlap the new node ever with a 
time jump of 0. But that doesn't mean anything, cause we will have big time jumps.
We will have overlapping nodes. But that's ok, because parents always contain children.
We thought of this already.

So let's say we always create a parent. The only issue I see is that it will
make the other point have a narrow zoom level, so that means that a new point will
probably not be accepted by it. When means we will have a long dragon of a chain.

So I think we need to minimize maximum zoom level differences.

So in the case with 3, we need to decide, do we create a new top? It would create
a zoom level of x for the current top, and MIN_ZOOM_LEVEL for the new point.
Or, we could expand the current top so it does fit. Then we have two guys underneath
to decide which one to add it to. It would probably make sense here to go for the
one with the lowest rise in zoom level. That fits with the parent. Like, if we always
decide based on creating the lowest difference of zoom levels.

Of course, it may not fit with either of low guys due to time jump. In which case...
wait a second, so with time jump, we don't allow a point to be added to another
if the time jump doesn't fit, regardless of the zoom level?

If not, that might create a dragon. Because there will always be only one choice for
adding the point. To the top one. True, but look at it like a snake at the bottom,
that goes crazy directions. The upper levels *can* be balanced. They each contain
x number of points. 

So lets think, suppose we have a home,road,work scenario.

I think it will be fine.

So the idea is, yes we want to have consistent zoom levels for each child. 
So if we have a large difference, this is where we get a dragon. 

Because consistant zoom levels allow the points to be spread as evenly as possible.

So let's start again with the 3 node scenario.

1. It doesn't fit:

We pretend to create a new item outside, which would have a minimum zoom level
and get the difference of the system.

Then we pretend to create a new item as one of the children (note in the general
case this may not be possible due to insufficient time jump)

We compare the difference of the time jump of this system in this case with the
previous. Whichever is less gets the point created.

Now we need to understand for the general case that the underlying node will 
need to upset the difference not only for itself, but for it's children as well.

Now are we sure we just want to compare the total difference? Becuase suppose we
are dealing with a 1000 layer deep tree. Then if we go ahead and add it to
a node, vs creating another, we will have a 1000 new additions which may overload
it so we just create a point along side the other node. Now is that what we want
to do?

Well the point of the system is the ability to take out the largest chunk of area 
possible... or is it points? or what is it? So suppose there is one little 
hunk of land that has millions of points. At a high zoom level, we know that
either we want it or not. Because that's the way the user scrolls... not by points
but by distance. 

True, but we want to eliminate as many points as possible with each succession.
So the truth is that the number of points is what matters. I think..
because if we eliminate a whole section of land, but it only contains one point,
then who cares, because the issue is not drawing too much. And it's easy to draw
a single point over a huge section of land.

So in other words, we could have a giant box, with a tiny box inside of it covering
the little acre of land with 1000s of points, and that would be grand.

And it would be better than having cut up the rest of the land (considering a 
light scattering of points) just because this tiny little acreage is so popular.

So the point is that we just want to have the fewest depth levels. It's obviously
what we want to avoid (aka the dragon) and we also the thing we should shoot for
to be most efficient. So let's say we did that. We might have to split the
entire world in two where this area is centered.

But that is ok... well... it does mean that to search the blank country side,
we are going to the bottom. See, so maybe a little more dragon makes sense here.

Anyway, it doesn't matter too much... it's a decision that can be isolated.

So if we use points, we need to know the number of points per item. Which shouldn't
be a problem. So the thing is, let's go back to the 3 node thingy. What we 
do is see that one item has 2 children, and there is no other item, so we just
add it to the one item. Then we go down to the bottom two, and we will use points
as well as area. The thing is we don't want overlap. I suppose in this case, where
there is a tie, we use the zoom area difference?

What I'm worried about is one sibling overlapping another completely, just because
it has fewer points. Because another consideration is that if the zoom level can become
small enough, we can stop right there. If we don't care about zoom levels, we are
in effect going to all the points all the time, which isn't good for high level.
So then there is the question, is it the total zoom slop that matters, or do we want
individual slop.

So it's untrue that we want the most balanced points. We have different objectives
based on whether we are zoomed in or zoomed out.

When we are zoomed in, it's just the points that matter.. but we also want to avoid
overlap. When we are zoomed out, we care more about zoom level, because we don't
have to go down that far. 

I am thinking now just about zoom level. Except that, suppose that we have 
a big monster of points in one and not so in the other. Except that is ok,
because we have a limit of two, which means, I mean I think it would be alright.
Well kind of. I mean it would be better if we were able to make the decision at
each level individually. But how can we do that? Simple, we try to add the point
to each child. Whichever increases less in zoom area wins. Hmm, I could see some
situations with great overlap. I don't really want to try to detect overlap, however.

Well, lets see, in terms of absolute size of the box, one is going to increase more
than the other. Actually overlap is easy. Just ask each box, do you contain this point?
Which is the edge? No it doesn't work. It's not easy.

The thing is, though, I'm not sure how big of a problem this will be. Overlap is 
mostly going to occur in the cases where you have a small zoom area vs a large one.

Now what was that problem we wanted to avoid. We have a small city of points,
and a large area. This is getting kind of ridiculous. Let's just use the zoom area
... well.. lets go throug hthe scenarios.

So lets start with the 3. In the case of one initial point, the decision will
always be to go underneath it. I think that should be ok. We'll try it, I guess.
I can't really think of any scenarios where it will be messed.

So then we go to the children. Now like you said, average zoom levels. The reason
for this is that we are looking for a single zoom level, so the minimum points per zoom
level is the purpose. 

I think that's good enough. So we go for the children, find out which one will,
on initial examination, will average the zoom levels the most.

Simple as that. We'll add it to that point. It will keep going down until
it goes to the bottom. Once we get there, we see if we can add it, probably
not. So what we do is create a sibling and a parent that holds both.

Ok, simple and easy. Lets hope it works well enough.

So anyway, after we finish this for 0 time jump, we hit up the 1 time jump,
now, we want to basically do the same thing. Except that.. we allow 

wait, lets go back one second:

Of course, if the time jump doesn't allow for it, we don't add it. So, lets
explore that a little. So as we add points, how does the tree look?
Lets see, we start with 1, then we are at the bottom so we create a parent
and have 3. Then we automatically add it to the last point, so we will
have   
       p
      /\ 
     c1 c2
        /\
       c3 c4

Then we will have another which will be added to c4, right? So we keep going

       p
      /\ 
     c1 c2
        /\
       c3 c4
          /\
         c5 c6

So as you can see we get a dragon. If we want the latest point, we are going
a long way down. 

So it would be a lot better if we had something like

                p
              /  \
             c2  c4
             /\   /\
           c1 c3 c5 c6

So the idea here is that when we start with 

       p
      /\ 
     c1 c2

Then we want c2 to become a parent of c1, I guess.

                p
              /  \
             c2  c4
             /\   /\
           c1 c3 c5 c6

No maybe we go like

       p
      /\ 
     c1 c2
        /\
       c3 c4

Then, we want c1 to become a child of c2 at this point, to get the following:

                p
              /  \
             c2  c4
             /\   /\
           c1 c3 c5 c6

So the point is that one side of the tree is starving in the time dimension,
while the other has so much. Do you see, it's simple, the right side gets more and more
time, which means when we try to search and eliminate, the tree keeps eliminating
a niggle each time, and we end up in a long dragon.

So we have a point that we need to add to c4, yes. But afterwards, we look at c4
and say, hey, it's time is too long compared to c2. So maybe what we do is kill it
and stick c5 into c2. What a pain, because c5 might have kids. 
It's not really a big deal that it does, though. 
But that will handle the time length. 


Now is there any other concern. What about area? Well for this, we have to think
differently. Because for a small time area, we don't really care.

But now lets go to a higher time level. The thing that worries me, we are talking
about moving rows around, and killing rows, but we may have multiple parents.
Because when we hit time level 1, lets see what the current plan is.

Let's see, so what we do is ask the children for time level 0, can I add you?

If the child says no, it's fine. But lets see, let's say the top level says yes.

Ok so we start again with the point, we say, parent 0, can we add to you? And
parent 0 will of course say yes. So then we go to both children, and say, can
I add you. Wait... this is too complicated to consider right now. Let's seaprate
the treees again.

So we have time level x. Let's say level x is really really high. So high that we
can consider it infinite. Then, what do we have?

Well, now we can add to any of the children. So moving around may not be a problem.
We will want to equalize zoom area, now. So in this case, we don't really even
care about elimination by time. Elimination by area is much more important. So we
will want to equalize by area. Now the thing is that we could basically have the
same problem as with time, if, if the user travels around and never visits the same
place twice. Now we were using zoom area slop to handle this. See the thing is 
that now there is no restriction. Before, with time jump, we absolutely can't add
a point to a particular node. We may like to go down a certain path to balance
the tree, but we couldn't. 

The reason being is that we need to be able to follow the path of movement.
Now, I suppose that if the user takes a small detour along a large route, we
could ignore it. but that's where zoom area comes in. But then suppose they
take this triangular route... then what? Like they go in the excursion, then
jump back to the same spot a month later. Well, it won't work either, because
it would be a different color. So yes, we need this, but it's hard to define
the meaning of it. We are basically saying that we want to know absolutely between
these time periods, where the user was. We need to know all the locations. We can't
accept a few missing points here and there.

Whereas with a large time jump, we don't really worry so much about it. Because,
we are more concerned with, in this area, where did they go? Not so much when.

I mean, small time jump, we need to know, yes they were somewhere around there at
that exact time.

Whereas with a large time jump, we can be a little more relaxed about this.

And you think we can balance the area without adjusting the tree. I think so.
I mean we have options of what to do... we just have to make the right decision.

However, here comes the trick. You were saying before the time jump never increases,
but that is untrue. because infinite area always has infinite time jump. So the
thing is, that as soon as we change a node based on time jump, we need to copy
all of it's parents, and create a new set. 

Oh, what a pain. It might be better just to keep them as copies, for now..
Becuase it will be a lot of work to make them link together.

So then we got the basic system. Now, what about testing? Can we do this well?

If we extend GpsLocCacheRow, we can replace encrypt and decrypt, insert and update,
and have our own in memory db. Or we can create gps locations, and just drop
and recreate the table itself. Then print it out. That may be the best. I suppose.

So then, just in GpsTrailerCreator, we just create a if clause for test mode.
Or just a parameter containing a list of gps locations.

Maybe just an iterator. If we could squeeze the gps location stuff into an iterator
of sorts, we'd be golden. 

It shouldn't be too hard.. I suppose. ok

So now, how do we organize this.

Well, ok, so wait we have 29 time levels. Ugh. I wish we could combine those.

It's like, well, some important changes have to be made in the upper time levels.
But in the end, I mean, we are talking about a time jump. It's just that I forsee
the tree changing so much... what would it be like if there is just one time jump
that got merged in?

Because, in this case, darn. What if we kept the levels separate, kind of.

So it's like this. We consider each node as that for the time level. Only when
we need to make a different decision, do we separate and separate all the parents.

Because, if you think about it, the separation is going to occur at the lowest levels,
not the highest. It's almost backwards, you know?

Like we should keep the parents, but the children should change. Because if they
come back to the same spot a day later, we reuse the same child, but then again
we will always have one parent. So it's true that the time jump always increases,
as we go down, right? Because the parent will cover at least the same area,
so then, yes, it makes sense, the parent can only cover more points than the 
child, so the gaps between the points decreases

as we go down in the tree, the time jump always increases or stays the same

So then the thing is, that this will be difficult.

But see, the thing is that as we go down, there will be a point where we no longer
will join because the time jump is too high.

So I can imagine little trees below, that are based on a different time jump value.
Since the time jump can only increase... but that's not true, now is it.

Because the number of points being visited will decrease. And we may lose the point
that causes the huge time jump. 

But what if, what if we had a time jump associated with a path. So we choose the path
based on our time jump requirements. Then, when we are creating the tree, we just
create it as a top node and have multiple children based on the time jump. 
so based on the time jump granularity, we choose whether to create a split or not.

It's just that it would be possible to join back up. Yea, but only until we find
another reason to split. Well, who cares? True. And if you think about it,
at a certain level, the cache rows should match again exactly. Because they
will be identical. Have identical points. Well, maybe... I'm not sure exactly
about that. I think that may be better than having 29 mostly duplicated copies,
however. And we *can* do it. We can do anything, we have variable size points.

So the thing is, what we do is, this. Wait wait wait... what about reorganizing
the tree and killing things off. I mean geez, can we get more complicated?

Because, well think. The monster parent won't have this. Now we have to split into
two, because of the time jump. Because this monster parent has a big area (but not all)
so we wandered back into this monster parent. But then it's parent has all. So it's
parent (the mom, monstor of monster) says, wait, there is the monster parent which
could handle the point, but it's time jump is too small. So, shoot, we duplicate
it and all it's children and place it as another path. But here comes the thing.
Like that monster parent that we duplicate, we only need to duplicate the paths where
the point goes down. the rest can be copied. It's hard to think about, but the
roots remain the same. So the path the new point does not go down can stay the
same. Which is fine. But this whole reorganization of the tree...

ok, so

FACT: once we find a heavier time jump, we split down only where the point goes. 
Along this path, the time jump only increases. 

So the thing is, as the time jump increases, we just go ahead and let it. We still
need it.. But what if it increases too much. Then I suppose we end right there...
I guess. I mean if you think about it, we are readding the same point twice. Because
we need to add it for the previous time jump. So we add it again for the next one.
And... we either link back. So lets say that we have this split off. What do we do?

We first add the point for level zero (lets not consider tree reorganization for now)
Then we add it again, let's say, then we say, hey, we can add it to this guy (where
we previously added it a different guy) and it would be better, because of zoom area
or size, lets say size, so because of size difference, it would be better to add it
here. But it creates this time jump, so we copy the current node, and create a
"time jump sibling", and this tjs will be duplicated and point to the same child
the previous one... wait, now we have the point added twice. 

So that means, it's different. It uses the old copy. So you see, yes we can link back
but only where both the current and last time level point paths are not there.

So lets start over. We get a point that we need to add. We look, and it appears
that a tjs should be created. 

FACT: tjs is time jump sibling.. a sideways link between nodes

So we create it and what... because think we can't link the previous node to it
because that will change when we readd the point. So lets say that we create a new
node for the new guy.. I think we need a new term, called a "template". So the new tjs
uses the unused path of the first item as a "template". The rule is that you can 
link anywhere you want to the template, but you can't change it. So anywhere the point
path doesn't follow, the "template" can be reused. So the same with the previous
guy. So the previous guy takes the "template" from the next guy. 

So now, we are happy with this time jump, and we go down to the kid, and wait,
here comes another time jump that's too large, but would be good. So what do we do?
We create another tjs. Now remember that for each tjs we are always following only
one path. And we are splitting up from all the other tjs's there are. So we can
give this other tjs a template, and it can give it's unused path as our template
and we all keep going down again. 

So the end result is that we have all these tjs's that have all these kids,
and so forth and so forth. Sounds great until we need to reorganize the tree
because of the dragon time problem.

So lets see, the dragon time problem..

So lets see, the dragon time problem... well. darn. what the heck. 

Ok, so let's see wait a minute.

FACT: with time jump, we have to add to the children
and when we've already added a point to a child, we don't re-add it incase
of links back in.

So, now for rebalancing the tree. What do we have here.. Lets take a look:

No maybe we go like

       p
      /\ 
     c1 c2
        /\
       c3 c4

Then, we want c1 to become a child of c2 at this point, to get the following:

                p
              /  \
             c2  c4
             /\   /\
           c1 c3 c5 c6

So in this case, c2 becomes sibling to its child, c4.

Hmm, can we just make c1 a child of c2.. add it as a simple point?
That may work. 

So however, c2 has multiple parents. It has time siblings. true. 
Each time sibling can be it's own guy... er, we'll come back to that.

But what abut it's multiple parents? Well, if we consider p as the node 
we are fixing, not c2, then I guess it would be ok. The issue is that we
haven't saved anything, cause c4 is it's only child now, and we still need
to go to c4. So even if we make c2 a child of c4, we still have c4. We
still have c4's parent(s). So we'd have to replace c4 with p.
Merge c4 and p together. So that means we need to find all p's parents,
and point them at c4 and get rid of p.
and now all of the sudden, c4 is bigger. It should be the same size as
p, but I can't guarantee it, I think. No I can't. So we not only need
to find p's parents but update their size, too.

Now the thing is, that we do this all based on a point, I suppose. We
get to a tipping scale, so will we find the parents automatically, that
is the question. The answer to that is ... hmm... because the item in
question, I mean it contains the point. But... wait.. so lets
say there are happily multiple time zones. Right and one links to another
like:

    v
     \ 
     p1---p2
    /  \ /  \
  c1    c2   c3

So let's see, in this case, we add a point to mp. We need to of course go
to p2 because it is a time sibling. Right... because, well wait. So we add it
to p1, and then that adds it to c2. But then p2 doesn't even know that.

So when p2 gets the point. It must know that p1 has updated c2.. but thats
the thing. If p1 and p2 agree that c2 must be updated, it should be updated.
But if p1 says c2 and p2 says c3, then we need to make a duplicate of c2.

So it's like this. Maybe we need to store parents. Because anytime we want
to make a change, all parents must agree. If they don't, then they must be 
split up into separate groups. 

So, in other words, if p1 and p2 agree, then we are ok. But if they disagree,
we need to make templates and so forth.

And when we start out fresh. What is it that we are doing, but the following:


      p1--p2 (new)
      /   \
     c1   c2

Then, we add the point to p1 and p2. p1 says, hey we need to change c1.
Then c1 looks, and says, I have another parent, p2. What is it's opinion?
p2 says, no c2 needs to change. So we split c1 into c1a and c1b. But
 it becomes like this

      p1--p2 (new)
      /  /   \
   c1a  c1b  c2 (parent is both p1 and p2 at this point)
    
and of course, since we are pushing the point to c2 for p2, and p1 disagrees we get

       p1------p2
      /  \    /  \
    c1a  c2a c1b c2b

There we go. But suppose there are children for c1. Then:

       p1------p2
      /  \    /  \
    c1a  c2a c1b c2b
      /  \  
     d1  d2 (both d1 and d2 are children of c1a and c1b)

Now again, as long as all parents agree... we're cool. But as soon as we get a disagreement
we need to split up. So you can imagine it as a tree with a lot of duplicate children. We
are just lazy about creating them. We only create duplicates when the parents disagree on
what points a child should contain.

Now lets go back to the dragon problem:

No maybe we go like

       p
      /\ 
     c1 c2
        /\
       c3 c4

Then, we want c1 to become a child of c2 at this point, to get the following:

                p
              /  \
             c2  c4
             /\   /\
           c1 c3 c5 c6

So the point is that we add c1 as a child of c2. And we delete p, and replace it with c2
(or vice versa). but anyway, the data from c2 becomes what p is.

Now we look at the parents of p and we update them all. But what about the parents of c4.
We can just say darn it and make them parents of p. Darn. man. This. is. Hard.

But the dragon problem is a real problem. Why can't we just delete the parent that we are
trying to fix? So in other words, who cares about other time jumps. We just remove p.
And have c4 replace it. Well, that would work, but we need to update the other parents,
at least, to handle the new c4.......

But what we are talking about is getting rid of the c4. Really. Because if we add c2
to c4. Then it will become a child of it. A baby of everything, really. Expanding it all
increasing the size of everything in this part of the tree.

But we got to do it. I don't know what the effect will be.. I know it will increase the 
size of any parent... you also have to realize that we are deleting c2 from more than
one parent. So all those parents must be deleted. However, lets put this back into perspective
of multiple nodes. What if we just trim it and start over. c2 will land in the appropriate place.
Right? So the idea is that we trim c2. Then we have:

                  p
                   \
             c2    c4
             /\     /\
           c1 c3   c5 c6

Now this same scenario is going to be repeated again and again for all the weird tree scenarios.
But it's not a panic situation, because we are just handling the same situation again and again.
Just slightly changed. Because we can still have a dragon even if the time jump is not zero. 
Suppose its close to zero, but not exactly there. It would still happen.

So then. What do we have? 

We have a tree like the above. So we just join p and c4, and then update p's parents.

Afterwards, we take c2 and stick it in as another new point. Follow the same logic as if
it was a tiny little point. The only thing is then it becomes a baby. It is kind of baby
anyway. That's why we are moving it. But it might not be that small. I mean suppose it's
really big. Covers a lot of points with a big zoom area. Then we try to squeeze it into
the tree and what happens? We have a lot darn..... darn darn darn... It's so difficult.

I suppose we can do anything if we don't worry about this duplication part. which means
that we could do the following:

No maybe we go like

       p
      /\ 
     c1 c2
        /\
       c3 c4

Then, we want c1 to become a child of c2 at this point, to get the following:

                p
              /  \
             c2  c4
             /\   /\
           c1 c3 c5 c6

FACT: Whenever we make a change to any point internally, we make all parents agree or 
duplicate it

The thing is, the awfully strange thing, no it's not true. We can't turn the tree on
it's side and get good results. Just half what we had before. 

So we are stuck. Why, the trees have nothing to do with each other. Keep remembering
that. Forget about the duplicates.

We'll figure it out.

Ok, so what do we want to do then. Lets start over:

No maybe we go like

       p
      /\ 
     c1 c2
        /\
       c3 c4

Then, we want c1 to become a child of c2 at this point, to get the following:

                p
              /  \
             c2  c4
             /\   /\
           c1 c3 c5 c6

What if we just adjust the times?

If we move points over. If we can subtract. And why not?
Because we know the extent of the parent from the immediate children.
Because when we prune we will always have a node with only child, 
and that is not allowed, and when we paste we will always have a node
with 3 children.

So it always implies the creation and destruction of a node.

So lets say we have
c1 = t1
c2 = t2 + t3

We want to move t2 to c1. So we take the subtree that represents
t2 and push it into c1

So what we are really doing is the following:

         p
      /    \
    c1      c2
           /  \
          c2a c2b


         p
      /    \
    c3      c2b
   /  \    /  \
  c1  c2a ... ...


So in reality, what we are doing is
c2 -> c3, goes to left. And c2b is in place

Ok, that makes sense.

So here we go. Start simple.

We start with zero points. What do we do.
We create one.

now we have one point. What do we do?
1. We ask, can we add this point to it? 
2. If yes, then we do. Since there are no children, we eat the point
3. If no we make a copy of the point, make it a child, and create a sibling for the new
   point.

now we have 3 points. What do we do?
1. We ask each child, can you handle this? What are the parameters in terms of zoom
level if you do.
2. We determine which point to add to based on zoom level.
3. We add it to that child. Then we repeat
4. Look at time level. If there is too much of a discrepancy, ie. the left child of 
   the right node would be better served as the right child of the left node,

is this always true? can we guarantee that we can always add the left child of 
the right node to the right child of the left node? What if we can't?

What if instead of trying to fix the tree after the fact, we built it up differently.

We are always adding to the right, right? But what if we... what if we made the
parent a child somehow and added to the right as a small node?

So lets say we have

   p
  / \
 c1 c2

then we want to add another node. So we do
      w
     / \
   p    c3
  / \
 c1 c2

Then we don't need to worry. So even though p can accept it, we
don't add it to p? Well wait. Why can p always accept it? It just
marks the boundry of c1 and c2. I mean if we don't have predefined zoom 
levels, there is nothing to say that c3 might not fall outside of p.
Sure, we can expand p. But suppose that we say, hold on a second! If
we add to p than c2 will be too weighty as compared to c1! thank goodness.
So we say no, I refuse to add to p. Instead I create w, and p and c3
and then we keep adding to c3 the additional points until

       w
     /  \
   p     c3
  / \    / \
 c1 c2 ... ...

where c3 is bigger than p in time. So let me think.. yea that should work.
Yes, c1 will be small, but that's alright its a one off thing.. Ok good.

So there we are. We have it all. I'm excited, lets do this!

Ok fine, so lets detail out the steps again.

* 0 points:

1. Create a point

* 1 point:

1. If the point accepts it, eat it.
2. Otherwise, create a sibling and a parent, ie:

    n2
   / \
  o   n1

3. Update top record if necessary

* 3 points:

    p
   / \
 c1   c2

1. If parent does not accept point (based on time jump) or we can't add to
the best child... by adding we would make the tree even further out of proportion,
either in time, or area, then we make p a child like the following

      n1
     /  \
    p    n2
   / \
  c1  c2

2. Otherwise, we can add to the best node of p1, so we do the following. 
Consider c1 to be best in this case:

         p
        / \
     c1(*) c2

we repeat this exercise with c1.

* Time jumps

Now the idea here is that we may not be able to add to p with the current
zero time jump. But we can if we raise the time jump. So in the above
setup, we ask each point again, can you handle the point, ignoring time jumps?
If the point says yes, we break reality and go into an alternate universe.

If p says no, we are done.
If p yes, then we create an effective duplicate tree, starting with p,
and creating a time jump sibling link.

7/22

So the thing that has been bothering me is that we are not adding to 
a parent, even though it fits, if it screws up the children. Well
that's the rub. Define "fit". I mean there is fitting and increasing
the zoom level and just fitting. I think that if we can rule that if 
we are increasing the zoom level, that then we should reject the point
that should be fine. The only question is, is that good enough?

Well, wait so the idea before is go down to increase zoom level below.
But what if it's better if we don't. What if it makes more sense to always
go up? What if we should just leave it alone and go up. Would that change things?
       p2
      /  \
     /    \
    p     c3
   / \
 c1   c2
   


          p4
         /  \
        /    \
       p2     c4
      /  \
     /    \
    p     c3
   / \
 c1   c2
   

Wow, so the dragon goes the opposite way if we keep going up,
and lets go back and forth, I mean can we make it good?

       p2
      /  \
     /    \
    p      c3
   / \     
 c1   c2
  
       p2
      /  \
     /    \
    p      c3
   / \     / \
 c1   c2  c4  c5

Yea, that would work. Hmm.. so I do think if it fits in p, we should always
add the point. Because otherwise, what is the point of the sim? But on
the other hand, it could leave the tree unbalanced, I think.

Well lets pretend we have a huge parent with two children

    hp
   /  \
  c1  c2

       hp
      /  \
     c1   p (big but not huge)
         / \
        c2  c3

       hp
      /  \
     c1   p (big but not huge)
         / \
        c2  p (small)
           / \
          c3  c4

       hp
      /  \
     c1   p (big but not huge)
         / \
        c2  px
           /  \
          p    c5
         / \
        c3  c4

I don't think it'll be a problem. Because we only keep adding to a parent
if the guy is inside of it. So lets add another that doesn't fit in p (because
of time jump)

       hp
      /  \
     c1   x
         / \
        p   c6
       / \
     c2  px
        /  \
       p    c5
      / \
     c3  c4

Then we can decide whether to add as a child of c6 or not for the next point,
but I don't think that really matters. Well wait, there is only one choice:

       hp
      /  \
     c1   x
         / \
        p   np
      ...  /  \
          c6  c7

But now, it really does matter. Lets say there is no fit but we can expand
the zoom level. If we do that, we add to the right. If we don't we add to
the left, so we either have:

       hp
      /  \
     c1   x
         / \
        p   nnp
       ...  / \
          np   c8
         /  \
        c6  c7

or

       hp
      /  \
     c1   x
         / \
        p   np
      ...  /  \
          c6  nnp
              / \
             c7 c8


The first one is kind of better, because it means we can then add to c8 without
lengthening the tree.

But that is the thing, we don't want to add to np if it's going to make things
lopsided. See, nnp is now bigger than c6 (presumably) and that is why we
don't like it. Now, if we added even more to it, it would make it even bigger 
and nnp to become even more lopsided.

But there is more to this story, because producing something like #2 means
that the right edge is now the leader... c8 is kind of the leader, too..
I mean that future points are more likely to be added to c8, to the right
side of the tree. 

But what does it mean? We know the next point is close to c8, nnp, np, x, and hp.

But I think real point are the nodes that can't be added to because of the time jump.

In either case, the same nodes can't be added to. But in the second case,
we are forced to make tree even more unbalanced, because all the points that we
put near the current time are ones that are already imbalanced.

So is that the key? We want to in general put smaller nodes on the right so
that we don't get unbalanced even more when we add stuff to the right.

But of course it won't matter if we aren't adding to the right. 

Which we might not. But is the point that we don't want to unbalance to right
especially because in the future, those will be the only points that can accept
them.

So the balancing needs to be slanted based on when we think the next points
will occur. I mean we could even have a cache to keep track of this. 

This is almost turning into real strategy here. And i'm not sure how much strategy
we need. Yes it won't be perfect, but... you know. Because we can unbalance the
tree the other way, too. 

So let's assume this will work. I mean it will, right?

Although if it fits, I mean if it fits the parent with no zoom level changes,
So we looked at that. It's ok. I wonder if we can look at it another way. 
Like we want to keep the zoom levels closer together. Like we have two tiny
guys. I mean they may be balanced, but they aren't balanced according to the parent.

So what are we talking about here:

FACT: * 0 points:

1. Create a point

* 1 point:

1. If the point accepts it, eat it.
2. Otherwise, create a sibling and a parent, ie:

    n2
   / \
  o   n1

3. Update top record if necessary

* 3 points:

    p
   / \
 c1   c2

1. If the parent can add the point without affecting it's zoom area, we add it
   to the best child if possible or the other one if not. (based on the zoom
   balance and time balance * see below) and exit

2. Otherwise, if parent does not accept point (based on time jump) or we can't add to
the best child... by adding we would make the tree even further out of proportion,
either in time, or area, then we make p a child like the following

      n1
     /  \
    p    n2
   / \
  c1  c2

and exit

3. Otherwise, if the parent can add to the best child
Consider c1 to be best in this case:

         p
        / \
     c1(*) c2
     / \
    c3  c4

we repeat this exercise with c1.

* Time jumps

Now the idea here is that we may not be able to add to p with the current
zero time jump. But we can if we raise the time jump. So in the above
setup, we ask each point again, can you handle the point, ignoring time jumps?
If the point says yes, we break reality and go into an alternate universe, by making
a virtual copy of the tree.

The virtual copy / time jump stuff

1. In the above logic, if a node says it cannot handle a point, we ask again having it 
ignore time jumps (up to a maximum of 3 months or something)
2. If the node cannot handle the point, still, it will exit
3. Otherwise, it will create a copy of itself that will become part of a virtual tree. The
copy will link to all children of the original and increase their reference count by 1
4. 

Ok wait, we need to deal with the fact that as soon as we have a new time jump, we need 
to keep that in mind. Now the other problems is suppose we have something like this
(and already existing, now a new time jump)

    p   tjp (links to both c1 and c2)
   / \  / /
  c1 c2
 / \
c3  c4

And lets say we add to c3.. See, it effects the tjp. So anytime the reference
count is greater than 1 and ooohh see, whenever we add a tjp, we need to update
the reference count of c3 somehow. Can we inherit from p? Could it be a dynamic
count based on the parent? So when we add 1 to the parent, it adds to all the children?

Because think, when we create a tjp, we are referencing all children twice. There 
are now 2 virtual trees for all these nodes. So for c1 and c2, we can just add
1 to references. And we will always know the dynamic reference count because we 
are going down through the tree.

Now, each time we add a point, I think that we will have to cache all changes.. I
think that will be ok. So we cache the changes for each tree. So whenever we get
to a point, we add it differently using the time jumps. So then after we are done
with all time jumps and point changes and yada yada, we save. When we do this, each
point will compare itself to it's other copies and work out references.

So there are several questions about this. What happens when we add a point?
I suppose that what we are doing is changing the parent to reference a different
child. Right? Yes, that would make sense... But I wonder

    p   tjp (links to both c1 and c2)
   / \  / /
  c1 c2
 / \
c3  c4

Lets expand it:

    p   
   / \  
  c1 c2
 / \
c3  c4

   tjp   
   / \  
  c1 c2
 / \
c3  c4

Then:

      p
     / \
    nc  c2
   / \
  c1 c5
 / \
c3 c4

and still

   tjp   
   / \  
  c1 c2
 / \
c3  c4

So lets mark reference counts (dynamic ones)

      p 
     / \
    nc  c2
   / \
  c1 c5
 / \
c3 c4

-->tjp (+1) 
   / \  
  c1 c2
 / \
c3  c4


Now lets pretend we did something different for the tjp

      p 
     / \
    nc  c2
   / \
  c1 c5
 / \
c3 c4

-->tjp (+1) 
   /     \  
  c1      np2
 / \     / \
c3  c4  c2  c5

This is fine but the problem here is that we don't know what
the reference count is without going to all the parents.
Which we can't do. That's the thing, whenever we have a time jump
split, if we add the reference count to the children, we are fine.
Because if we can only reference a child by one parent, we know
where we are coming from. But if we can reference by two parents, that
is where we get stuck. So instead, it should look like this:

        p 
      /    \
     nc    c2 (+1)
   /   \
 c1(+1) c5
 / \
c3 c4

  -->tjp  
   /       \  
  c1(+1)    np2
 / \     /     \ 
c3  c4  c2(+1)  c5

So originally,  when we create the tjp, we get:

        p   
   /       \  
  c1(+1)  c2(+1)
 / \
c3  c4

      tjp   
   /      \   
  c1(+1)  c2(+1)
 / \
c3  c4

Well wait, now we still need to consider the parent of p and tjp, because p and tjp
might be different. Will be different. So realistically this is what will happen:

        mp
       /   \
      p1    c3
     /  \ 
    c1  c2

Then we add a new point and get two virtual realities:

          mp
       /     \
      p1     np
     /  \    / \
    c1  c2  c3  c4

         mp
       /    \
      p1     c3
     /  \ 
    c1   np2
         /  \
        c2  c4'

So in reality it looks like

         mp(op)
       /     \
      p1(op)  np
     /  \    / \
    c1+ c2+ c3+ c4

         mp(tjp)
       /      \
      p1(tjp)  c3+
     /  \ 
    c1+  np2
         /  \
        c2+  c4'

(where '+' means +1 dynamic reference)

So the thing is that once we create a time jump, we have to affect the top
as well as the bottom of the tree. Because the mp might change, in fact probably
will. And then it's mp might change as well. And if that happens, then darn.

So the point is more like, everything is virtual. Then we make a change for a point
given a timejump. So consider the tree being what the tree is. We make a change
with a timejump value always.

And that will be a copy. So it's like this. We start at time jump 0, and we 
add all the points that are necessary (i.e. create all the changes necessary
as copies, preserving the original tree and only doing this in memory)
Then, in a recursive manner, we make a change using the next available time jump 
which will cause an accept for the point.

---

Ok, so the thing is that each point has a range of time jumps. This starts
out as 0 to infinity. When given an option to choose another time jump level,
we split the node in two. These are not officially time jump siblings, however.

Now the thing is, this can affect everything up to the root. So as soon as
we create the split in the node, we split it's parents the same way. 

Hmm... because, see we already went half way down with the point. The parent
already accepted it. I suppose it's not changed yet, officially. But the thing
is, we may not even accept the point and need to revert it.. grr...

So what if we just do the zero time jump, create everything, and figure out
what the smallest time jump is. Then we rerun with the smallest timejump as 
the max. We figure out what the next smallest time jump is and so on.

This sounds good. But the thing is that the smallest time jump needs to be
integrated somehow with the zero time jump path.

Because from then on, if the smallest time jump is greater than the maximum,
we don't create it. There is no point, because there is always one time jump
value from the root downwards.

Ok. But we can go backwards is one option. We can start by accepting the largest
time jump and creating nodes that way, then going to the second largest, knowing
the largest. That would work. 

But the thing is we have to go backwards when we create a node. Yes, it's true.
But think of it like thousands of different realities. See, everytime a node is 
created, we are doing so based on state 0. So afterwards we have all these splintered
nodes all pointing to each other. 

Then we collapse them. We create time siblings only where one parent 0 state node 
links to two children nodes. 

Because, think, whenever we split off, and have to update the parent, we split it
off as well. So it's like this...

When we add a point we start at the parent first, it's true. Then we go down to
the children, all the way down until we figure out how to add a node to fit the
point. When we do that, we are creating a new node (basically it's state 0 is null)
then we link it to 0 state children if they haven't changed. We also have a 0 state
parent which we must duplicate and make a 1 state to make a change to it.

Now 0 state nodes have a min and max time jump built in already. So we aren't jumping
into new realities in this point, but we are finding the minimum next valid time jump
for each node. And placing it directly in the node. Right? Let's get to that later.

Ok, so forget the other time jumps for now. Whenever we make a change to a 0 state node,
even to link it to another child, we create a 1 state guy. So we go down to the bottom,
create the new node, link it properly, make changes all the way up as far as we need
to go in the tree, creating 1 state nodes each time.

Now we start out at 0 time jump. While we doing this, we find the minimum next 
time jump. If there is one, we start over using state 0 nodes and add them as normal
with the new time jump value, creating state 2 nodes. And we continue until
there are no new time jump values.

Ok, so now we have states 1...x

This is what we do now. Wherever the nodes are exactly the same, but only have
different changes in children, we create time siblings. Time siblings are just ways
of splitting up the children. But anyway, we create time siblings. These nodes are
completely unrelated, they don't need to come from the previous nodes or anything like
that. 

They just set up an actual time sibling link from 1 state to the other. 

But this brings up an issue. what if the parent splits in a future point? 
At that time, the children will all be state 0. True, but there are many of them
and the break may not be clean. You know? 

Is there a reason we can't just split all the parents up to the top? Would that help?
Let's say we did that. No more time siblings. Whenever a link changes the parent changes
too. Then for example each parent could still point to a child that had an infinity 
time jump zone. So is that a problem? It's cause then we have to send the point
down to the same zero state child twice. I don't like it.

If the parents are separated by time jump zone, this could still happen. So the
thing is that we have treat the time jump zone as the time jump zone we are working
on. So if we make a 1 state, that state is defined by the time jump zone we are working
on. 

So lets bring back the time sibling links. So now lets say the parent splits on a future
point. What that means is when we get to the time sibling links we split again. Yes,
we split again and define each new node we create by the length of the time split.
Finally we take fragments and place them back as best as we can. 

What a Pain.

So the idea is whenever we hit any node, it or it and it's time siblings must cover
all infinity timejump zone. No, thats not true. It's just that at the end all time
jump zones must be accounted for. So we start with infinity time jump zone, and
if we hit a divide (based on time siblings, always must be based on this), then
we split it into two. All the time siblings must always cover the entire infinity
of time zones. 

So again, this is what we do. We start with a time zone of 0, and find the minimum
next time jump. Then, wait... maybe while we are going down, we split the time zones
.... yes. that I think makes more sense. So while we are going down, we use existing
time zones in state 0 to determine min and max possible time zone splits.

So we investigate this time sibling with a time zone from 0 to y. Then we try to add
the point, and it's like, well with a time zone of x I could handle accept this, but
with 0 I can't. So then we split the 0 to y into 0 to x and x to y (this is all assuming
of course that 0 < x < y). Then we have 0 to x. We handle this as normal, in other words
we create a state with 0 to x, and we don't accept the point with that state. We don't
accept the point. So then it goes to the parent, but we are only dealing with 0 to x.

So the thing is, we have a work list. It starts as one time zone from 0 to infinity.

We then go downwards. If we reach a time sibling, we split it up based on the time
siblings. Then we take 0 to x, and x to y. Then lets say the time siblings says,
I could handle this with w where 0 < w < x. Then we add to the work load, and create
0 to w, w to x. Now the 0 to w goes down, and we may even split further, and that
should add to the workload. But lets pretend we don't. So then we go all the way down
with 0 to w, and create the necesssary nodes tagged with the "0 to w" state.

If we did, let's say we split into v, then we would go down and create nodes based
on "0 to v" state. We would be clear at this point and could go all the way to the top.
Afterwards we see the next work item is "v to w", so we go down with that, and 
go back up again. Since the children are always being created first, we shouldn't
have a problem. Now there may still be undo??? not sure. But let's suppose there is.

Then we start to go back up, and the parent is like, undo yourselves, children. So what
that means is the "0 to v" state nodes that were created are linked from parent to child.
So that we can just delete them all and keep going. If we accidentally created an extra
state (ie the 0 to v state doesn't make sense, this should be ok, because we can combine
nodes and create time siblings).

Ok so lets say that all works fine and good. Then we have the problem of combining the
state nodes. So what we do is be super lazy. Here is the trick, we create a hash code
of each node based on it's values, and add them to the hash. So what happens is we
find all the equal nodes, and combine them creating time siblings... But we 
only combine the ones which have adajacent (lets ignore overlapping) nodes.

Hmm, I don't think the hash is necessary... well, no because either we copy or create
a new node. And a copy is always a state node that is attached to the 0 state.
And in the end, we can audit. I mean all state nodes should be adjacent and add up
to the count of 0 state (or be non existant, right?) not sure.... think about this.

FACT: So, I think I got this:

1. We start with a 0 to infinite time jump
2. We start processing the point.
3. If a node has only one option across the entire range of the time zoom we 
   process as normal.
4. If a node is a time sibling and therefore has a smaller time range, we break
the work load to two and set the current work load piece to the size of the time sibling
5. If a node has two options across the entire range of the time jump, we break
the work load into two for each option and process one at a time
6. When making any change or adding any new node, we tag using the current work load.
7. After we are done, we combine all nodes based on work load and internal variables.
   If this causes there to be more than two children, we create time siblings. Note
   that time siblings do not need to be related in anyway (ie. do not need to be created
    by the same node)

Audit:
1. We can audit by making sure that all new states work load are adjacent and add up to
the zero state work load for that node
2. We can audit by making sure all child nodes work load add up to parent.

So here is the plan. One thing is that we don't want to add a lot of crap that isn't
necessary to GpsLocCacheRow. So I think we will use a container class.

So the idea is that the first thing we need is a WorkLoad. This will be passed along to
all classes. There is no child workload or anything like that. It just keeps getting
broken up more and more.

So what we do is start with one top point. So the first thing is to create that, I guess.

Then we have a container that we wrap over it. Or maybe even extend it. Yea, that would
be better, and easier cause we could still keep everything in the cache, I guess...

So let's see, lets start with an already existing tree.

We get a new point, take the work load and call
cp.addSegment(gpslocrow1, gpslocrow2, workLoad, maxZoomAreaIncrease, addSegmentStatus)

addSegment() will look at time jump and everything to determine if it can add. it will
basically do the following:
Take the minimum time jump and look for an add (based on the FACT rules). If yes, then 
just do it and follow the FACT rules.
Otherwise it will check for an add using any time jump available given the workload
(just a cursory search???) and if it can, will split it up the workload, and set
the workload to the first half. Then it will return "no" to the parent

FACT rules for addSegment:
1. Do cursory check for point acceptance based on time jump rules and max zoom
    increase parameter. If these are over, return "no"
2. Figure max zoom area increase for each child. This is based on the following:
   a) First calculate the difference between both child area
   b) Calculate the zoom area difference + the smaller child's zoom area,
        this is the max zoom area increase for the smaller child
   c) For the bigger child, the value is zero
2. Contact children 
   a) ask each child to handle point by calling addSegment() on it. The 
      children may change the workload, which is ok. 
   b) Compare the size of each child in its zoom area 
      1) If the childs zoom area doesn't change, choose it, unless they both
         don't change, in which case, choose the smallest.
      2) Otherwise, undo() the one with the greatest absolute zoom area

   * The rational here is we want to prevent overlap, by allowing a node to accept
     a point if it otherwise naturally would. If we keep expanding the other node
     to accept the point, we are causing the siblings to overlap.
   ** Additionally, we want to balance the size of the nodes. So we add to the 
      smallest one if there is a tie.
  c) if both children say "no", we need to create a new parent
     1. Take the smallest child, and create a parent using 
        smallestChild.createParent(gpslocrow1, gpslocrow2, addsegmentstatus)
3. Create state copy of self for work load. Place this in the AddSegmentStatus
component, using addSegmentStatus.createCopy()
4. Make changes in copy
5. return "yes"

createParent(gpslocrow,gpslocrow2, addSgementStatus)
1. create new gpsloccacherow
2. add it to




        


2. If the point can only be added with increasing the zoom area (using cursory check)
   a) ask each child to handle point by calling addSegment() on it. The 
      children may change the workload, which is ok. 
   b) Compare the size of each child in its zoom area and
         undo() the one with the greatest zoom area

3. In this case, we can add the point, but we don't want to, because 
   c) If only one says yes, finish.
   d) If neither say yes, 
       1. create a new child as a potential parent for each child along with the point
       2. Whichever parent causes the zoom area difference 
       2. add the point using addSegment to this new potential parent
       3. 
       2. create a new child with tw
       

...

So where do we 



---
wait if it's zoomed in really close, then, well it's the same. The difference
is in the time view. If we have a close up time zoom, with little or no time jump,
what do we do? We do this. We will be searching based on time mostly. 

So in that case, if there are two points, and one has all the time and the other
doesn't we will be lopsided, right? But if their areas are lopsided and their times
aren't then we will be more messed up, because the area will always be an indicator.
The time not so much. 


7/24

FACT: gosh that's too much. Lets not spell everything out so much anymore

Ok, so the question is whether we want to use the estimate for the child to
determine whether we want to add to the child. Or actually add it for real.
The issue is that if we use the estimate, I mean, it will increase, it could
increase and unbalance the tree some more. 

But if we don't, we still need to have a general idea of whether we can add
to the child to prevent a huge amount of work... we need some type of cursory
examination.

So maybe that's it, we have a cursory examination. If that goes through, we
then add the point. We know that the cursory will be the minimum expansion,
so we can use that info. Ok. Sounds good.

Ok, so the issue is that we must add to the child if it doesn't increase zoom area,
but why? I mean really, why does it matter if it's a little outside rather than
straight smack in the middle. Why is that an absolute requirement?


Ok, so the basic idea is this:

FACT: We start out with an unbalanced pair, where the fat side is on the left. 
When the right side raises enough to be balanced, we create a new node on the 
right of a fat guy and do the same thing. 

And that's how we keep balanced, because the time jump
will assure a guy on the right.

So, of course there might be unbalanced children under a balanced parent,
but the parent is more important since parents will be visited more than children.

Of course we could start deleting points, to fix this, but that's a lot of code.

7/25

Ok, so we need to create copies of the node.

Now, there are two things that may happen, a node might change or a
parent might be added... inserted inbetween.

So when we change.. we are basically associating the data
with the 0 state guy. now this hash is per point being added.

So what about the new parent?

A parent is just an addition. So what is the best way to represent this?

Well maybe we can do this by id. However the issue with this is that
when we create a new node, the id is zero.

I suppose that we can we figure out the id based on what we are loading.

We could also use a negative id. But there are a few options. We could
extend gpslocrow, maybe that is the best, because we still need to keep 
the id the same.

That would make a lot of sense... lets see, because if there are different
links according to the time jump... then ... wait is there a case where
a smaller time jump links to a greater one? Well it could. But it doesn't
matter becaues when we use the cache, we just have a point in jump time
land.

So lets stick all the creator stuff in a new package.

7/26

So what about time jumps... We do a time jump whenever we can, I guess.
Or whenever the max, will we have a max? Yea, for less bugs.

I'm not sure what to do about this joining problem. Because even new
rows might be joined together. Is it possible for time jumps to overlap?
I don't think so. Well it could could be.. I suppose. I wonder, it seems
to me that really what should have the time jump is not the node itself,
but the links to it. 

It's just that... well it probably won't matter. 

So lets see. I just want to know what a time jump means. It means
that the points of the cache may have points that are not continuous
up to the amount of the given start time jump.

So maybe we should get rid of time jump end. It's confusing.
Yes, yes, that makes sense. So the point is that we may start out with
a time jump of zero, which means everyone can come in.

Then there might be a node with a higher time jump. Only certain guys
can come in there. So we always have a backup for lower time jumps including
time jump 0.

No, we always have a backup at most the size of it's parent time jump value.

And there becomes the point. Because it might be like this


     n1 (tj 5000)
    / ...
   n2 (tj 0)
  /
 n3 (tj 5000)

But the point is, we never want something like:
     n1 (tj 5000)
    / ...
   n2 (tj 0)
  /
 n3 (tj 0) -tj> n3 (tj 5000)

because it's pointless. We might as well just get rid of n3 (tj 0), because
it's impossible to find your way there.

Right now we use workloads. We have a min and a max time jump. We calculate
that starting from the top, so yes, this shouldn't happen and we don't need
to worry about it.

But then comes the thing.. Suppose we have

    n1 (tj 0)   -----------tj>              n1(tj 5000)
    /                                       /
   n2 (tj 0)                               n2 (tj 0)
  /                                        /
 n3 (tj 0)                                n3 (tj 5000)

So, can we and should we combine n2, like this (note this won't happen very often)?
    n1 (tj 0)   -----------tj> n1(tj 5000)
    /                         /
   n2 (tj 0) <----------------
  /                                       
 n3 (tj 0)     -----------tj>  n3 (tj 5000)
 
Sure... especially since it doesn't happen often. 
I think we are going fine with this. 

Ok, so we combine anything where all the variables are the same,
and make multiple tj children. This whole child1, child2 thing doesn't
make a lot of sense, though. We should just have siblings I think. There
is no reason not go through them all (especially since there are 2)
and there is no reason to limit ourselves to a hardcoded number of siblings,
and I don't think there is any reason not to include time jump siblings
as siblings. So when we combine we treat it as a sibling as any other.
It's just that we order these siblings by time jump value, so we don't have
to search too far. Now, does that make sense? There are only 2 children
max per time jump level, anyway. 

It should work, it's just that the data is still cloudy for me. 

So lets consider home, road and work again.

So we start at the root node. We create a bunch of tiny nodes at home
then road and then work, and that becomes a time jump zero tree, and it
should be well balanced, because of the way we build it up (by always trying
to balance the size of children). And I still think it should work.

So now we built that up, and we go back on the road. Because of time
jump, we will create two (or more) universes. One where we reuse the same
nodes, and the other we we create brand new ones. Now what we are saying
is that to get to the brand new ones, we start at the all encompassing ones.

No, we aren't saying that. We start out, and say, ok we need to split up
the top node! To deal with this other workload. So in other words,
we keep going through each top node every time until we get to the right
split amount. Now, the other thing is that we combine this data. So 
because the top node is going to be quite fat and cover all the known
area, it will end up being one again. But once we get to home,road, and
work shaped nodes, that is when we have time siblings. So we go through
them. But it seems rather efficient. I think it will work for this.

GOOOD!!!!!!!!!! Lets do it (later) grr....

So, we have a hash of each guy... oh we should make a hash set of
RowWorkSets! We create a new RowWorkSet and just merge them I guess.
A little sloppy but not too bad. The alternative of using Integers
would also be an object anyway...

7/27

Ok, can we handle a single time jump.. I mean it's time jump start or
time jump end, too. It's time jump end. Which means that the time
jump will be at most "x". So the point is that if the time jump is 1
year or something, then, no you can't use it for anything less than that
But why is there naturally a gap? I mean there could be a gap. I mean
what does that mean? It means you've gotten off track. It's weird though
cause it throws out all the auditing we were doing. We could add it back,
I suppose. That's a testing thing..

Ok.. so the problem is that we need to create new parents of children.

So the problem is that the children may not be rowdatas, they may be just
rows.

So the issue is that when we insert a new parent.. we , ok, we insert a new
parent in a tree. But the tree looks like this

    p
   / \
 c1   c2


So this is what we do normally (before siblings)

      n1
     /  \
    p    n2
   / \
  c1  c2

But actually, because we want to keep p at the top, for various reasons, we do this

       p
     /  \
    n1    n2
   / \
  c1  c2

Where n1 becomes a copy of what p was before.

So in the new system, it's like this:

    p
   / 
 c1-->c2

And what we want is:

       p
     /
    n1-->n2
   /
  c1->c2

So when we call promote to parent,
we want to take n1, and set it to the child of
what p was, and make its sibling the new second
child.

Yea, that works fine the way it is.


Wait, we confused this a little still.

This is because of time jump siblings... wait did we?

Ok, so lets think... we need to make siblings, but is it
only in the case of the data being the same? If so, then
why do we need time siblings at all?

The idea is that sometimes we can accept a point where we 
otherwise couldn't in a time jump. But, you know I really need paper.

So lets' think, anyway. Lets go get paper. grr.

oh, the links change, but yes the data is the same, I think.

But the thing is, the root node, it may siblings where both change.

Ok... that;s true

So I think that besides the root node, we only need to make sibling links 
where the data is the same and that is only to save tree data..

but wait, why do we have sibling links at all? 

So wait there is a problem. No there isn't.. the tree stays balanced and the
root moves down.

So time jump, is it max or min? I suppose that time jump corresponds to the node
itself. What is the max time jump inside that node and that node only.

So basically, we delete a lot of nodes and create a lot of new ones.

What we are doing is for a particular timejump range, we create the "best" node.

Now we have a timejump range. But for any timejump after the start of the range, this
should work. There could be better, but this would work. It would be legal, in other words.
So, we end up with a bunch of nodes for each state. Right, I mean they are separate 
beforehand. 

So we have a time jump range, and we find the best children for that range, based on
the max time jump. A range is what? We start out from 0 to infinity. Think of it as 
threads. All these threads correspond to the same nodes. So taken individually, as
long as a child has a start timejump below the thread, it can use it. In general,
though, we go for the highest time jump under the thread. Right, so the end time jump,
it is the maximum time jump we can accept. So we start with a node, then we create 
a node underneath it that has a certain time jump, it may affect the above node, in
which case we create a copy, or not, in which case we don't. Now the 0 state nodes that
have been touched are all removed and replaced by all these copies. These copies are
either linked to by the same or different parents. 

So two nodes can be time siblings only if their parents are the same. Which is the data
I think we are losing. Because when we create a time sibling, we start with the 0 state
node as a parent. We know the 0 state node which is the parent of it. However it becomes
weird when there are other parents besides the 0 state node due to existing time jump 
siblings.

Ok, so we know that we are really only focusing on one parent, but that doesn't mean
there aren't others. Which is why we need a reference count. 

The deal is this, we are changing a node for a particular time jump range. But what 
confuses me is there are several time jump ranges that can be covered by one node.
Yes, it's true. But the thing is that each node is specifically handling the needs
of one or more time ranges. So what happens is the time range and the node break up,
and if the reference count (ie. the number of time ranges being handled by a node)
changes, we have to consider deletion. But here is a question. We split up time ranges,
so if a node has a reference of 1, and the time range splits, it needs to be changed to 
a reference of 2. Ah... yes, I see. So that might be the issue. We have each node having
a max time jump, and that may be true. But that does not specify the time ranges being
*handled* by the point. Each node has a certain set of time ranges that it is created for.
So the thing is, when we split a time range, we still want the nodes to exist unless
they have zero time range left. Of course, all the time ranges handled by the point have
to be greater than a certain minimum which is the points current time jump. But, 

FACT: it doesn't matter what the max time jump of the node is, just what it can be.

FACT: nodes contain a set of time ranges that it is currently handling. This allows us
to decide what points a node can handle, and tell us when the node is no longer needed
(because all the time ranges have created a new node)

FACT: the max time jump of the node must always be before or equal to the minimum time
jump of the time ranges.

So again, the deal is that when the time ranges of the state 0 nodes go to zero, we
can delete them. The time ranges they are, that lets us know which node to start with.
I mean that we start at a parent, and then we go to a child. So we start 

Questions:

How do we deal with time jumps before we start? Will we screw up parent to child nodes?

So we have a maze of nodes. Then what do we do? We said that everytime we get to a
time jump sibling, we take both paths. Because a node is only responsible for it's
time jump range. And when that time jump range goes to zero, it is not responsible
for anything. Now. That is the important point. But there is more. A node can
only handle a certain level.. in other words, it's max time jump precludes it from
handling any other time jump below this minimum. 

Right? So what we have here are nodes that are held for a certain time jump range, but
can handle other time jumps. Now is this important? I'm not sure... I'm really not sure.

So we have this tree for a certain time jump. Just think about it as a million parralel
trees, that are all the same. Until there is a time jump... which there will never
be for the first few nodes, we won't split. We will always be the same darn tree.
But as soon as we split... now think of it. Once we split, we will probably always
be split. This is because all the nodes will be slightly different. Hmm... should
we make that always the case. That the further we go down, the... hmm will we join
again? Yes, because once we go down a new trail, that we have never gone done before
it will be new.

so we split and then rejoin. And so think, it's like, for every thread we already
have a path that we go down. And it's based on two things. The children nodes, and
the time jump. We are trying to find the largest node which has a time jump under
our thread. So then we add a point. And for each thread, we need to figure out 
under which node(s) this point should be placed. So the thread determines the time
jump, and we have rules for which node can accept the point and which cannot.

So we start to roll through the nodes for all similar time jumps. We make changes
to these nodes, thereby reducing what was. When we make these changes, we 
may make changes to the upper nodes as well. A complete path is formed based on
the time range. So think in these terms that for a bundle of time paths (a time range)
either a node is created afresh (ie changed) or it is kept the same. But get this,
that doesn't matter... and what if... what if we changed each and every node. 
Of course we can't do that. We can change each and every node along the path. And 
allow it to rejoin to state 0 if it can. Now... here is the thing. There are nodes
not along the path. The path is unidirectional. If we replace them all, there is
nothing that can link to them. However, they can and will link to state 0 nodes.

Because lets imagine it the other way around. Let's say the normal state is infinity
time jump. When we go down in time jump, we are inserting a new node, usually, which means
we are diverting the stream. So we place as children of the same parent two different
paths. And that is the important part there. Time jump siblings are simply where
the streams diverge. Now, they may join back up again. But wait, so they diverge
Which means that we are taking away from state 0. That is the important point to be 
clear about. When we create another state, like state 1, we need to take out the 
data from stream 0. That's why I was always confused about why there is a difference
and always worried about diverging streams not reaching the nodes. 

So we take a chunk out of stream 0 to get to stream 1. We are subtracting from it.
Ah.

So it's like a bank. Each node is stream 0 has a bank of threads. When we pull them out
to create stream 1, we are withdrawing from the bank. Now, think about this.
We may change the *parent* as well. We create a new child, by changing the existing one.
What we are doing now is we now have this new child with a specific time range. That
is subtracted from the stream 0 child. However, we don't have anything to link it to.
So we create a stream 1 parent to link to the stream 1 child. And then, that is 
how we link it. So it's like this. We have a stream 0 child, we build the stream 1
child out of the stream 0 child by subtracting the area which is changing.
Of course, when creating a new node, we are not subtracting from anywhere. It's just
a diversion of path for those guys only. 
So that should work. I don't see any holes in the design. really.

FACT: when creating state x nodes, we need to subtract from state 0 nodes, thereby 
changing both.

So then if you think about it, we are separating all the parent nodes, and we have
cut out the area from the state 0 nodes. So the state 0 node paths remain the same.
The state 1 paths are going to hook up to the state 0 node path in the direction away
from the new point. 

FACT: So where the stream x is not following the new point, it links back into the
stream 0 children.

So stream x guys that haven't changed from stream 0 do not need to be changed. but
this would generate time sibling links. But if we don't allow parents with different 
children to be merged back we could do away with time sibling links except for at the top
level.

FACT: It is ok to go from a big time range to a smaller one when adding a point. 

The above is because what we are truly following is the minimum of the time range. And 
while we are at it, we are proving to ourselves the maximum time path that follows the same
path. 

Can we get away with only having the time jump range when we are adding the point?

Yes, I suppose so. So back to the beginning in a method of saying. But then, well
so when we have the time range... what do we do? We create a new node, and
the zero node, how do we know if it's empty? Because, we start at the beginning
for the node.. But, we may not follow all paths. So if we aren't following a path 
because the point doesn't demand it, but we are, too. I can't think that far ahead.
So we need them.

But there could be another point that needs it. That's the key I think I don't see yet
When we say a time path, we are talking about a certain point... No we're not, we're
talking about the whole tree. So the point is a certain tree, a different tree might
need the node for an entirely different point. A certain time path, an individual
time path might be a node. It's true. But we don't know what trees a node is for.
And because so when we subtract out a tree, we still have the nodes left. So we 
don't know what trees the node was supported by. And that all depends on the parents.
Which we aren't following:

FACT: We need a time jump range for each node. 

The above is because we don't follow all links from parents to children when adding a 
point. So there could be a parent that we don't know that is pointing to a node, and
because of this, we will never know when we can delete it.



Ok so here is the deal.. there are two types of row copy creations:

1. We alter a node. Here we have to subtract out the path threads (thread range)
2. We create a brand new node.
For 2, we are doing a trick to keep the top node top. This is to protect the topRow
to keep it at id 1.
In this case, what are we doing? Are we altering? No.. we already drew this:

    p
   / 
 c1-->c2

what we want is:

       p
     /
    n1-->n2
   /
  c1->c2

So we may alter p, or we may not. So we have to create a new p, either way,
right? We need to have a handle to p. Because p is now going to change. It
needs to be changed using a row copy to link it to n1. Now, what we are doing
now is that we are changing the child first. Then the parent.

So in this case, we changed c1's children... ok. So what?
Then we create n1, and we want to alter p to point to it. However, afterwards
we will be changing p again. And that is where the issue lies. So what we need
to do is return n1.. (luckily we have the sibling link, so we don't need to also
return n2). 

FACT: we are keeping the following true. A node only changes itself during it's addPoint.
If a child node wants the parent to link somewhere else, it returns the new child.

Ah... there we go. There is another missing piece I think. That is that when the child
changes, we get a state 1 node. Now the parent which is also state 1 shouldn't be linking
to state 0 anymore. It needs to link to state 1. Of course! So we return the new child
even if it's just an alter! Awesome!!!!!!!!!!!!!!!

FACT: whenever we alter a node using addPoint(), we return the copy so that it can be linked by the parent.

When we replace the second child, what do we do? Because, I mean, we are updating the 
sibling to a state 1 node. Now we can only go one direction, so we won't be updating
the child1 node at any other time. So, what we do is whenever we change anything,
including the links, we create a new state node. See, that is how we hold on to things
without links. So the child will get a state 1 copy. And that state 1 copy will have
the link replacement. When the state 1 copy gets merged back to state 0, we can then
create the time siblings for the nodes... Right? I guess.... bla

FALSE: whenever we change anything about a node, including it's sibling or first child link we need to create a state x copy
7/29 FACT: whenever we change anything about a node, including it's first or second child link we need to create a state x copy

7/28

Ok so the problem is that we need to associate a row to a subtraction of data.

We already have a hashmap to combine new rows together. Now the thing is, we also need
something to associate state 0 with the subtraction.

These are different things. The second is just a set of ranges. Yea, I know.

Now the combination is a bunch of rows each with a work segment.
So I think both things can use this new class which is a collection of work segments,
or a worksegmentset gosh doggoneit!

Ok, so for the state 0 nodes. Are we going to save the 0 level parent and make time 
siblings? Because it's special... nothing will ever link to row 1.. it would be bad
Everything else, I don't care about. But if we can save row 1, we are cool. and good
and whatever.

Now the thing is, we don

ok for saving this stuff. We have the old state 0 rows and the
new state 1 rows. I'm not sure how a database works, but I have a feeling that
it would be better to update then to delete and insert.

And that fits right into the top row issue. We want to keep the top row.

So I think that we need to mark top rows as top rows. 

And that will let us know that they need to be siblings of the top row.
Now as for the rest of the stuff.. lets see,
ok, so oh... so we need to include state 0 rows in those that we are merging.
That is how we tell if we need time links or not.



--- ok here are the problems

state0
topRow


So when we combine, we combine state x stuff with state0 stuff.

Then we have some free extra state0 stuff which should be reused (becaues I think
that updates are better then insert and deletes) I'm just guessing though.

So we need to do the following:
1. combine state0 and state x nodes
2. Any state0 row that is empty and not combined, we can update (parents should
all be gone.. this makes me nervous, so maybe we'll only do this for the top row)
3. Create tjs links for children of combined nodes.

Ok, so what we do is go through the Combinable Rows. If we find an empty 0 state
row, we will stick it in an update (or a delete) bucket

I just did a test and a new row always get the latest id, so this will
let us test for links that still point to old rows if we just do insert and delete
for everything but row0.

But the other problem is that rowData specifies whether it's top, but row does not.
I suppose there are a couple of things. The first is that any row with a 
0 id or a minvalue id could be considered a non zero state. Also, then we 
could just create a rowdata for the top and 0state and mark it as zerostate

So here's the deal:

We have:

rowMap = new HashMap<CombinableRowSet, CombinableRowSet>();
row0StateToWorkSegmentSet = new HashMap<GpsLocCacheRow, WorkSegmentSet>();

RowData - this is a row, except that it has actual references to other rows
(since we don't have ids yet), 
isTopRow
isState0

rowMap lets us combine the data.. and because sometimes it doesn't change from
state 0, we need to add the row0's from row0State to it. 




Whenever we change a row0 for a time period, and create a state, we delete the
work segment from row0. Since we are going down nodes in state 0 always, and
make modifications to it, we always know the row0 we are dealing with. We subtract
out the work segments that we make changes to.

Then, we add row0 data to rowMap.

Finally we go through rowMap. For each combinableRowSet, we:
1. 


So what we do is:

1. go down through the existing tree with a workset. 
   a As we go down, if the current node has a start time jump that is after
   the current workload time jump start, go through the tjs links to find the right
   one.
   b. If it is before or equal, we're good. We split the workload, though,
     to the end_time_jump
   c. We need to split against the children, even if they don't match. This is because
      although one time jump sibling doesn't match, the next one might 
   d. (audit point, if the end time jump is less than the workload time jump start
       assert fail)
4. Once we get to the bottom, we come back up. While we are doing this,
   we create new nodes using the current workload (which was split in 3)
   a. When we create new nodes, we subtract from the existing state 0 node,
    by populating row0StateToWorkSegment and deleting out the replaced work segments
   b. We also update rowMap with new created nodes using rowData. (We use rowData
      so that we can use real references since in some cases nodes don't have ids,
      or we have multiple copies of the same id)
   c. We also mark nodes that are topRow as topRow. (We do this because we need
      to know where the tops of the trees that we are creating is)
5. Now we populate rowCombiner with nodes using add0State(). (We do this
   so that we can combine with 0 state nodes. If 0 state nodes don't change, but
   instead some time jump links increase, we want to do that rather than making an
   exact copy of a 0 state with an x state).
   a) we record the id's of all state0 nodes in the combined rows, so that we can
      keep them if they are non empty (to avoid the state 0 dead parent problem)
   b) *the above is true except for top row items, which we just set the id to null
      (or integer min value)
6. *Go through rowCombiner looking for trash. Any combined row that has a state0 node 
   but is otherwise empty (no valid workstate), we throw into a trashcan for use.
   a) *If there is a row1, we put row1 in a special trash can. 
   b) *If there is a row1 state 0 node that is already there, we demote it to
      a state x (??? do we really need to do this???) (so that we can use
      the state 0 as a fresh copy)
   c) *We find all the topRows there are, order them by timejump descending
7. *Place toprow 1 in top node, and use the trashcan/insert for the rest. (actually
    update the database in this step)
8. *Go through the rowMap again looking for non top rows
   a) *Merge all combinable row maps. 
      1. *Sort the rows in a combinable row map high time jump first. state 0
           node is treated like a regular row
      2. *Go through it's immediate children, linking it's last node to the
         start child of the next combinable row. (This is an audit point to make
          sure the children match the parents and there is no overlap)
9. *Go through the rowMap one more time for non top rows
   *a) Save to db using trashcan or insert for the rest.



---
The only thing with #7 is that we want to go high time jump to low,
because we want to find the highest time jump start that is under the 
necessary one. So we want row 1 to be the highest time jump start that is
a top row.

7/29/10

Ok, so the issue is this. We need to combine the rows, and the working sets
Now when we create a row, we subtract out the time periods from the state0 guy.
We are combining rows pretty much at random. There is an underlying assumption
that two cache points can't be unrelated (ie come from different state0 nodes)
and cover the exact same area. Because if that happens, we can't merge these 
guys, because they have ... wait what happens to the parents when we 
merge? I think there is something I really don't like about this, but the alternative
is not great either. I don't like that we merge at random. 

I can understand why we do so, though. I'd like to wave my hands at it, but grr,
this whole thing is so hard! Darn I never thought... 

I mean I know its possible for siblings to overlap.
I mean, lets say that there are some overlapping time ranges, what does that mean?
What it means is that you are never sure when you can delete a row, because you
are going down a different path, it's bizarre. Ok, so here is the thing.

We need to think through this again. Why do we keep state 0 nodes? 
Just for speed (and the row 1 issue). Why do we merge? Just for space constraints.
So here is the thing. We throw these nodes into space, like DNA. And they find
each other and link to each other. There is no requirement to link two nodes if
they share the same data. We only do so if they have time jumps that can be
patched together like a quilt.

Ok. so we create all these nodes. When we create them in this DNA land, they
each have their own start and end time jumps. The only exception is state 0
where a bite can be chomped out. So when that happens, we create two state 0
copies. But wait, why? Ooooh.. here comes the issue. Suppose that we do take a
bite out of the middle. Then we have a state 0 node which may have a parent 
from someother non touched state 0 which has a random timejump.
But wait, don't we have that problem anyway?

So I think the point is what we allow. If we allows parents with random time jumps
to link to children. Then, so the issue is that we can't allow two different
parents to link to children. Because then, if we get a point that goes down the
child, and we start splitting the parent into two time jumps.. then, wait,
why is this an issue? 

One parent links to a child. Wait the child says which time jumps it's responsible
for. So we are allowed to split it as long as they add up to the same time periods.

But can we arbritrarily increase it? The point is that we are replacing a child 
for a particular range. But my point is, why do we care? I mean you talk about 
deleting a child that has no space left in it's time jump range. but a child
that goes to infinity time jump, I mean... who cares? We can do the same
with only one. Just consider the end goes to infinity. 

Ah, but here is the rub. We don't keep parent links. See, so suppose a child
goes from 0 to infinity. Then we create a new fangled 5 to infinity child.
See? how do we point everything to the 5 to infinity child first. But with
multiple parents that's a rub anyway. And that's a big problem, because even
if we handled this, we may need to increase the size of the parent. In other
words, we have to increase the size of *all* parents.

So the point is that we should never point to an existing node that is pointed
to by another parent.

FACT: We can only have one parent per child per time jump range

The above is because if we have more than one parent, and its dead (ie it's not
visited to when we are processing a point), it won't change size if we split
the time range.

Ok, so that is settled. I am starting to see this better now. 

Alright, we may not need a range. But I think we should have them for
auditing purposes, anyway, so no need to think more about that.

Alright so one parent per time range. Is it true? I sound like janey from korea..
haha. Ok, is it true? Because it is possible that two items could run into
each other. Ok.. I don't think it could happen, but I'm unsure. Anyway, we'll
code in a way that prevents this issue. 

Now the other issue is that yes, there are multiple parents. They will always
be for unrelated time periods though. For a particular time jump strand, there
can be only one parent. For a particular time range... well it should be ok
because we are dividing to the point where time ranges are all going to the same
place.

However, what happens if there happens to be multiple parents for the same
diviable time range, exactly the same? Is this a problem? Yes, because we are
always assuming one parent, so when we reach a child, we feel like we can
change it. Ah, so any overlap we have to drop. 

FACT: We can never allow two parents for the same time range to point to the same child

This is because we will end up with the dead parent problem, where one of the
parents aren't visited, so when we change a child, we are messing with the dead
parent.

So here is the key... when nodes overlap, even if they exactly overlap, there
will be only one parent per time range. We will hold both parents then, so we will
know their children. So we don't have to worry about id's being the same, etc.
because there is no dead parent problem until we create it. And we won't create it.

FACT: We don't have a dead parent problem for a given time range. We do need to 
respect the id of any non empty state0 node because there might be dead parents 
there (which have different time ranges then those we are examining.. except for top row)

So, given all of that, we have the following extra things to remember:

FACT: We will always have a live parent for every child (besides the top row) and
we can handle any change to node ids.

We will have to change node ids, because as soon as there are changes to a 0 state
node, it splits into another. So where a parent used to point to a child, it now
points to another. Oh darn! What that means is suppose we have a dead child that
happens to be the first child. It would have to be split too, right? 
In other words, a parent points to a dead child, and splits in two. Now the dead
child points to two live different siblings. Or needs to. So we'd have to split
the dead child up. Or go back to child1, child2 and time sibling fk.

Hold on, lets think about this. No, it doesn't work. We have no mechanism for this.
It was assumed that child1 and child2 would share the same time range, and it's not
true.

Darn.

FACT: We need direct parent to child1 and child2 links, and separate tj sibling links

This is because if child1 is dead and child2 is alive, a sibling link would have to
be created from child1 to child2 and child3. We don't have a mechanism to do this.

I suppose that we could link child1 to both child 2 and child3. But let's think,
it could go the other way, too. What do we do then? It becomes a mess. No, we need
to have child1 and child2.

Ok, back to the issue, now that this is fixed.

We always will have one parent per child. We can handle this by preventing overlapping
DNA from matching.

So what we have here are a bunch of strands, including state0 strands. state0 strands
are the same as state1, only the ids musn't change. So what that means is that if a 
clump of DNA has a state0 strand, then it picks out which piece of garbage to recycle
itself into. (sounds crazy, doesn't it)

There will be some empty DNA strands with attached state0 nodes. 
(audit point) There should never be any non state0 empty DNA strands.

So each dna strand will have one and only one end point that can be placed
into a hash map (or a hash set). Then when we try to find dna strands to add
to, we can search based on this. Actually we'll need both head and tail placed
into the hash map. So what we'll have is this

HashMap<Integer,DNA> headToDNA, tailToDNA;

Then we create DNA and put it in there and join it as normal. We must remember
to reinsert dna that maps wrong.... no, wait, that won't work.

Because all the fields have to match, for the DNA to match. It's an odd thing.
We have to have a jointed, two object thing.

HashMap<HeadDna,Dna> headToDna;
HashMap<TailDna, Dna> tailToDna;

And
HeadDna Dna.getHead()
TailDna Dna.getTail()

So we add to these hashmaps, and that's that. The DNA joins up automatically, and
we are done. So what we got is

7/30

But we got off track. Chomping the middle. What happens there? Does it happen? Will
it happen?  I'm not sure.. We'll just assert it false and see the condition that leads
to it and decide if it's right. I want to stop thinking so hard. This baby has to be 
born!

Darn, ok, what about time jump siblings when we combine?

So all the new rows won't have any. The state0 rows might have some, so we need to
be careful. We'll do an audit check. Only one state0 row per combination.
That should do it. If we get more, we'll analyze the situation...

7/31 

Ok, now, the fact we want to say is:

FACT: All top level rows will always have empty state0 rows.
Yes this is true, because
a) We go through all siblings of top level everytime.
b) Everytime we go through a time range, we subtract from the state0 stuff

So what this means is that we do not need to worry about pulling a state0
row away from the row1 id. But we still need to know whether a row is a 
top level row or not. Because that is how we decide to link to top level

So how does this fit in? Well, instead of setting up the state 0 row id 1
as an automatic top row, we can just assert that it is empty.

We have rowcombiner, and that is all well and good. So after we add everything
to it, if we find a top level, we can verify that state0 is empty, it should
always be (for auditing) and then we handle it separately from the normal
path... ?????????? update 1,2,3 steps ?????????????

So my question is, what if an item points to a state0 node...

Ok, so the thing we have to remember:

Wait, so is it possible for two parents from different time ranges to point
to the same node? I wonder if we should prevent such a thing...

Because the issue is, we need to preserve the id for the state0 node if it's
not empty (if it's not empty, that guarantees this condition, because where 
else would you get that node?) 

Now remember, that time siblings are at the bottom. Now, that also brings
up a point. Suppose that we have a state0 ..

but wait, to simplify, because when we combine, it's usually because
the time jump split between children, but there is only one parent.
Now, let's say the parent splits next time, and the children are already
split. What does that mean?

It means that, I mean does the child's nodes have to match with the parent?
Can two time ranges point to the same child? Yes. Because if we have
a child without a time jump, all the parents will point to it.
Conversely if a child has time jumps, and a parent doesn't, it will point
to two.

So there you go.. there will be dead parents because of the situation

    p1 (tj)    p2 (no tj - almost tj))
       \         /
         c ('tj' to 'no tj' -- because it just contains one point) 

So if a point has a tj, it will land to p1 (but then not added to c). --
 however the above situation is still useful, since when looking up, we could
still find child. 

So if another point came through, it would go through both p1, and p2, this is because
it would start out like

                n
         /              \
        p1(tj - no tj)   x

and (let's say that p1 and p2 become different sizes... even if they do, if n is the same size we still will have the same thing)
                       n
           /                            \
       p1(tj) -> p2(no tj - almost tj)   x
        /
       c (tj - no tj)

or, if n does change then:

                    p
	         /                    
            n1 ----tj-->   n2_________________
           /  \            /                   \
       p1(tj) (also to x)  p2(no tj - almost tj) x
        /
       c (tj - no tj)

Ok, so suppose the above was state 0.. very interesting situation
????????????? what happens ... how do we preserve state 0 nodes in various
     point additions so that a parent that was there before still points to the
     same node again ?????????????

So lets see, let's say that we add a point that belongs to x. 
And lets say it has zero time jump. 

What would happen?

                           p
	         /                   \---------------------------
            n1(tj)   ----tj-->  n2(no tj)                        \
           /  \               /                    \              y
       p1(tj) (also to x)    p2(no tj - almost tj)   x
        /
       c (tj - no tj)

But it belongs everywhere. So lets see, we start at p
then we see it has two children that are time jump separated. This means
two identical trees, right? So we should check them both... 
so the simple case is that both can contain the node.

So what we do is split up the workload. 

Splitting up the workload doesn't mean the node accepts it. That is the
important point to take here. But we don't need to split the children of
the children unless the node accepts it.

So now, what happens? We put the new point down p, which accepts it.
Then it sees child n2(tj) and y (it sees n2 because it immediately
follows the time jump to find the node that contains the start of the workload.
Then it splits the workload based on n2(tj), so that n1(tj) will also get called
It also gets checked against y in both cases (because y contains the workload in both
cases).
Then, in our scenario, n1 and n2 both accept tj. Now lets say x accepts it, in both cases.
What happens? Well, two copies of 0state x is made, both of them are changed the same
way, and poof, x is updated. Yes! You think well, my friend.

-->case: n1 accepts and n2 rejects, so we go down y

p accepts it, then for no tj, we create a state 1 y and create a child. Then
for tj, we create a state 1 n1 and go down it, too. If p stays the same, we 
create a tj for y. 


-->case: n1 rejects and n2 accepts, so we go down y


---

Wait, so what about cases where a dead tjs links to a live one?
Well, we always should be pointing to the max tjs and we go through
each tjs, so there is no such thing.

We create a tjs when:

the parent is the same as the other. 

So to start out with, the parent will point to the greatest time jump sibling.

Now, what happens when we split the parent? Let's say the parent gets a tjs, too.
What happens then?

So the situation is

             p(tj 0-y)
   /                         \
n1(tj x-y) ----> n2(tj 0-x)    q

FACT: TIME_JUMP_START is the maximum timejump allowed for a point

Then we get something to separate out p. let's say time jump of w where w < x for 
n2.. . So first we try to add the point, then when we hit n2, given that the workload
starts at 0, we do this. So the time jump max is 0, remember this. So we see,
hey the time jump would work, so we split the load. And then we get 3 workloads
0 - w
w - x
x - y

So we go through 0 - w, and what do we do? For p, lets assume it's everywhere.
We create a p(tj 0-w), and it's accepted, because the point has a 0 time jump
for p. Then we go to n2, and it's not accepted for a tj of 0, so we add it to q
or create a new child, etc.

Now we go through w - x, and what do we do? We create a p(tj(w-x)),
then we go into n2 and create a n2(tj w-x), and it accepts it

But the question is still, what about dead time jump siblings? 
Ok, so from 0 - w, we add to node "q". So let's say q accepts it (which it should),
then we have 0 - w state for q. But q goes from 0-y. Right, so it splits in two.
Because there is a 0 state for q already. 

So wait, if there is a dtjs, then 

Ugh this is so frustrating that I can't visualize it. 

So the thing is this. It may be possible for there to be a dtjs. Now, the other thing
is that when we create a state 1 node, ahhh now I get it.

Whatever the highest time jump is needs to get the node. So

Wait, can we have the situation

    p(0-x)   p2(x-y)
       \      /
        q(0-y)

Yes.

So then we could have

    p(0-x)   p2(x-y)
       \      /
        q(0-y)
        /   
       r(x-y) --> r(0-x)

So then, if we get a point that reaches p(0-x)
then only q(0-x) is covered. So darn it. Lets just try and see what happens.

So the thing is, when we create new nodes, we are subtracting from state0 somewhere,
somehow. So we need to associate the new nodes to the state0 node we are subtracting from.

Ok, so the thing now is that when link rows together, we need to create time jump siblings
of the children. And it's no big deal, because nothing else will set them or care.. it's
just when we save that we have to set these values. So we can't save until we've gone 
through everything. 


CAVEAT: We are assuming the time jump sibling fk will always be there. This may
not be true, but I think so...

8/5/10

D/HACKVTS ( 6297): start
D/HACKVTS ( 6297): id=1~0 c1=-1~0 c2=-1073741825~0 r=(-2147483648) tjs=(-2147483648) wstj=0 wetj=717834,
                   id=1~1 c1=-1~1 c2=-1073741825~1 r=(-2147483648) tjs=(-2147483648) wstj=717834 wetj=7776000000
D/HACKVTS ( 6297): id=-1~0 c1=null c2=null r=(-2147483648) tjs=(-2147483648) wstj=0 wetj=717834
                   id=-1~1 c1=null c2=null r=(-2147483648) tjs=(-2147483648) wstj=717834 wetj=7776000000
D/HACKVTS ( 6297): id=-1073741825~0 c1=(-2147483648) c2=(-2147483648) r=(-2147483648) tjs=(-2147483648) wstj=0 wetj=717834
                   id=-1073741825~1 c1=(-2147483648) c2=(-2147483648) r=(-2147483648) tjs=(-2147483648) wstj=717834 wetj=7776000000
D/HACKVTS ( 6297): id=-1~0 c1=null c2=null r=(-2147483648) tjs=(-2147483648) wstj=0 wetj=717834
                   id=-1~1 c1=null c2=null r=(-2147483648) tjs=(-2147483648) wstj=717834 wetj=7776000000
D/HACKVTS ( 6297): id=-1073741825~0 c1=(-2147483648) c2=(-2147483648) r=(-2147483648) tjs=(-2147483648) wstj=0 wetj=717834,id=-1073741825~1 c1=(-2147483648) c2=(-2147483648) r=(-2147483648) tjs=(-2147483648) wstj=717834 wetj=7776000000

8/6

Point 5
Point 7, id 1

Point 4
E/GpsTrailer( 2756): Bad child data!!! GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25599487,minLatEnd=           -25595097,minLonStart=           -54572048,minLonEnd=           -54568793,slopeLatm=        0.0000858289,slopeLonm=       -0.0024121189,id=1,c1=4,c2=5,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000) child: GpsLocCacheRow(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:20:47.134,minLatStart=           -25595746,minLatEnd=           -25594563,minLonStart=           -54575893,minLonEnd=           -54573691,slopeLatm=       -0.0030139345,slopeLonm=        0.0000575411,id=5,c1=8,c2=9,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000)                                                                                                                                                                                                                                                     

Before addpoint:

GpsLocCacheRow(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:34.358,minLatStart=           -25595189,minLatEnd=           -25595189,minLonStart=           -54574735,minLonEnd=           -54574735,slopeLatm=       -0.0025539817,slopeLonm=       -0.0008227026,id=5,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000)

After:

GpsLocCacheRow(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:34.358,minLatStart=           -25595189,minLatEnd=           -25595189,minLonStart=           -54574735,minLonEnd=           -54574735,slopeLatm=       -0.0025539817,slopeLonm=       -0.0008227026,id=5,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000)

lstb before:

1271715334358
-0.0025490837
-8.178533E-4
-2147483648
-2147483648
2147483647
2147483647
0
1271714030120

lstb after child1:

1271715334358
-0.0025490837
-8.178533E-4
-25595189
-54574735
-25595197
-54574742
0
1271714030120

After child2:

1271715343691
-0.0025490837
-8.178533E-4
-25595175
-54574722
-25595197
-54574742
0
1271714030120

Child 5:

psLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=           -25599487,minLatEnd=           -25598252,minLonStart=           -54570449,minLonEnd=           -54568793,slopeLatm=        0.0026635625,slopeLonm=       -0.0037179322,id=4,c1=2,c2=3,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000)

updated child 5:

GpsLocCacheRow(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25595197,minLatEnd=           -25595175,minLonStart=           -54574742,minLonEnd=           -54574722,slopeLatm=       -0.0025490837,slopeLonm=       -0.0008178533,id=5,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000)

Why is child 5's start time increased?????
nonissue


			//update the current row for the changes to the child (which probably increased the size of it)
			//since the child was already updated to contain the point, we use null as the gps points
			LopsidedSpaceTimeBox ourLstb = doCursoryExamination(rowCopy, null, null);
			rowCopy.updateLstb(ourLstb);
			

this is point 21, look, id 1 is messed up:

E/CacheViewer( 3172): id,child1,child2,tjs,numsegments,startTimeJump,endTimeJump,startTimeMs,endTimeMs,minLatStart,minLatEnd,minLonStart,minLonEnd,slopeLatm,slopeLonm                                                                                      
E/CacheViewer( 3172): 1,38,39,-2147483648,1,0,354675,2010-04-20 05:33:26.470,2010-04-21 04:05:43.965,-25748410,-25360892,-54614778,-54505778,-0.002775,0.000191                                                                                             
E/CacheViewer( 3172): 38,28,29,-2147483648,1,0,354675,2010-04-20 05:33:26.470,2010-04-21 03:58:59.957,-25748410,-25465633,-54614778,-54520092,-0.001478,0.000369                                                                                            
E/CacheViewer( 3172): 39,42,43,-2147483648,1,0,354675,2010-04-21 03:58:59.957,2010-04-21 04:05:43.965,-25862759,-25860198,-54559623,-54550969,-0.107981,-0.018304                                                                                           
E/CacheViewer( 3172): 28,24,25,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 03:43:09.590,-25652754,-25557894,-54612347,-54520092,-0.000322,0.000338                                                                                        
E/CacheViewer( 3172): 29,36,37,-2147483648,1,0,7776000000,2010-04-21 03:43:09.590,2010-04-21 03:58:59.957,-25672065,-25665679,-54517969,-54500060,-0.205894,-0.054332                                                                                       
E/CacheViewer( 3172): 24,16,17,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 02:50:57.471,-25601584,-25593334,-54579167,-54568613,-0.000012,0.000036                                                                                        
E/CacheViewer( 3172): 25,26,27,-2147483648,1,0,7776000000,2010-04-21 02:50:57.471,2010-04-21 03:43:09.590,-25598837,-25582553,-54586422,-54570621,-0.025409,0.024747                                                                                        
E/CacheViewer( 3172): 16,4,5,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 06:21:40.008,-25599487,-25593334,-54574852,-54568613,-0.000736,-0.001455                                                                                         
E/CacheViewer( 3172): 17,20,21,-2147483648,1,0,7776000000,2010-04-20 06:21:40.008,2010-04-21 02:50:57.471,-25600705,-25599149,-54573653,-54572532,0.000012,0.000031                                                                                         
E/CacheViewer( 3172): 4,2,3,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:53:50.120,-25599487,-25598252,-54570449,-54568793,0.002664,-0.003718                                                                                           
E/CacheViewer( 3172): 5,14,15,-2147483648,1,0,7776000000,2010-04-20 05:53:50.120,2010-04-20 06:21:40.008,-25595754,-25594235,-54576632,-54573683,-0.003212,0.000515                                                                                         
E/CacheViewer( 3172): 2,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:41:52.286,-25598252,-25598252,-54570449,-54570449,0.000223,-0.000445                                                                       
E/CacheViewer( 3172): 3,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:41:52.286,2010-04-20 05:53:50.120,-25598139,-25598139,-54570674,-54570674,0.004110,-0.005657                                                                       
E/CacheViewer( 3172): 14,8,9,-2147483648,1,0,7776000000,2010-04-20 05:53:50.120,2010-04-20 06:20:49.126,-25595754,-25594556,-54575900,-54573683,-0.003014,0.000063                                                                                          
E/CacheViewer( 3172): 15,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 06:20:49.126,2010-04-20 06:21:40.008,-25600612,-25600612,-54573593,-54573593,0.000727,-0.000314                                                                      
E/CacheViewer( 3172): 8,6,7,-2147483648,1,0,7776000000,2010-04-20 05:53:50.120,2010-04-20 06:15:43.691,-25595197,-25595175,-54574742,-54574722,-0.002549,-0.000818                                                                                          
E/CacheViewer( 3172): 9,12,13,-2147483648,1,0,7776000000,2010-04-20 06:15:43.691,2010-04-20 06:20:49.126,-25598538,-25598516,-54575803,-54575791,-0.006862,0.007235                                                                                         
E/CacheViewer( 3172): 6,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:53:50.120,2010-04-20 06:15:34.358,-25595189,-25595189,-54574735,-54574735,-0.002554,-0.000823                                                                      
E/CacheViewer( 3172): 7,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 06:15:34.358,2010-04-20 06:15:43.691,-25598520,-25598520,-54575808,-54575808,-0.000536,0.001179                                                                       
E/CacheViewer( 3172): 12,10,11,-2147483648,1,0,7776000000,2010-04-20 06:15:43.691,2010-04-20 06:20:48.112,-25598530,-25598516,-54575800,-54575795,-0.006885,0.007243                                                                                        
E/CacheViewer( 3172): 13,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 06:20:48.112,2010-04-20 06:20:49.126,-25600612,-25600612,-54573593,-54573593,0.000000,0.000000                                                                       
E/CacheViewer( 3172): 10,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 06:15:43.691,2010-04-20 06:20:47.134,-25598525,-25598525,-54575797,-54575797,-0.006898,0.007247                                                                      
E/CacheViewer( 3172): 11,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 06:20:47.134,2010-04-20 06:20:48.112,-25600618,-25600618,-54573598,-54573598,0.006135,0.005112                                                                       
E/CacheViewer( 3172): 20,18,19,-2147483648,1,0,7776000000,2010-04-20 06:21:40.008,2010-04-20 06:24:37.263,-25600705,-25600575,-54573653,-54573609,0.008056,0.006103                                                                                         
E/CacheViewer( 3172): 21,22,23,-2147483648,1,0,7776000000,2010-04-20 06:24:37.263,2010-04-21 02:50:57.471,-25599224,-25599222,-54572554,-54572552,0.000005,0.000026                                                                                         
E/CacheViewer( 3172): 18,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 06:21:40.008,2010-04-20 06:21:54.011,-25600575,-25600575,-54573609,-54573609,-0.001143,0.003071                                                                      
E/CacheViewer( 3172): 19,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 06:21:54.011,2010-04-20 06:24:37.263,-25600591,-25600591,-54573566,-54573566,0.008380,0.006211                                                                       
E/CacheViewer( 3172): 22,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 06:24:37.263,2010-04-20 06:25:11.331,-25599223,-25599223,-54572552,-54572552,0.000000,0.000000
E/CacheViewer( 3172): 23,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 06:25:11.331,2010-04-21 02:50:57.471,-25599223,-25599223,-54572552,-54572552,0.000005,0.000026
E/CacheViewer( 3172): 26,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-21 02:50:57.471,2010-04-21 03:01:28.558,-25598837,-25598837,-54570621,-54570621,0.000391,-0.000288
E/CacheViewer( 3172): 27,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-21 03:01:28.558,2010-04-21 03:43:09.590,-25598590,-25598590,-54570803,-54570803,-0.028926,0.028160
E/CacheViewer( 3172): 36,32,33,-2147483648,1,0,7776000000,2010-04-21 03:43:09.590,2010-04-21 03:58:21.200,-25670934,-25666379,-54513106,-54500060,-0.207135,-0.059665
E/CacheViewer( 3172): 37,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-21 03:58:21.200,2010-04-21 03:58:59.957,-25858727,-25858727,-54555187,-54555187,-0.067807,0.008024
E/CacheViewer( 3172): 32,30,31,-2147483648,1,0,7776000000,2010-04-21 03:43:09.590,2010-04-21 03:50:26.431,-25670934,-25668837,-54500571,-54500374,-0.201510,-0.088358
E/CacheViewer( 3172): 33,34,35,-2147483648,1,0,7776000000,2010-04-21 03:50:26.431,2010-04-21 03:58:21.200,-25758831,-25757666,-54539094,-54537286,-0.211407,-0.036159
E/CacheViewer( 3172): 30,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-21 03:43:09.590,2010-04-21 03:43:32.690,-25670934,-25670934,-54500374,-54500374,-0.110779,-0.096840
E/CacheViewer( 3172): 31,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-21 03:43:32.690,2010-04-21 03:50:26.431,-25673493,-25673493,-54502611,-54502611,-0.203444,-0.088178
E/CacheViewer( 3172): 34,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-21 03:50:26.431,2010-04-21 03:51:03.223,-25757666,-25757666,-54539094,-54539094,-0.243069,0.012965
E/CacheViewer( 3172): 35,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-21 03:51:03.223,2010-04-21 03:58:21.200,-25766609,-25766609,-54538617,-54538617,-0.210326,-0.037833
E/CacheViewer( 3172): 42,-2147483648,-2147483648,-2147483648,1,0,354675,2010-04-21 03:58:59.957,2010-04-21 04:04:54.632,-25861355,-25861355,-54554876,-54554876,-0.104724,-0.007291
E/CacheViewer( 3172): 43,-2147483648,-2147483648,-2147483648,1,0,354675,2010-04-21 04:04:54.632,2010-04-21 04:05:43.965,-25898498,-25898498,-54557462,-54557462,-0.159852,-0.193663

Before that, id 1 is fine

On point 21 (actually 22) , id 38,39 has 0 state

Tree universe:

D/HACKVTU ( 3250): id,c1,c2,rep,tjs,wss,wse,id,child1,child2,tjs,numsegments,startTimeJump,endTimeJump,startTimeMs,endTimeMs,minLatStart,minLatEnd,minLonStart,minLonEnd,slopeLatm,slopeLonm
D/HACKVTU ( 3250): 1~0,38,39~0,(-2147483648),(-2147483648),0,354675,1,38,39,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 04:05:43.965,-25748410,-25360892,-54614778,-54505778,-0.002775,0.000191
D/HACKVTU ( 3250): 1~1,38~0,39,(-2147483648),(-2147483648),354675,1305042,1,38,39,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 04:05:43.965,-25748410,-25285354,-54614778,-54496725,-0.003707,0.000080
D/HACKVTU ( 3250): 1~2,38~1,39,(-2147483648),(-2147483648),1305042,7776000000,1,38,39,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 04:05:43.965,-25748410,-25285354,-54614778,-54496725,-0.003707,0.000080
D/HACKVTU ( 3250): 39~0,-39~0,-1073741863~0,(-2147483648),(-2147483648),0,354675,39,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-21 03:58:59.957,2010-04-21 04:05:43.965,-25862759,-25860198,-54559623,-54550969,-0.107981,-0.018304
D/HACKVTU ( 3250): 39~1,(-2147483648),(-2147483648),(-2147483648),(-2147483648),354675,7776000000,39,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-21 03:58:59.957,2010-04-21 04:04:54.632,-25861355,-25861355,-54554876,-54554876,-0.104724,-0.007291
D/HACKVTU ( 3250): -39~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),0,354675,-39,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-21 03:58:59.957,2010-04-21 04:04:54.632,-25861355,-25861355,-54554876,-54554876,-0.104724,-0.007291
D/HACKVTU ( 3250): -1073741863~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),0,354675,-1073741863,-2147483648,-2147483648,-2147483648,1,0,354675,2010-04-21 04:04:54.632,2010-04-21 04:05:43.965,-25898498,-25898498,-54557462,-54557462,-0.159852,-0.193663
D/HACKVTU ( 3250): 38~0,-38~0,-1073741862~0,(-2147483648),(-2147483648),354675,1305042,38,28,29,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 04:05:43.965,-25748410,-25348250,-54614778,-54503564,-0.002932,0.000164
D/HACKVTU ( 3250): 38~1,-38~1,-1073741862~1,(-2147483648),(-2147483648),1305042,7776000000,38,28,29,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 04:05:43.965,-25748410,-25348250,-54614778,-54503564,-0.002932,0.000164
D/HACKVTU ( 3250): 38~2,28,29,(-2147483648),(-2147483648),0,354675,38,28,29,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 03:58:59.957,-25748410,-25465633,-54614778,-54520092,-0.001478,0.000369
D/HACKVTU ( 3250): -38~0,28,29,(-2147483648),(-2147483648),354675,1305042,-38,28,29,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 03:58:59.957,-25748410,-25465633,-54614778,-54520092,-0.001478,0.000369
D/HACKVTU ( 3250): -38~1,28,29,(-2147483648),(-2147483648),1305042,7776000000,-38,28,29,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 03:58:59.957,-25748410,-25465633,-54614778,-54520092,-0.001478,0.000369
D/HACKVTU ( 3250): -1073741862~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),354675,1305042,-1073741862,-2147483648,-2147483648,-2147483648,1,354675,1305042,2010-04-21 04:04:54.632,2010-04-21 04:05:43.965,-25898498,-25898498,-54557462,-54557462,-0.159852,-0.193663
D/HACKVTU ( 3250): -1073741862~1,(-2147483648),(-2147483648),(-2147483648),(-2147483648),1305042,7776000000,-1073741862,-2147483648,-2147483648,-2147483648,1,1305042,7776000000,2010-04-21 04:04:54.632,2010-04-21 04:05:43.965,-25898498,-25898498,-54557462,-54557462,-0.159852,-0.193663


Ok bad child problem. Also we're aren't printing out the time siblings

ok time siblings is now fixed.

Here is what 38 was before:

38,28,29,-2147483648,1,0,     7776000000,2010-04-20 05:33:26.470,2010-04-21 03:58:59.957,-25748410,-25465633,-54614778,-54520092,-0.001478,0.000369
38,40,41,-2147483648,1,354675,7776000000,2010-04-20 05:33:26.470,2010-04-21 04:05:43.965,-25748410,-25348250,-54614778,-54503564,-0.002932,0.000164 

Yea, so it changed. So it doesn't update the other parent... why not?

Point is 22

PERF: DANGER Note that we updating 38 more than once, because we use the id's to find the array of 38's and then update them all each time.

Here is the 38 that the bad data is of:
GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 04:05:43.965,minLatStart=           -25748410,minLatEnd=           -25348250,minLonStart=           -54614778,minLonEnd=           -54503564,slopeLatm=       -0.0029320361,slopeLonm=        0.0001639645,id=38,c1=40,c2=41,tj=-2147483648,numsegments=1,startTimeJump=354675,endTimeJump=7776000000)
Here is the old 38:
38,28,29,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 03:58:59.957,-25748410,-25465633,-54614778,-54520092,-0.001478,0.000369

Oh gosh! They are different! Wait, so 38 was already saved. And there is no replacement. So, oh, I see what happened!
38 is a dead link. But we reordered 38 so the live link took over that id, and it's line sibling got a new id..
so to fix this... ok.. so the point is that when... hmm. ok No it's simple. When we compare for joining, we 
can keep it the same way, so the time siblings are correct. But when we save, we have to put the 0 state dead
guy first (if there is one).

DONE: We should fix that PERF DANGER, too.. That's nasty.


Look, the state 0 thing works fine, but there are two issues
n/a 1) we sometimes use l.get(0).id to identify the row... which won't work anymore
DONE 2) top row needs to be handled differently since the row with the highest time jump needs to be first
DONE 3) if there is no state 0, we need to do the first one


GpsLocationRow(timeMs=2010-04-21 04:09:28.707,latm=           -25951429,lonm=           -54578002,id=23)

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 04:05:43.965,minLatStart=           -25748410,minLatEnd=           -25360892,minLonStart=           -54614778,minLonEnd=           -54505778,slopeLatm=       -0.0027754519,slopeLonm=        0.0001913859,id=47,c1=38,c2=45,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=354675)

Here is what it becomes:

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 04:09:28.707,minLatStart=           -25748410,minLatEnd=           -25349500,minLonStart=           -54614778,minLonEnd=           -54503104,slopeLatm=       -0.0029165540,slopeLonm=        0.0001582688,id=47,c1=38,c2=45,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=354675)

D/HACKVTU ( 5135): id,c1,c2,rep,tjs,wss,wse,id,child1,child2,tjs,numsegments,startTimeJump,endTimeJump,startTimeMs,endTimeMs,minLatStart,minLatEnd,minLonStart,minLonEnd,slopeLatm,slopeLonm
D/HACKVTU ( 5135): 1~0,46,39~0,(-2147483648),(47),354675,7776000000,1,46,39,47,1,354675,7776000000,2010-04-20 05:33:26.470,2010-04-21 04:09:28.707,-25748410,-25271647,-54614778,-54493099,-0.003876,0.000035
D/HACKVTU ( 5135): 39~0,-39~0,-1073741863~0,(-2147483648),(-2147483648),354675,7776000000,39,-2147483648,-2147483648,-2147483648,1,354675,7776000000,2010-04-21 03:58:59.957,2010-04-21 04:09:28.707,-25863960,-25849155,-54555611,-54544829,-0.139118,-0.035614
D/HACKVTU ( 5135): -39~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),354675,7776000000,-39,-2147483648,-2147483648,-2147483648,1,354675,7776000000,2010-04-21 03:58:59.957,2010-04-21 04:04:54.632,-25861355,-25861355,-54554876,-54554876,-0.104724,-0.007291
D/HACKVTU ( 5135): -1073741863~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),354675,7776000000,-1073741863,-2147483648,-2147483648,-2147483648,1,354675,7776000000,2010-04-21 04:05:43.965,2010-04-21 04:09:28.707,-25906384,-25906384,-54567016,-54567016,-0.200430,-0.048883
D/HACKVTU ( 5135): 47~0,38,45~0,(-2147483648),(-2147483648),0,49333,47,38,45,-2147483648,1,0,354675,2010-04-20 05:33:26.470,2010-04-21 04:09:28.707,-25748410,-25349500,-54614778,-54503104,-0.002917,0.000158
D/HACKVTU ( 5135): 47~1,38,45~1,(-2147483648),(-2147483648),49333,354675,47,38,45,-2147483648,1,0,354675,2010-04-20 05:33:26.470,2010-04-21 04:09:28.707,-25748410,-25349500,-54614778,-54503104,-0.002917,0.000158
D/HACKVTU ( 5135): 45~0,-45~0,-1073741869~0,(-2147483648),(-2147483648),0,49333,45,43,44,-2147483648,1,0,354675,2010-04-21 03:58:59.957,2010-04-21 04:09:28.707,-25865187,-25848406,-54559623,-54544658,-0.137166,-0.033921
D/HACKVTU ( 5135): 45~1,-45~1,-1073741869~1,(-2147483648),(-2147483648),49333,354675,45,43,44,-2147483648,1,0,354675,2010-04-21 03:58:59.957,2010-04-21 04:09:28.707,-25865187,-25848406,-54559623,-54544658,-0.137166,-0.033921
D/HACKVTU ( 5135): -45~0,43,44,(-2147483648),(-2147483648),0,49333,-45,43,44,-2147483648,1,0,354675,2010-04-21 03:58:59.957,2010-04-21 04:05:43.965,-25862759,-25860198,-54559623,-54550969,-0.107981,-0.018304
D/HACKVTU ( 5135): -45~1,43,44,(-2147483648),(-2147483648),49333,354675,-45,43,44,-2147483648,1,0,354675,2010-04-21 03:58:59.957,2010-04-21 04:05:43.965,-25862759,-25860198,-54559623,-54550969,-0.107981,-0.018304
D/HACKVTU ( 5135): -1073741869~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),0,49333,-1073741869,-2147483648,-2147483648,-2147483648,1,0,49333,2010-04-21 04:05:43.965,2010-04-21 04:09:28.707,-25906384,-25906384,-54567016,-54567016,-0.200430,-0.048883
D/HACKVTU ( 5135): -1073741869~1,(-2147483648),(-2147483648),(-2147483648),(-2147483648),49333,354675,-1073741869,-2147483648,-2147483648,-2147483648,1,49333,354675,2010-04-21 04:05:43.965,2010-04-21 04:09:28.707,-25906384,-25906384,-54567016,-54567016,-0.200430,-0.048883

----

E/CacheViewer( 5135): id,child1,child2,tjs,numsegments,startTimeJump,endTimeJump,startTimeMs,endTimeMs,minLatStart,minLatEnd,minLonStart,minLonEnd,slopeLatm,slopeLonm
E/CacheViewer( 5135): 1,46,39,47,1,354675,7776000000,2010-04-20 05:33:26.470,2010-04-21 04:09:28.707,-25748410,-25271647,-54614778,-54493099,-0.003876,0.000035
E/CacheViewer( 5135): 47,38,45,-2147483648,1,0,354675,2010-04-20 05:33:26.470,2010-04-21 04:05:43.965,-25748410,-25360892,-54614778,-54505778,-0.002775,0.000191
E/CacheViewer( 5135): 38,28,29,-2147483648,1,0,354675,2010-04-20 05:33:26.470,2010-04-21 03:58:59.957,-25748410,-25465633,-54614778,-54520092,-0.001478,0.000369
E/CacheViewer( 5135): 45,43,44,-2147483648,1,0,354675,2010-04-21 03:58:59.957,2010-04-21 04:05:43.965,-25862759,-25860198,-54559623,-54550969,-0.107981,-0.018304
E/CacheViewer( 5135): 28,24,25,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 03:43:09.590,-25652754,-25557894,-54612347,-54520092,-0.000322,0.000338
E/CacheViewer( 5135): 29,36,37,-2147483648,1,0,7776000000,2010-04-21 03:43:09.590,2010-04-21 03:58:59.957,-25672065,-25665679,-54517969,-54500060,-0.205894,-0.054332
E/CacheViewer( 5135): 24,16,17,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 02:50:57.471,-25601584,-25593334,-54579167,-54568613,-0.000012,0.000036
E/CacheViewer( 5135): 25,26,27,-2147483648,1,0,7776000000,2010-04-21 02:50:57.471,2010-04-21 03:43:09.590,-25598837,-25582553,-54586422,-54570621,-0.025409,0.024747

---



/GpsTrailer( 5281): Gps loc 25 added 24
E/CacheViewer( 5281): id,child1,child2,tjs,numsegments,startTimeJump,endTimeJump,startTimeMs,endTimeMs,minLatStart,minLatEnd,minLonStart,minLonEnd,slopeLatm,slopeLonm
E/CacheViewer( 5281): 1,46,39,47,1,354675,7776000000,2010-04-20 05:33:26.470,2010-04-21 04:15:59.414,-25748410,-25245261,-54614778,-54486572,-0.004198,-0.000045
E/CacheViewer( 5281): 47,38,45,-2147483648,1,0,354675,2010-04-20 05:33:26.470,2010-04-21 04:15:59.414,-25767843,-25325876,-54614778,-54498634,-0.003209,0.000103
E/CacheViewer( 5281): 38,28,29,-2147483648,1,0,354675,2010-04-20 05:33:26.470,2010-04-21 03:58:59.957,-25748410,-25465633,-54614778,-54520092,-0.001478,0.000369
E/CacheViewer( 5281): 45,48,49,-2147483648,1,0,354675,2010-04-21 03:58:59.957,2010-04-21 04:15:59.414,-25862759,-25837465,-54559623,-54545071,-0.164247,-0.032899
E/CacheViewer( 5281): 28,24,25,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 03:43:09.590,-25652754,-25557894,-54612347,-54520092,-0.000322,0.000338
E/CacheViewer( 5281): 29,36,37,-2147483648,1,0,7776000000,2010-04-21 03:43:09.590,2010-04-21 03:58:59.957,-25672065,-25665679,-54517969,-54500060,-0.205894,-0.054332
E/CacheViewer( 5281): 24,16,17,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-21 02:50:57.471,-25601584,-25593334,-54579167,-54568613,-0.000012,0.000036
E/CacheViewer( 5281): 25,26,27,-2147483648,1,0,7776000000,2010-04-21 02:50:57.471,2010-04-21 03:43:09.590,-25598837,-25582553,-54586422,-54570621,-0.025409,0.024747
E/CacheViewer( 5281): 16,4,5,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 06:21:40.008,-25599487,-25593334,-54574852,-54568613,-0.000736,-0.001455
E/CacheViewer( 5281): 17,20,21,-2147483648,1,0,7776000000,2010-04-20 06:21:40.008,2010-04-21 02:50:57.471,-25600705,-25599149,-54573653,-54572532,0.000012,0.000031
E/CacheViewer( 5281): 4,2,3,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:53:50.120,-25599487,-25598252,-54570449,-54568793,0.002664,-0.003718
E/CacheViewer( 5281): 5,14,15,-2147483648,1,0,7776000000,2010-04-20 05:53:50.120,2010-04-20 06:21:40.008,-25595754,-25594235,-54576632,-54573683,-0.003212,0.000515
E/CacheViewer( 5281): 2,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:41:52.286,-25598252,-25598252,-54570449,-54570449,0.000223,-0.000445
E/CacheViewer( 5281): 3,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:41:52.286,2010-04-20 05:53:50.120,-25598139,-25598139,-54570674,-54570674,0.004110,-0.005657
E/CacheViewer( 5281): 14,8,9,-2147483648,1,0,7776000000,2010-04-20 05:53:50.120,2010-04-20 06:20:49.126,-25595754,-25594556,-54575900,-54573683,-0.003014,0.000063
E/CacheViewer( 5281): 15,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 06:20:49.126,2010-04-20 06:21:40.008,-25600612,-25600612,-54573593,-54573593,0.000727,-0.000314

Point 38

Oh shoot, so the problem is that 47 state0 is 47. No, there are no state0's

but that brings up a good point. We save the data, then we create a time sibling, but to what?
And what if we already have one.

1 -> 47, then let's say there is a state 0. Then that becomes 47 for dead links. 
But a time link needs to point the the highest relative timelink.. Don't you see?

But tim engler, there are no dead timelinks. All timelinks are living.
1. Which means we need to update them whenever a change is made. Let's investigate this later.

---

I think it was 959

Heres the deal:

E/CacheViewer( 1052): 1190,1189,959,871,1,1015688,1468830,2010-04-20 05:33:26.470,2010-04-21 08:29:46.437,-25933161,-22642371,-54831224,-52122457,-0.032841,-0.024888
E/CacheViewer( 1052): 1286,958,959,1190,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25959566,-22639960,-54831224,-52091735,-0.032591,-0.025206

The chomp in the middle is
1015688 - 1260193
1468830 - 2666276

Which means that 1190 has the weirdness
1190 is a tjs of:
E/CacheViewer( 1052): 347,1235,1236,1190,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:29:46.437,-26016978,-22639960,-54831224,-52076543,-0.031998,-0.025363

1190 is actually a top level row.

E/CacheViewer( 1052): 1189,1048,1187,871,1,1015688,1468830,2010-04-20 05:33:26.470,2010-04-21 08:29:46.437,-25924132,-22642371,-54831224,-52176924,-0.032934,-0.024326

Nope, this is a perfectly reasonable action. In one case a new node is being created, in another, 959 is being used. So we'll have to deal with this.

So the thing is the time link has to link to the *highest* node. Oh but we handle time links already.

This whole thing seems unsafe to me. I think that we have to actually search for time links and children... ah that would be difficult considering everything
is encrypted. Darn. The point of a time link, no this is simple. All we need to do is set state 0 for the highest guy. The less than highest guy becomes a
state 1 like everything else. That should work.

{com.rareventure.gps2.GpsTrailerCrypt.Preferences.salt=com.rareventure.gps2.GpsTrailerCrypt$Preferences@4606d468, com.rareventure.gps2.GpsTrailerCrypt.Preferences.encryptedPrivateKey=com.rareventure.gps2.GpsTrailerCrypt$Preferences@4606d468, com.rareventure.gps2.GpsTrailerCrypt.Preferences.isNoPassword=com.rareventure.gps2.GpsTrailerCrypt$Preferences@4606d468, com.rareventure.gps2.GpsTrailerCrypt.Preferences.publicKey=com.rareventure.gps2.GpsTrailerCrypt$Preferences@4606d468, com.rareventure.gps2.reviewer.map.GpsTrailerReviewerApplication.Preferences.promptUserForPassword=com.rareventure.gps2.reviewer.map.GpsTrailerReviewerApplication$Preferences@4606caa8, com.rareventure.gps2.GpsTrailerCrypt.Preferences.initialWorkPerformed=com.rareventure00:25:86:57:22:DA.gps2.GpsTrailerCrypt$Preferences@4606d468, com.rareventure.gps2.database.cachecreator.GpsTrailerCacheCreator.Preferences.lastPointCachedId=com.rareventure.gps2.database.cachecreator.GpsTrailerCacheCreator$Preferences@460710d0}

E/CacheViewer( 2928): 1,962,963,387,1,2723383,7776000000,2010-04-20 05:33:26.470,2010-04-21 08:08:46.244,-25839448,-22842170,-54618964,-52420436,-0.031545,-0.022821
D/GpsTrailer( 2928): Processing gps loc row GpsLocationRow(timeMs=2010-04-21 08:20:20.852,latm=           -27402037,lonm=           -55914606,id=82)

New problem: 87

E/AndroidRuntime( 3154): junit.framework.AssertionFailedError: GpsLocCacheRow(startTimeMs=2010-04-21 07:58:11.626,endTimeMs=2010-04-21 08:08:46.244,minLatStart=           -27421097,minLatEnd=           -27419647,minLonStart=           -55899498,minLonEnd=           -55899494,slopeLatm=        0.0270435996,slopeLonm=       -0.0252545066,id=959,c1=1006,c2=1007,tj=-2147483648,numsegments=1,startTimeJump=1015688,endTimeJump=1260193) and timejump 1468830

Here we go:

E/CacheViewer( 3154): 1338,1189,959,871,1,1015688,1260193,2010-04-20 05:33:26.470,2010-04-21 08:29:46.437,-25933161,-22642371,-54831224,-52122457,-0.032841,-0.024888
E/CacheViewer( 3154): 1343,958,959,1190,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25959566,-22639960,-54831224,-52091735,-0.032591,-0.025206
E/CacheViewer( 3154): 959,1006,1007,-2147483648,1,1015688,1260193,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255


Alright so the 

E/AndroidRuntime( 7057): Caused by: java.lang.NullPointerException
E/AndroidRuntime( 7057):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPoint(AddPointStatus.java:477)
E/AndroidRuntime( 7057):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPoint(AddPointStatus.java:527)
E/AndroidRuntime( 7057):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPointToTop(AddPointStatus.java:89)
E/AndroidRuntime( 7057):        at com.rareventure.gps2.database.cachecreator.GpsTrailerCacheCreator.fillInCache(GpsTrailerCacheCreator.java:128)
E/AndroidRuntime( 7057):        at com.rareventure.gps2.GpsTrailerDb.dropAndRecreateCacheTable(GpsTrailerDb.java:248)
E/AndroidRuntime( 7057):        at com.rareventure.gps2.reviewer.map.GpsTrailerReviewerStart$1.run(GpsTrailerReviewerStart.java:42)
E/AndroidRuntime( 7057):        at com.rareventure.gps2.reviewer.map.GpsTrailerReviewerStart.onCreate(GpsTrailerReviewerStart.java:132)
E/AndroidRuntime( 7057):        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1047)

E/AndroidRuntime( 7243): junit.framework.AssertionFailedError: row GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25933161,minLatEnd=           -22642371,minLonStart=           -54831224,minLonEnd=           -52122457,slopeLatm=       -0.0328408517,slopeLonm=       -0.0248879436,id=1336,c1=1189,c2=959,tj=871,numsegments=1,startTimeJump=1015688,endTimeJump=1260193), child1 GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25924132,minLatEnd=           -22642371,minLonStart=           -54831224,minLonEnd=           -52176924,slopeLatm=       -0.0329339392,slopeLonm=       -0.0243263170,id=1189,c1=1048,c2=1187,tj=871,numsegments=1,startTimeJump=1015688,endTimeJump=1468830) child2 null
 
So the tjs startTimeJump=1015688,endTimeJump=1468830

E/CacheViewer( 7243): 959,1006,1007,-2147483648,1,1468830,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255

So 959 has no tjs. And has this funky parent here, oh. I see. They both are dead parents, and they represent different ranges. Oh no.



---

8/9

First we add to row 882:

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25894241,minLatEnd=           -22726692,minLonStart=           -54717049,minLonEnd=           -52273347,slopeLatm=       -0.0324252620,slopeLonm=       -0.0243667774,id=882,c1=1012,c2=1013,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=5752)

Next is:

E/CacheViewer( 7243): 959,1006,1007,-2147483648,1,1468830,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255

These are all top level.

Next is:

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25940075,minLatEnd=           -22702260,minLonStart=           -54717049,minLonEnd=           -52171963,slopeLatm=       -0.0321991928,slopeLonm=       -0.0254070908,id=122,c1=1100,c2=1101,tj=975,numsegments=1,startTimeJump=19040,endTimeJump=33166)

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25871568,minLatEnd=           -22817841,minLonStart=           -54714939,minLonEnd=           -52388659,slopeLatm=       -0.0318377502,slopeLonm=       -0.0232353825,id=577,c1=1018,c2=1019,tj=122,numsegments=1,startTimeJump=33166,endTimeJump=45101)

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25868240,minLatEnd=           -22830697,minLonStart=           -54714939,minLonEnd=           -52456479,slopeLatm=       -0.0317542218,slopeLonm=       -0.0225378126,id=504,c1=1020,c2=1021,tj=577,numsegments=1,startTimeJump=45101,endTimeJump=60784)

Now

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25933161,minLatEnd=           -22642371,minLonStart=           -54831224,minLonEnd=           -52122457,slopeLatm=       -0.0328408517,slopeLonm=       -0.0248879436,id=1190,c1=1189,c2=959,tj=871,numsegments=1,startTimeJump=1015688,endTimeJump=1468830)

It has an end time jump of 1468830. So it should not be touching 959, which goes from 1468830 to 2666276

But it has a child of 959. Why is that?


D/HACKVTU ( 7243): 347~0,958~0,959,(-2147483648),(871),1015688,1468830,347,958,959,871,1,1015688,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25930918,-22642371,-54831224,-52120802,-0.032864,-0.024907
D/HACKVTU ( 7243): 347~1,958~1,959,(-2147483648),(871),1468830,2075404,347,958,959,871,1,1015688,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25959566,-22639960,-54831224,-52091735,-0.032591,-0.025206
D/HACKVTU ( 7243): 347~2,958~2,959,(-2147483648),(871),2075404,2666276,347,958,959,871,1,1015688,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25959566,-22639960,-54831224,-52091735,-0.032591,-0.025206

E/CacheViewer( 7243): 959,1006,1007,-2147483648,1,1015688,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255

Ah, see here that it's ok right now at point 84.

So after point 84, 1190 gets created as follows:

E/CacheViewer( 7243): 1190,1189,959,871,1,1015688,1468830,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25930918,-22642371,-54831224,-52120802,-0.032864,-0.024907

E/CacheViewer( 7243): 959,1006,1007,-2147483648,1,1015688,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255

So it changes to this after point 87. So before that, it seems ok
E/CacheViewer( 7243): 959,1006,1007,-2147483648,1,1468830,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255

So 1190 has two children:

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25924132,minLatEnd=           -22642371,minLonStart=           -54831224,minLonEnd=           -52176924,slopeLatm=       -0.0329339392,slopeLonm=       -0.0243263170,id=1189,c1=1048,c2=1187,tj=871,numsegments=1,startTimeJump=1015688,endTimeJump=1468830)

and 

GpsLocCacheRow(startTimeMs=2010-04-21 07:58:11.626,endTimeMs=2010-04-21 08:08:46.244,minLatStart=           -27421097,minLatEnd=           -27419647,minLonStart=           -55899498,minLonEnd=           -55899494,slopeLatm=        0.0270435996,slopeLonm=       -0.0252545066,id=959,c1=1006,c2=1007,tj=-2147483648,numsegments=1,startTimeJump=1015688,endTimeJump=2666276)

Ok, so for this workload, the time jump is too high for 959 and adding it to 1189 would create a tree imbalance, so we create a new parent of 1190.

One of the weird things here is that 959 is the right child and 1189 is the left. Also, 959 has a bigger time jump than 1189. It's a little strange, but it might make sense

So anyway, in this case, we choose to add it to 959. Before, we couldn't because 959 ... hmm this seems imbalanced. Why for the first one is there no right child for 
the particular time jump? There should always be, right?

By looking at the tree, 1190 has 1189 and 959 as children, and they all have the same starting time jump.

Oh, because when adding the new point to 959, it would increase the time jump, because 959 has an earlier end time than 1189.
So, we don't want to add it to 1189, because that would make the box too lopsided.

Ok, so now when we add the point to 959, we are in effect increasing the time jump for that node. That's why we split off.
Now, it also comes into effect that there is another parent for 959 with a higher time jump.
So that other parent shouldn't be affected by this operation, as long as the time jump we are increasing 959 doesn't 
cross the start time jump of that parent. Let's make sure of this.

Here is the workload that adds to 959:
1260193 ms
1468830 ms

So for 959, we are creating a new parent. So for this row, we demote the current 959, but we are really creating a
duplicate of it. And changing the current 959 to contain that duplicate. So, for this issue, the only thing to remember
is that 959 is being updated for the current set.

Now remember, that 959 is also being pointed to by a different parent.

E/CacheViewer( 7243): 1286,958,959,1190,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25959566,-22639960,-54831224,-52091735,-0.032591,-0.025206

Note that 1286 tjs links to 1190. So this guy is the other parent. Wait, so why for 126-146 are we adding to 1190? Because it's range is from ~100-146 and we are trying
to add to the cache for anything equal to or below ~126

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -26016978,minLatEnd=           -22639960,minLonStart=           -54831224,minLonEnd=           -52076543,slopeLatm=       -0.0319982991,slopeLonm=       -0.0253633521,id=347,c1=1235,c2=1236,tj=1190,numsegments=1,startTimeJump=1468830,endTimeJump=2666276)

So wait, why does 347 enter in here?

So 1235 and 347 name 1190 as a time sibling... not good.

So at point 84, 1190 gets created and linked to 347

So at point 85, 1235 gets created and the bad tjs starts.

E/CacheViewer( 7243): 347,1235,1236,1190,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:39.292,-25992819,-22639960,-54831224,-52081679,-0.032247,-0.025310
E/CacheViewer( 7243): 1235,958,959,1190,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25959566,-22639960,-54831224,-52091735,-0.032591,-0.025206

So just as I suspected, were creating a new parent and the child for it gets the same tjs. So in our example we're at the top row.

When searching, what do we do? 
We have a tj in mind. Then we find the node with the highest tj that is under us. So we would go to 347 time sibling, and not 1235 if necessary.
If we choose 347, there would be no need to find the tj for 1235.

So in other words, we don't want to have any time jump created for 1235.


Row 1 has no time link when adding point 24 or so
GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 04:09:28.707,minLatStart=           -25748410,minLatEnd=           -25271647,minLonStart=           -54614778,minLonEnd=           -54493099,slopeLatm=       -0.0038761368,slopeLonm=        0.0000349921,id=1,c1=46,c2=39,tj=-2147483648,numsegments=1,startTimeJump=354675,endTimeJump=7776000000)

Ok, so now, point 87:

row GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25933161,minLatEnd=           -22642371,minLonStart=           -54831224,minLonEnd=           -52122457,slopeLatm=       -0.0328408517,slopeLonm=       -0.0248879436,id=1336,c1=1189,c2=959,tj=-2147483648,numsegments=1,startTimeJump=1015688,endTimeJump=1260193), child1 GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25924132,minLatEnd=           -22642371,minLonStart=           -54831224,minLonEnd=           -52176924,slopeLatm=       -0.0329339392,slopeLonm=       -0.0243263170,id=1189,c1=1048,c2=1187,tj=-2147483648,numsegments=1,startTimeJump=1015688,endTimeJump=1468830) child2 null

Ok, this looks like a cache problem, because it only happens if we start over from row 1:

E/AndroidRuntime( 1952): junit.framework.AssertionFailedError: row GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25933161,minLatEnd=           -22642371,minLonStart=           -54831224,minLonEnd=           -52122457,slopeLatm=       -0.0328408517,slopeLonm=       -0.0248879436,id=1336,c1=1189,c2=959,tj=-2147483648,numsegments=1,startTimeJump=1015688,endTimeJump=1260193), child1 GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 08:29:46.437,minLatStart=           -25924132,minLatEnd=           -22642371,minLonStart=           -54831224,minLonEnd=           -52176924,slopeLatm=       -0.0329339392,slopeLonm=       -0.0243263170,id=1189,c1=1048,c2=1187,tj=-2147483648,numsegments=1,startTimeJump=1015688,endTimeJump=1468830) child2 null
E/AndroidRuntime( 1952):        at junit.framework.Assert.fail(Assert.java:47)
E/AndroidRuntime( 1952):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPoint(AddPointStatus.java:482)
E/AndroidRuntime( 1952):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPoint(AddPointStatus.java:534)
E/AndroidRuntime( 1952):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPointToTop(AddPointStatus.java:89)

It always comes back to 959. Here is the row:

E/CacheViewer( 1952): 1336,1189,959,-2147483648,1,1015688,1260193,2010-04-20 05:33:26.470,2010-04-21 08:29:46.437,-25933161,-22642371,-54831224,-52122457,-0.032841,-0.024888

Strange that it has 959 available to it, yet child 2 is null, how does this happen?

Here we go:

E/CacheViewer( 1952): 1189,1048,1187,-2147483648,1,1015688,1468830,2010-04-20 05:33:26.470,2010-04-21 08:29:46.437,-25924132,-22642371,-54831224,-52176924,-0.032934,-0.024326
E/CacheViewer( 1952): 959,1006,1007,-2147483648,1,1468830,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255

But the question still remains, why only when started from 1?

worksegment of problem:
1260193
1015688

Darn, now it fails... I wonder what happened?

Anyway, back to the problem at hand.


E/CacheViewer( 1952): 1189,1048,1187,-2147483648,1,1015688,1468830,2010-04-20 05:33:26.470,2010-04-21 08:29:46.437,-25924132,-22642371,-54831224,-52176924,-0.032934,-0.024326
E/CacheViewer( 1952): 959,1006,1007,-2147483648,1,1468830,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255



E/CacheViewer( 1952): 347,958,959,871,1,1015688,2666276,2010-04-20 05:33:26.470,2010-04-21 08:22:40.456,-25929736,-22642371,-54831224,-52119917,-0.032877,-0.024917
E/CacheViewer( 1952): 959,1006,1007,-2147483648,1,1015688,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255

Oh so easy right now. Then after point 84:

E/CacheViewer( 1952): 347,958,959,1190,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25959566,-22639960,-54831224,-52091735,-0.032591,-0.025206
E/CacheViewer( 1952): 1190,1189,959,871,1,1015688,1468830,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25930918,-22642371,-54831224,-52120802,-0.032864,-0.024907

See, so both 347 and 1190 point to it

E/CacheViewer( 1952): 959,1006,1007,-2147483648,1,1015688,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255

But it hasn't changed at all, it was dead for point 84.

So for point 85. Here is what it's doing:

E/CacheViewer( 1952): 347,1235,1236,1190,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:39.292,-25992819,-22639960,-54831224,-52081679,-0.032247,-0.025310
E/CacheViewer( 1952): 1190,1189,959,871,1,1015688,1468830,2010-04-20 05:33:26.470,2010-04-21 08:25:39.292,-25931167,-22642371,-54831224,-52120986,-0.032862,-0.024905

E/CacheViewer( 1952): 1235,958,959,-2147483648,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25959566,-22639960,-54831224,-52091735,-0.032591,-0.025206

E/CacheViewer( 1952): 959,1006,1007,-2147483648,1,1015688,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255

Which seems not to be an issue at all. So lets check out point 86:

E/CacheViewer( 1952): 1190,1189,959,871,1,1015688,1468830,2010-04-20 05:33:26.470,2010-04-21 08:29:46.437,-25933161,-22642371,-54831224,-52122457,-0.032841,-0.024888
E/CacheViewer( 1952): 959,1006,1007,-2147483648,1,1015688,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255
E/CacheViewer( 1952): 1286,958,959,-2147483648,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:25:08.545,-25959566,-22639960,-54831224,-52091735,-0.032591,-0.025206
E/CacheViewer( 1952): 1235,1286,1287,-2147483648,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:29:46.437,-25995875,-22639960,-54831224,-52084012,-0.032216,-0.025286
E/CacheViewer( 1952): 347,1235,1236,1190,1,1468830,2666276,2010-04-20 05:33:26.470,2010-04-21 08:29:46.437,-26016978,-22639960,-54831224,-52076543,-0.031998,-0.025363

          --<-tjs-<-347
         /         /   \
        /       1235    1236
      |/_      /    \      ...
     1190   1286    1287
     /  \  /2   \1     ...
   1189 959     958

Still looks fine. So point 87

E/CacheViewer( 1952): 1336,1189,959,-2147483648,1,1015688,1260193,2010-04-20 05:33:26.470,2010-04-21 08:29:46.437,-25933161,-22642371,-54831224,-52122457,-0.032841,-0.024888
E/CacheViewer( 1952): 959,1006,1007,-2147483648,1,1468830,2666276,2010-04-21 07:58:11.626,2010-04-21 08:08:46.244,-27421097,-27419647,-55899498,-55899494,0.027044,-0.025255

Point 87 reduces 959 to one link and its from 1336 which doesn't share the same time jump period. Here is where the problem lies.

E/CacheViewer( 1952): 1338,1336,1337,871,1,1015688,1260193,2010-04-20 05:33:26.470,2010-04-21 08:35:34.113,-25983777,-22642371,-54831224,-52109543,-0.032319,-0.025021
E/CacheViewer( 1952): 1190,1189,1335,1338,1,1260193,1468830,2010-04-20 05:33:26.470,2010-04-21 08:35:34.113,-25955388,-22642371,-54831224,-52133117,-0.032612,-0.024778

Ok so, 1190 is pointing to the top level.. But what is going on with the point for point 87?

So there is a timejump in the middle of 1190's range, at ~126. So from 100-126 we do a recreate parent, pushing the existing thing down. So whatever will point to 959
changes from 100-126, and we create a new node to handle this point.

So that being done, we go back to 1190, and rerun for the time period from 126-146. Here we want to add it to 959, so we make a change to 959 for
the time range ~126-146

So for 347, we go to 1235, and go to 1286, which chooses to recreate the parent. So that is why 959 is being cut in the middle. Now as you can
see, 1286 is not even bothering with 959. So it's link to it is dead. 1190 is bothering with it but only for that certain range, so it also wants to link to it.
But we are doing something different for 959 so we either need to make a time sibling or completely new rows.
The big issue here is that since we don't have links to parents, we don't know that 1286 is linking to it. Since we don't have a continuous range
we're kind of stuck.

Ok, so I think the best idea is to try and find the dead parent.
We have a ws range. We also have a start and end time, and a start and end lopsided box. So we *must* be able to find the node that 
is pointing to it. We just throw the starting point values from the ws into the cache system and find the dead parent. Then we update
it appropriately by making a copy of the actual node.

But should we be going against the dead tree or the live one? The live one, I would say would be better.

Ahha, so it's

  1190 --tjs--> 1338
  /   \         /  \
1189  1335   1336  1337

8/10

So what we have here is dead parents and live ones.
Now the thing is the lives ones are contained within the dead ones.
We also have cases where a link points to a specific live parent. like the
live parent has already been found.

Ok, so I think that we can just search for the dead parent first, then find
the live one after that. However, the only issue here is that sometimes live
parents don't have a dead parent backing. You know? So if we go down searching
for dead parents, and ignore the live ones, we're going to miss this case.
Well to tell the truth, maybe not. I mean, darn, though. Because yes, the dead
parent will link to the old child. But it would be better, I think, just to follow
the live links. Follow them like they are real. So then that begs the question. What
is the answer?

Because look, we have a dead link, or a live one. We have a time jump. So what we really
have is a ghost, with two children and a parent. I think the liveOrDead idea is the best.
So what we do is we find either the dead or live link. Then we check if it contains the
point. If so, we check for the parent child relationship... remember that is the goal.

Point 306

E/AndroidRuntime( 3902): junit.framework.AssertionFailedError: curr row is [GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-22 22:24:03.873,minLatStart=           -35839319,minLatEnd=           -13174040,minLonStart=           -60888481,minLonEnd=           -47968187,slopeLatm=       -0.0496031456,slopeLonm=       -0.0194366015,id=1,c1=30379,c2=29699,tj=30637,numsegments=1,startTimeJump=75365737,endTimeJump=7776000000,ws.start=75365737,ws.end=7776000000)], timejump is 74702744
E/AndroidRuntime( 3902):        at junit.framework.Assert.fail(Assert.java:47)
E/AndroidRuntime( 3902):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.findParentRowData2(AddPointStatus.java:235)
E/AndroidRuntime( 3902):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.findParentRowData(AddPointStatus.java:180)
E/AndroidRuntime( 3902):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPointToTop(AddPointStatus.java:126)
E/AndroidRuntime( 3902):        at com.rareventure.gps2.database.cachecreator.GpsTrailerCacheCreator.fillInCache(GpsTrailerCacheCreator.java:128)

Here is the guy being chomped:

GpsLocCacheRow(startTimeMs=2010-04-22 22:09:51.090,endTimeMs=2010-04-22 22:21:12.947,minLatStart=           -34636105,minLatEnd=           -34636105,minLonStart=           -58393493,minLonEnd=           -58393493,slopeLatm=       -0.0030607590,slopeLonm=       -0.0161426812,id=30151,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=74702744,endTimeJump=7776000000)

Here is the range we are trying to find a parent for:
74702744
75059288

Starting top row:
GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-22 22:23:08.920,minLatStart=           -34143676,minLatEnd=           -14465214,minLonStart=           -60230290,minLonEnd=           -48469509,slopeLatm=       -0.0512905866,slopeLonm=       -0.0200913269,id=30635,c1=30632,c2=29699,tj=30380,numsegments=1,startTimeJump=74702744,endTimeJump=74754714)

then,
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-22 22:24:03.873,minLatStart=           -33110284,minLatEnd=           -14465214,minLonStart=           -59826396,minLonEnd=           -48469509,slopeLatm=       -0.0557162240,slopeLonm=       -0.0218212008,id=30632,c1=30372,c2=30625,tj=-2147483648,numsegments=1,startTimeJump=74702744,endTimeJump=74754714,ws.start=74702744,ws.end=74754714)

Then death? both children don't contain the point? But the parent does? 

GpsLocCacheRow(startTimeMs=2010-04-22 01:36:10.203,endTimeMs=2010-04-22 22:21:12.947,minLatStart=           -34618847,minLatEnd=           -34614994,minLonStart=           -58389637,minLonEnd=           -58378015,slopeLatm=       -0.0002593578,slopeLonm=       -0.0001989790,id=29699,c1=30150,c2=30151,tj=-2147483648,numsegments=1,startTimeJump=74702744,endTimeJump=7776000000)

Above is child 2.

but notice how the worksegment start and end get mangled in the live row.

[GpsLocCacheRowData(startTimeMs=2010-04-22 01:36:10.203,endTimeMs=2010-04-22 22:24:03.873,minLatStart=           -34620001,minLatEnd=           -34614684,minLonStart=           -58391424,minLonEnd=           -58377275,slopeLatm=       -0.0002635387,slopeLonm=       -0.0002089761,id=29699,c1=30150,c2=30151,tj=-2147483648,numsegments=1,startTimeJump=74702744,endTimeJump=7776000000,ws.start=75059288,ws.end=75365737)]

Ah, we're half way through adding the state0 points. So the state0 point for this guy has not been added yet.

...

So we need to add the 0state items, but that causes an issue if the parents don't have links yet.
Why? Because then the parents won't be pointing at the right spot. So, we could 
load up the nodes in an order that they are guaranteed not to parent each other. Or 2, just update
the node on the fly while we are working on another. So one issue with that is we are going
through the



GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-22 22:24:03.873,minLatStart=           -34144235,minLatEnd=           -14465214,minLonStart=           -60230472,minLonEnd=           -48469509,slopeLatm=       -0.0512869842,slopeLonm=       -0.0200902242,id=30635,c1=30632,c2=29699,tj=30380,numsegments=1,startTimeJump=74702744,endTimeJump=74754714,ws.start=74702744,ws.end=74754714)
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-22 22:24:03.873,minLatStart=           -34144235,minLatEnd=           -14465214,minLonStart=           -60230472,minLonEnd=           -48469509,slopeLatm=       -0.0512869842,slopeLonm=       -0.0200902242,id=30635,c1=30632,c2=29699,tj=30380,numsegments=1,startTimeJump=74702744,endTimeJump=74754714,ws.start=74702744,ws.end=74754714)

The child for time jump 74702744 that cannot be found.

GpsLocCacheRow(startTimeMs=2010-04-22 22:09:51.090,endTimeMs=2010-04-22 22:21:12.947,minLatStart=           -34636105,minLatEnd=           -34636105,minLonStart=           -58393493,minLonEnd=           -58393493,slopeLatm=       -0.0030607590,slopeLonm=       -0.0161426812,id=30151,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=74702744,endTimeJump=7776000000)

E/CacheViewer( 5072): 30151,-2147483648,-2147483648,-2147483648,1,74702744,7776000000,2010-04-22 22:09:51.090,2010-04-22 22:21:12.947,-34636105,-34636105,-58393493,-58393493,-0.003061,-0.016143
E/CacheViewer( 5072): 29699,30150,30151,-2147483648,1,74702744,7776000000,2010-04-22 01:36:10.203,2010-04-22 22:21:12.947,-34618847,-34614994,-58389637,-58378015,-0.000259,-0.000199

Above is the linkage. AS you can see there is only one parent.
So, it should be able to find 30151.

/CacheViewer( 5072): 1,30379,29699,30637,1,75365737,7776000000,2010-04-20 05:33:26.470,2010-04-22 22:23:08.920,-35837509,-13174040,-60887893,-47968187,-0.049610,-0.019439
E/CacheViewer( 5072): 30637,30634,29699,30636,1,75059288,75365737,2010-04-20 05:33:26.470,2010-04-22 22:23:08.920,-35539806,-13795944,-60772366,-48209522,-0.048191,-0.018888
E/CacheViewer( 5072): 30636,30633,29699,30635,1,74754714,75059288,2010-04-20 05:33:26.470,2010-04-22 22:23:08.920,-35218125,-14465214,-60647395,-48469509,-0.046685,-0.018304
E/CacheViewer( 5072): 30635,30632,29699,30380,1,74702744,74754714,2010-04-20 05:33:26.470,2010-04-22 22:23:08.920,-34143676,-14465214,-60230290,-48469509,-0.051291,-0.020091
t

74702744 is the time we want to get.

D/GpsTrailer( 5294): Processing gps loc row GpsLocationRow(timeMs=2010-04-22 22:24:03.873,latm=           -34639732,lonm=           -58405874,id=306)
D/dalvikvm( 5294): GC_FOR_MALLOC freed 45090 objects / 2041064 bytes in 140ms
D/AndroidRuntime( 5294): Shutting down VM
W/dalvikvm( 5294): threadid=1: thread exiting with uncaught exception (group=0x4001d7e0)
E/AndroidRuntime( 5294): FATAL EXCEPTION: main
E/AndroidRuntime( 5294): junit.framework.AssertionFailedError: curr row is [GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-22 22:24:03.873,minLatStart=           -35839319,minLatEnd=           -13174040,minLonStart=           -60888481,minLonEnd=           -47968187,slopeLatm=       -0.0496031456,slopeLonm=       -0.0194366015,id=1,c1=30379,c2=29699,tj=30637,numsegments=1,startTimeJump=75365737,endTimeJump=7776000000,ws.start=75365737,ws.end=7776000000)], timejump is 74702744
E/AndroidRuntime( 5294):        at junit.framework.Assert.fail(Assert.java:47)
E/AndroidRuntime( 5294):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.findParentRowData2(AddPointStatus.java:249)
E/AndroidRuntime( 5294):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.findParentRowData(AddPointStatus.java:188)
E/AndroidRuntime( 5294):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.updateRowDataForRow0(AddPointStatus.java:147)
E/AndroidRuntime( 5294):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPointToTop(AddPointStatus.java:99)



E/AndroidRuntime( 5486): junit.framework.AssertionFailedError: r1 is GpsLocCacheRowData(startTimeMs=2010-04-22 01:36:10.203,endTimeMs=2010-04-22 22:21:12.947,minLatStart=           -34618847,minLatEnd=           -34614994,minLonStart=           -58389637,minLonEnd=           -58378015,slopeLatm=       -0.0002593578,slopeLonm=       -0.0001989790,id=-2147483648,c1=30150,c2=30151,tj=-2147483648,numsegments=1,startTimeJump=74702744,endTimeJump=7776000000,ws.start=74702744,ws.end=75059288), r2 is GpsLocCacheRowData(startTimeMs=2010-04-22 01:36:10.203,endTimeMs=2010-04-22 22:21:12.947,minLatStart=           -34618847,minLatEnd=           -34614994,minLonStart=           -58389637,minLonEnd=           -58378015,slopeLatm=       -0.0002593578,slopeLonm=       -0.0001989790,id=-2147483648,c1=30150,c2=30151,tj=-2147483648,numsegments=1,startTimeJump=74702744,endTimeJump=7776000000,ws.start=74754714,ws.end=74818717)                                                                       
E/AndroidRuntime( 5486):        at junit.framework.Assert.fail(Assert.java:47)                                
E/AndroidRuntime( 5486):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.updateRowAndChildren(AddPointStatus.java:488)                                                                                   
E/AndroidRuntime( 5486):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.updateRowAndChildren(AddPointStatus.java:460)                                                                                   
E/AndroidRuntime( 5486):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.updateRowAndChildren(AddPointStatus.java:433)                                                                                   
E/AndroidRuntime( 5486):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.updateRowAndChildren(AddPointStatus.java:433)
E/AndroidRuntime( 5486):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.updateRowAndChildren(AddPointStatus.java:433)
E/AndroidRuntime( 5486):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPointToTop(AddPointStatus.java:112)
E/AndroidRuntime( 5486):        at com.rareventure.gps2.database.cachecreator.GpsTrailerCacheCreator.fillInCache(GpsTrailerCacheCreator.java:132)


So the issue is with 29699.


GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-22 22:24:03.873,minLatStart=           -34144235,minLatEnd=           -14465214,minLonStart=           -60230472,minLonEnd=           -48469509,slopeLatm=       -0.0512869842,slopeLonm=       -0.0200902242,id=30635,c1=30632,c2=29699,tj=30380,numsegments=1,startTimeJump=74702744,endTimeJump=74754714,ws.start=74702744,ws.end=74754714)

Row 306:

E/AndroidRuntime( 5876): junit.framework.AssertionFailedError: curr row is [GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-22 22:24:03.873,minLatStart=           -35839319,minLatEnd=           -13174040,minLonStart=           -60888481,minLonEnd=           -47968187,slopeLatm=       -0.0496031456,slopeLonm=       -0.0194366015,id=1,c1=30379,c2=29699,tj=30637,numsegments=1,startTimeJump=75365737,endTimeJump=7776000000,ws.start=75365737,ws.end=7776000000)], timejump is 74702744
E/AndroidRuntime( 5876):        at junit.framework.Assert.fail(Assert.java:47)
E/AndroidRuntime( 5876):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.findParentRowData2(AddPointStatus.java:249)
E/AndroidRuntime( 5876):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.findParentRowData(AddPointStatus.java:188)
E/AndroidRuntime( 5876):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.updateRowDataForRow0(AddPointStatus.java:147)
E/AndroidRuntime( 5876):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPointToTop(AddPointStatus.java:99)
E/AndroidRuntime( 5876):        at com.rareventure.gps2.database.cachecreator.GpsTrailerCacheCreator.fillInCache(GpsTrailerCacheCreator.java:132)
E/AndroidRuntime( 5876):        at com.rareventure.gps2.GpsTrailerDb.dropAndRecreateCacheTable(GpsTrailerDb.java:248)
E/AndroidRuntime( 5876):        at com.rareventure.gps2.reviewer.map.GpsTrailerReviewerStart$1.run(GpsTrailerReviewerStart.java:42)
E/AndroidRuntime( 5876):        at com.rareventure.gps2.reviewer.map.GpsTrailerReviewerStart.onCreate(GpsTrailerReviewerStart.java:132)
E/

Ok here is what we want:

55 = 100

55 = 10 ^ x
x = 1.5
ceil(x)
10 ^ x = 100

x = Math.ceil(log(10)/log(55));

w * x ^ y = z

log(w) + log(x)*y = log(z)

log(z)/log(w) + log(x) = y



1000000000
7776000000
999999995904

717834
1000000

Crap, so the problem is that where we start, where we end, or in the middle, we are out of bounds.
We are earth size. Or we extend past the end of the earth.

For latm, I think we can ignore this

So we need to find the center of all the points in the lon direction before we can do this properly.
Because if we wrap past the edge of the international date line, we're screwed.


--

Ok, so we have the least square data and the children themselves.
Now,

max child end            = -54572327
end child position, id 2 = -54570674

child end time 1271713312286

parent end time 2010-04-20 05:53:50.120
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=           -25599487,minLatEnd=           -25598250,minLonStart=           -54570449,minLonEnd=           -54570447,slopeLatm=        0.0026635625,slopeLonm=       -0.0037179331,id=1,c1=2,c2=3,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000)

child
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=           -25598252,minLatEnd=           -25598252,minLonStart=           -54570449,minLonEnd=           -54570449,slopeLatm=        0.0002234014,slopeLonm=       -0.0004448258,id=2,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000)

ok when we add it to the lsd, its the same value  (int) -54570674
start lonm (int) -54574735
 (int) -54568795


GpsLocCacheRowData(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25595197,minLatEnd=           -25595175,minLonStart=           -54574742,minLonEnd=           -54574733,slopeLatm=       -0.0025490837,slopeLonm=       -0.0008178534,id=5,c1=6,c2=7,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000)
GpsLocCacheRowData(startTimeMs=2010-04-20 06:15:34.358,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25598520,minLatEnd=           -25598520,minLonStart=           -54575808,minLonEnd=           -54575808,slopeLatm=       -0.0005357334,slopeLonm=        0.0011786135,id=7,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000)

 (int) -54575797 max lonm of child end
 (int) -54575807 max lonm of parent at child end

child2

-54574724 is the parent lonm start of the child lonm end min
-54574722 is the parent lonm start of the child lonm end max

-54575797 is the child end max lonm
child endtime is 1271715343691
p start time is  1271714030120

GpsLocCacheRowData(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:34.358,minLatStart=           -25595189,minLatEnd=           -25595189,minLonStart=           -54574735,minLonEnd=           -54574735,slopeLatm=       -0.0025539817,slopeLonm=       -0.0008227026,id=5,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=1478277,lonmwidth=-5,latmheight=-1073741829)

GpsLocCacheRowData(startTimeMs=2010-04-20 06:15:34.358,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25598520,minLatEnd=           -25598520,minLonStart=           -54575808,minLonEnd=           -54575808,slopeLatm=       -0.0005357334,slopeLonm=        0.0011786135,id=-1073741829,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=1478277,endTimeJump=7776000000,ws.start=1478277,ws.end=7776000000,lonmwidth=0,latmheight=0)
GpsLocCacheRowData(startTimeMs=2010-04-20 06:15:34.358,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25598520,minLatEnd=           -25598520,minLonStart=           -54575808,minLonEnd=           -54575808,slopeLatm=       -0.0005357334,slopeLonm=        0.0011786135,id=7,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,lonmwidth=0,latmheight=0)

Everything is the same as when we check it


 (int) -54575807 parent max lonm at child end
 (int) -54575797 child max lonm at child end
1313571 matches our calculations for length of time
 (double) -1074.3084760961356 is the distance
We calculated:

-54575797 + 1074.3085084914
Got: -54575797 + 1074.3085084914

-54574722.6914915

They calculate:

 (int) -54574733 - 1074.3085084914

Wait, why is end a 33? not a 22?


Wait, why is 5 different?

The new 5:
GpsLocCacheRowData(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25595197,minLatEnd=           -25595175,minLonStart=           -54574742,minLonEnd=           -54574733,slopeLatm=       -0.0025490837,slopeLonm=       -0.0008178534,id=5,c1=6,c2=7,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,lonmwidth=6,latmheight=7)

The old one:
GpsLocCacheRowData(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:34.358,minLatStart=           -25595189,minLatEnd=           -25595189,minLonStart=           -54574735,minLonEnd=           -54574735,slopeLatm=       -0.0025539817,slopeLonm=       -0.0008227026,id=5,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=1478277,lonmwidth=-5,latmheight=-1073741829)
After update:
GpsLocCacheRowData(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25595197,minLatEnd=           -25595175,minLonStart=           -54574742,minLonEnd=           -54574733,slopeLatm=       -0.0025490837,slopeLonm=       -0.0008178534,id=5,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=1478277,lonmwidth=-5,latmheight=-1073741829)

Ok, so luckily it matches.

hEre is the time from the causing calucation:  (long) -1304238 but ours is 1313571

parent start time is 1271714030120 child start time is  (long) 1271715334358 which is different, is 2010-04-20 06:15:34.358
Ok, that seems right.

1271715334358

We need to think about this.

Ok, so we have a starting position in time, that is absolute and is:

1271714030120
-54574742,-54574733

Then, we calculate the child end position min at the start of the parent, which is:
(-(cet - pst) * slope) + ce = -((1271715343691 - 1271714030120) * -.0008178534) + (-54575798) = -54574723.6914915
Now as you can see, we are already outside the bounds of the parent, 5

lstb calcs this as -54574723.6914915




GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:45:06.889,minLatStart=          -177667692,minLatEnd=            41793224,minLonStart=          -165031802,minLonEnd=            -9867103,slopeLatm=        0.0490725264,slopeLonm=        0.0447158180,id=10871,c1=32895,c2=32896,tj=13857,numsegments=1,startTimeJump=25257752,endTimeJump=37886628,ws.start=25257752,ws.end=37886628,lonmwidth=32895,latmheight=0)

GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:45:06.889,minLatStart=          -155467688,minLatEnd=            41793222,minLonStart=          -165031802,minLonEnd=            10158352,slopeLatm=        0.0366195068,slopeLonm=        0.0334826037,id=32895,c1=32954,c2=32955,tj=-2147483648,numsegments=1,startTimeJump=25257752,endTimeJump=37886628,ws.start=25257752,ws.end=37886628,lonmwidth=32954,latmheight=32955)

endstartlonm of child is outside of parent box
Note the start and end times of both boxes are the same (and both are huge
Latm height is zero? No, not really. Those values are way incorrect!
So also the max lonm is too small.

Alright so the width of the parent is: 155164699
The width of the child is:             175190154

is lsd children being calcualted correctly?
Why are there two points in the lds?? The -674* points? Nevermind


GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:33:03.580,minLatStart=          -119482953,minLatEnd=             5755880,minLonStart=          -126516918,minLonEnd=           -46450055,slopeLatm=        0.0377663337,slopeLonm=        0.0329601020,id=25788,c1=32780,c2=32781,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=1000,ws.start=0,ws.end=1000,c1r=32780,c2r=32781,r=0,lonmwidth=80066863,latmheight=125238833)
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:27:59.803,minLatStart=           -88173442,minLatEnd=             5755878,minLonStart=          -105170126,minLonEnd=           -25103266,slopeLatm=        0.0201932415,slopeLonm=        0.0209787861,id=32780,c1=32724,c2=32725,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=1000,ws.start=0,ws.end=1000,c1r=32724,c2r=32725,r=0,lonmwidth=80066860,latmheight=93929320)

Child max start lonm is too big

GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:27:59.803,minLatStart=           -88173442,minLatEnd=             5755878,minLonStart=          -105170126,minLonEnd=           -25103266,slopeLatm=        0.0201932415,slopeLonm=        0.0209787861,id=-25788,c1=32724,c2=32725,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=1000,ws.start=0,ws.end=1000,c1r=32724,c2r=32725,r=0,lonmwidth=80066860,latmheight=93929320)
GpsLocCacheRowData(startTimeMs=2010-05-10 20:27:59.803,endTimeMs=2010-05-10 20:33:03.580,minLatStart=            41733643,minLatEnd=            41733643,minLonStart=            12274078,minLonEnd=            12274078,slopeLatm=        0.0000000000,slopeLonm=        0.0134045696,id=-1073767612,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=1000,ws.start=0,ws.end=1000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0)



GpsLocCacheRowData(startTimeMs=2010-05-07 23:16:13.588,endTimeMs=2010-05-10 20:33:03.580,minLatStart=           -36715711,minLatEnd=           -34633257,minLonStart=          -105263406,minLonEnd=                   0,slopeLatm=        0.3116145134,slopeLonm=        7.1876158714,id=32509,c1=32732,c2=32733,tj=-2147483648,numsegments=1,startTimeJump=11393,endTimeJump=17090,ws.start=11393,ws.end=17090,c1r=0,c2r=32733,r=0,lonmwidth=105263406,latmheight=2082454)
GpsLocCacheRowData(startTimeMs=2010-05-08 01:04:55.934,endTimeMs=2010-05-10 20:33:03.580,minLatStart=           -34683253,minLatEnd=           -34587677,minLonStart=           -58382898,minLonEnd=                   0,slopeLatm=        0.3146182895,slopeLonm=        0.1456425041,id=32733,c1=32788,c2=32789,tj=-2147483648,numsegments=1,startTimeJump=11393,endTimeJump=17090,ws.start=11393,ws.end=17090,c1r=32788,c2r=32789,r=0,lonmwidth=58382898,latmheight=95576)

Why is the work segment such a weird value??? 11393

The lonm slope is really big.. And there is only 10 second separation what is going on? It is over 3 days, I suppose. 3 days?
And 7 per millisecond? Something is really weird here!

-58385694
-58385087

lonm 1 start

-58383288
-58382681

lonm 1 end

GpsLocCacheRowData(startTimeMs=2010-05-08 01:04:55.934,endTimeMs=2010-05-10 20:33:03.580,minLatStart=           -34683253,minLatEnd=           -34587677,minLonStart=           -58382898,minLonEnd=                   0,slopeLatm=        0.3146182895,slopeLonm=        0.1456425041,id=32733,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=11393,endTimeJump=17090,ws.start=11393,ws.end=17090,c1r=-32733,c2r=-1073774557,r=0,lonmwidth=58382898,latmheight=95576)

Why does this go from -583* to 0 in lon?

I can see why we have an explosion of lonm slope. I guess we'll suspend this and go after 32733?? no later later






GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:33:03.580,minLatStart=           -86947743,minLatEnd=            13636155,minLonStart=           174529368,minLonEnd=            62912924,slopeLatm=        0.0205465332,slopeLonm=        0.0126984753,id=2901,c1=32392,c2=32393,tj=26009,numsegments=1,startTimeJump=1500,endTimeJump=5063,ws.start=1500,ws.end=5063,c1r=0,c2r=32393,r=0,lonmwidth=-111616444,latmheight=100583898)

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-07 03:06:51.437,minLatStart=           -54357075,minLatEnd=           -10070655,minLonStart=           -74643114,minLonEnd=           -45783207,slopeLatm=       -0.0017757652,slopeLonm=        0.0013405230,id=32392,c1=31384,c2=31385,tj=-2147483648,numsegments=1,startTimeJump=1500,endTimeJump=5063)

------

GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:27:59.803,minLatStart=           -88173442,minLatEnd=             5755878,minLonStart=          -105170126,minLonEnd=           -25103266,slopeLatm=        0.0201932415,slopeLonm=        0.0209787861,id=-25788,c1=32724,c2=32725,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=1000,ws.start=0,ws.end=1000,c1r=32724,c2r=32725,r=0,lonmwidth=80066860,latmheight=93929320)

GpsLocCacheRowData(startTimeMs=2010-05-08 01:04:55.934,endTimeMs=2010-05-10 20:27:59.803,minLatStart=           -34619647,minLatEnd=           -34619647,minLonStart=           -58382898,minLonEnd=           -58382898,slopeLatm=        0.3147500455,slopeLonm=        0.2912682295,id=32725,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=1000,ws.start=0,ws.end=1000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0)


LopsidedSpaceTimeBox.calcDist(child.getLong(START_TIME), child.getInt(MIN_LONM) + child.getInt(WIDTH_LONM), child.getLong(END_TIME), child.getFloat(LONM_PER_MS), 
				false)

E/GpsTrailer(10329): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:20:48.112,minLatStart=           -25599487,minLatEnd=           -25592325,minLonStart=           125431487,minLonEnd=            72214094,slopeLatm=       -0.0011291872,slopeLonm=        6.2435164452,id=13,c1=18,c2=19,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=1478277,ws.start=0,ws.end=1478277,c1r=18,c2r=19,r=0,lonmwidth=-53217393,latmheight=7162) child: GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:20:47.134,minLatStart=           -25599487,minLatEnd=           -25593827,minLonStart=           125431487,minLonEnd=           -31399486,slopeLatm=       -0.0006006531,slopeLonm=       42.7186431885,id=18,c1=4,c2=11,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=1478277,ws.start=0,ws.end=1478277,c1r=4,c2r=11,r=0,lonmwidth=-156830973,latmheight=5660)
E/GpsTrailer(10329): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:20:48.112,minLatStart=           -25599487,minLatEnd=           -25592325,minLonStart=           125431487,minLonEnd=            72214094,slopeLatm=       -0.0011291872,slopeLonm=        6.2435164452,id=13,c1=18,c2=19,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=1478277,ws.start=0,ws.end=1478277,c1r=18,c2r=19,r=0,lonmwidth=-53217393,latmheight=7162) child: GpsLocCacheRowData(startTimeMs=2010-04-20 06:20:47.134,endTimeMs=2010-04-20 06:20:48.112,minLatStart=           -25600618,minLatEnd=           -25600618,minLonStart=           -54573598,minLonEnd=           -54573598,slopeLatm=        0.0061349692,slopeLonm=        0.0051124743,id=19,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=1478277,ws.start=0,ws.end=1478277,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0)


13 is top level item.

E/CacheViewer(10329): 13,30,31,-2147483648,1,0,1478277,2010-04-20 05:33:26.470,2010-04-20 06:20:49.126,-25599487,-25591326,125431487,72223243,-0.001480,6.240297
E/CacheViewer(10329): 30,18,19,-2147483648,1,0,1478277,2010-04-20 05:33:26.470,2010-04-20 06:20:48.112,-25599487,-25592325,125431487,72214094,-0.001129,6.243516
E/CacheViewer(10329): 19,-2147483648,-2147483648,-2147483648,1,0,1478277,2010-04-20 06:20:47.134,2010-04-20 06:20:48.112,-25600618,-25600618,-54573598,-54573598,0.006135,0.005112

Ah, 13 doesn't even contain the point. That is the problem.

GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=           -25599487,minLatEnd=           -25598250,minLonStart=           125431487,minLonEnd=           -54570447,slopeLatm=        0.0026635625,slopeLonm=      355.8563537598,id=1,c1=2,c2=3,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=2,c2r=3,r=0,lonmwidth=-180001934,latmheight=1237)
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=           -25598252,minLatEnd=           -25598252,minLonStart=           -54570449,minLonEnd=           -54570449,slopeLatm=        0.0002234014,slopeLonm=       -0.0004448258,id=2,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0)

 (int) 305429551


((-54574735 -  -54570675) % 360000000 + 360000000) %  360000000 + startLonm



GpsLocCacheRowData(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25595197,minLatEnd=           -25595175,minLonStart=           -54574741,minLonEnd=           -54574722,slopeLatm=       -0.0025490841,slopeLonm=       -0.0008181096,id=5,c1=6,c2=7,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=6,c2r=7,r=0,lonmwidth=19,latmheight=22)
GpsLocCacheRowData(startTimeMs=2010-04-20 06:15:34.358,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25598520,minLatEnd=           -25598520,minLonStart=           -54575808,minLonEnd=           -54575808,slopeLatm=       -0.0005357334,slopeLonm=        0.0011786135,id=7,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0)

GpsLocCacheRowData(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25595197,minLatEnd=           -25595175,minLonStart=           -54574741,minLonEnd=           -54574722,slopeLatm=       -0.0025490841,slopeLonm=       -0.0008181096,id=5,c1=6,c2=7,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=6,c2r=7,r=0,lonmwidth=19,latmheight=22)
GpsLocCacheRowData(startTimeMs=2010-04-20 06:15:34.358,endTimeMs=2010-04-20 06:15:43.691,minLatStart=           -25598520,minLatEnd=           -25598520,minLonStart=           -54575808,minLonEnd=           -54575808,slopeLatm=       -0.0005357334,slopeLonm=        0.0011786135,id=7,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0)

 (int) -54575797
 (int) -54575797





GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:58:11.841,minLatStart=          -200421393,minLatEnd=            42295822,
minLonStart=           178409167,minLonEnd=            13952578,slopeLatm=        0.0622411221,slopeLonm=        0.0457783267,id=7759,c1=29134,c2=29135,tj=9612,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=29134,c2r=0,r=0,lonmwidth=-164456589,latmheight=242717215)
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:58:11.841,minLatStart=          -190593108,minLatEnd=            42295820,
minLonStart=          -171937418,minLonEnd=            13952576,slopeLatm=        0.0567304045,slopeLonm=        0.0403656587,id=29134,c1=29182,c2=29183,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=29182,c2r=29183,r=0,lonmwidth=185889994,latmheight=232888928)



 (int) 188062582 child start
 (int) 13952578 parent end
I see

No I don't

178409167


ugh another negative width problem


E/GpsTrailer( 8098): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:58:11.841,minLatStart=          -200421393,minLatEnd=            42295822,minLonStart=          -171937418,minLonEnd=            13952578,slopeLatm=        0.0622411221,slopeLonm=        0.0457783267,id=7759,c1=29134,c2=29135,tj=9612,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=29134,c2r=0,r=0,lonmwidth=185889996,latmheight=242717215) child: 
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:58:11.841,minLatStart=          -190593108,minLatEnd=            42295820,minLonStart=          -171937418,minLonEnd=            13952576,slopeLatm=        0.0567304045,slopeLonm=        0.0403656587,id=29134,c1=29037,c2=29038,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=-29134,c2r=-1073770958,r=0,lonmwidth=185889994,latmheight=232888928) 

Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMinLonm()) >= minLonmAtChildEnd + getWidthLonm()


GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:58:11.841,minLatStart=          -200421393,minLatEnd=            42295822,minLonStart=          -171937418,minLonEnd=            13952578,slopeLatm=        0.0622411221,slopeLonm=        0.0457783267,id=7759,c1=29134,c2=29135,tj=9612,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=29134,c2r=0,r=0,lonmwidth=185889996,latmheight=242717215)
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:58:11.841,minLatStart=          -190593108,minLatEnd=            42295820,minLonStart=          -171937418,minLonEnd=            13952576,slopeLatm=        0.0567304045,slopeLonm=        0.0403656587,id=29134,c1=29037,c2=29038,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=-29134,c2r=-1073770958,r=0,lonmwidth=185889994,latmheight=232888928)

260054143
95597553

1271712806470
1273496291841


GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:58:11.841,minLatStart=          -200421393,minLatEnd=            42295822,minLonStart=           178409167,minLonEnd=           373952578,slopeLatm=        0.0622411221,slopeLonm=        0.0457783267,id=7759,c1=29134,c2=29135,tj=9612,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=29134,c2r=0,r=0,lonmwidth=195543411,latmheight=242717215)
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 20:58:11.841,minLatStart=          -190593108,minLatEnd=            42295820,minLonStart=          -171937418,minLonEnd=            13952576,slopeLatm=        0.0567304045,slopeLonm=        0.0403656587,id=29134,c1=29037,c2=29038,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=-29134,c2r=-1073770958,r=0,lonmwidth=185889994,latmheight=232888928)

 (int) -99945857 child end min lon
 (int) 260054142 

again and again and again!

E/GpsTrailer( 8772): Bad child data!!! 

GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-16 23:07:42.388,minLatStart=          -221711516,minLatEnd=            99956357,minLonStart=                   0,minLonEnd=                   0,slopeLatm=        0.0455671959,slopeLonm=       -0.0093796188,id=37000,c1=36730,c2=36731,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=36730,c2r=0,r=0,lonmwidth=0,latmheight=321667873) child: 

GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-16 23:07:42.388,minLatStart=          -221403194,minLatEnd=            99956355,minLonStart=           149175050,minLonEnd=           416132913,slopeLatm=        0.0454337038,slopeLonm=        0.0411195457,id=36730,c1=36946,c2=36947,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=-36730,c2r=-1073778554,r=0,lonmwidth=266957863,latmheight=321359549) Util.makeContinuousFromStartLonm(minLonmAtChildStart, child.getMinLonm()) >= minLonmAtChildStart + getWidthLonm()



E/GpsTrailer(10722): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-30 17:44:00.943,minLatStart=          -185648353,minLatEnd=           145377171,minLonStart=          -180000000,minLonEnd=            49261643,slopeLatm=        0.0167952087,slopeLonm=        0.0240337346,id=-10460,c1=91288,c2=91289,tj=-2147483648,numsegments=1,startTimeJump=127867368,endTimeJump=191801056,ws.start=127867368,ws.end=191801056,c1r=91288,c2r=91289,r=0,lonmwidth=229261643,latmheight=331025524) child: GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-30 17:44:00.943,minLatStart=          -185648353,minLatEnd=           145374393,minLonStart=          -180000000,minLonEnd=           179999999,slopeLatm=        0.0167960022,slopeLonm=       -0.0133218486,id=91288,c1=91346,c2=91347,tj=-2147483648,numsegments=1,startTimeJump=127867368,endTimeJump=191801056,ws.start=127867368,ws.end=191801056,c1r=0,c2r=0,r=0,lonmwidth=359999999,latmheight=331022746) Util.makeContinuousFromStartLonm(minLonmAtChildStart, child.getMaxLonm()) >= minLonmAtChildStart + getWidthLonm()


--

//child max lonm at start
 (int) 179999999

//parent max lonm at start
 (int) 35755606

GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-16 23:07:42.388,minLatStart=          -221711516,minLatEnd=           100216237,minLonStart=          -180000000,minLonEnd=            35755606,slopeLatm=        0.0454546772,slopeLonm=        0.0238159262,id=7759,c1=37000,c2=37001,tj=9612,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=37000,c2r=0,r=0,lonmwidth=215755606,latmheight=321927753)

GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-16 23:07:42.388,minLatStart=          -221711516,minLatEnd=            99956357,minLonStart=          -180000000,minLonEnd=           179999999,slopeLatm=        0.0455671959,slopeLonm=       -0.0386368372,id=37000,c1=36730,c2=36731,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=36730,c2r=0,r=0,lonmwidth=359999999,latmheight=321667873)



E/GpsTrailer(11371): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-17 20:53:13.943,minLatStart=          -221403194,minLatEnd=           103554047,minLonStart=                   0,minLonEnd=          2147483647,slopeLatm=        0.0438760296,slopeLonm=        0.0003713412,id=36730,c1=37061,c2=37062,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=-36730,c2r=-1073778554,r=0,lonmwidth=2147483647,latmheight=324957241) child: GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-16 23:07:42.388,minLatStart=          -221403194,minLatEnd=            99956355,minLonStart=           149175050,minLonEnd=           416132913,slopeLatm=        0.0454337038,slopeLonm=        0.0411195457,id=-36730,c1=37061,c2=37062,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=37061,c2r=37062,r=0,lonmwidth=266957863,latmheight=321359549) Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMinLonm()) >= minLonmAtChildEnd + getWidthLonm()                                                                                                                                       
E/GpsTrailer(11371): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-17 20:53:13.943,minLatStart=          -221403194,minLatEnd=           103554047,minLonStart=                   0,minLonEnd=          2147483647,slopeLatm=        0.0438760296,slopeLonm=        0.0003713412,id=36730,c1=37061,c2=37062,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=-36730,c2r=-1073778554,r=0,lonmwidth=2147483647,latmheight=324957241) child: GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-16 23:07:42.388,minLatStart=          -221403194,minLatEnd=            99956355,minLonStart=           149175050,minLonEnd=           416132913,slopeLatm=        0.0454337038,slopeLonm=        0.0411195457,id=-36730,c1=37061,c2=37062,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=37061,c2r=37062,r=0,lonmwidth=266957863,latmheight=321359549) Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMaxLonm()) >= minLonmAtChildEnd + getWidthLonm()                                                                                                                                       
E/GpsTrailer(11371): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-17 20:53:13.943,minLatStart=          -221403194,minLatEnd=           103554047,minLonStart=                   0,minLonEnd=          2147483647,slopeLatm=        0.0438760296,slopeLonm=        0.0003713412,id=36730,c1=37061,c2=37062,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=-36730,c2r=-1073778554,r=0,lonmwidth=2147483647,latmheight=324957241) child: GpsLocCacheRowData(startTimeMs=2010-05-17 00:07:28.620,endTimeMs=2010-05-17 20:53:13.943,minLatStart=            40828258,minLatEnd=            40828258,minLonStart=            14247336,minLonEnd=            14247336,slopeLatm=        0.0002730873,slopeLonm=        0.0000343834,id=-1073778554,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0) Util.makeContinuousFromStartLonm(minLonmAtChildStart, child.getMinLonm()) >= minLonmAtChildStart + getWidthLonm()                                                                                                                                             
E/GpsTrailer(11371): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-17 20:53:13.943,minLatStart=          -221403194,minLatEnd=           103554047,minLonStart=                   0,minLonEnd=          2147483647,slopeLatm=        0.0438760296,slopeLonm=        0.0003713412,id=36730,c1=37061,c2=37062,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=-36730,c2r=-1073778554,r=0,lonmwidth=2147483647,latmheight=324957241) child: GpsLocCacheRowData(startTimeMs=2010-05-17 00:07:28.620,endTimeMs=2010-05-17 20:53:13.943,minLatStart=            40828258,minLatEnd=            40828258,minLonStart=            14247336,minLonEnd=            14247336,slopeLatm=        0.0002730873,slopeLonm=        0.0000343834,id=-1073778554,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0) Util.makeContinuousFromStartLonm(minLonmAtChildStart, child.getMaxLonm()) >= minLonmAtChildStart + getWidthLonm()                                                                                                                                             
E/GpsTrailer(11371): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-17 20:53:13.943,minLatStart=          -221403194,minLatEnd=           103554047,minLonStart=                   0,minLonEnd=          2147483647,slopeLatm=        0.0438760296,slopeLonm=        0.0003713412,id=36730,c1=37061,c2=37062,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=-36730,c2r=-1073778554,r=0,lonmwidth=2147483647,latmheight=324957241) child: GpsLocCacheRowData(startTimeMs=2010-05-17 00:07:28.620,endTimeMs=2010-05-17 20:53:13.943,minLatStart=            40828258,minLatEnd=            40828258,minLonStart=            14247336,minLonEnd=            14247336,slopeLatm=        0.0002730873,slopeLonm=        0.0000343834,id=-1073778554,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0) Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMinLonm()) >= minLonmAtChildEnd + getWidthLonm()                                                                                                                                              
E/GpsTrailer(11371): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-17 20:53:13.943,minLatStart=          -221403194,minLatEnd=           103554047,minLonStart=                   0,minLonEnd=          2147483647,slopeLatm=        0.0438760296,slopeLonm=        0.0003713412,id=36730,c1=37061,c2=37062,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=-36730,c2r=-1073778554,r=0,lonmwidth=2147483647,latmheight=324957241) child: GpsLocCacheRowData(startTimeMs=2010-05-17 00:07:28.620,endTimeMs=2010-05-17 20:53:13.943,minLatStart=            40828258,minLatEnd=            40828258,minLonStart=            14247336,minLonEnd=            14247336,slopeLatm=        0.0002730873,slopeLonm=        0.0000343834,id=-1073778554,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0) Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMaxLonm()) >= minLonmAtChildEnd + getWidthLonm()                                                                                                                                              
D/AndroidRuntime(11371): Shutting down VM                                                                                                                                                                                                                   
W/dalvikvm(11371): threadid=1: thread exiting with uncaught exception (group=0x4001d7e0)                                                                                                                                                                    
E/AndroidRuntime(11371): FATAL EXCEPTION: main                                                                                                                                                                                                              
E/AndroidRuntime(11371): junit.framework.AssertionFailedError                                                                                                                                                                                               
E/AndroidRuntime(11371):        at junit.framework.Assert.fail(Assert.java:47)                                                                                                                                                                              
E/AndroidRuntime(11371):        at junit.framework.Assert.fail(Assert.java:53)                                                                                                                                                                              
E/AndroidRuntime(11371):        at com.rareventure.gps2.database.GpsLocCacheRow.checkDataHack(GpsLocCacheRow.java:222)                                                                                                                                       
E/AndroidRuntime(11371):        at com.rareventure.gps2.database.cachecreator.GpsLocCacheRowData.updateLstb(GpsLocCacheRowData.java:137)                                                                                                                     
E/AndroidRuntime(11371):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPoint(AddPointStatus.java:742)                                                                                                                               



E/GpsTrailer(11481): Bad data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-22 23:11:31.167,minLatStart=         -2147483648,minLatEnd=           139295504,minLonStart=                   0,minLonEnd=          2147483647,slopeLatm=        0.8789836764,slopeLonm=        0.0000000000,id=7759,c1=37000,c2=37001,tj=9612,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=37000,c2r=0,r=0,lonmwidth=2147483647,latmheight=-2008188144) getInt(HEIGHT_LATM) <0
E/GpsTrailer(11481): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-22 23:11:31.167,minLatStart=         -2147483648,minLatEnd=           139295504,minLonStart=                   0,minLonEnd=          2147483647,slopeLatm=        0.8789836764,slopeLonm=        0.0000000000,id=7759,c1=37000,c2=37001,tj=9612,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=37000,c2r=0,r=0,lonmwidth=2147483647,latmheight=-2008188144) child: GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-22 23:11:31.167,minLatStart=          -248678368,minLatEnd=           139295502,minLonStart=                   0,minLonEnd=          2147483647,slopeLatm=        0.0399135835,slopeLonm=        0.0000000000,id=37000,c1=37117,c2=37118,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=37117,c2r=0,r=0,lonmwidth=2147483647,latmheight=387973870) child.getEndMinLatm() < minLatmAtChildEnd
E/GpsTrailer(11481): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-22 23:11:31.167,minLatStart=         -2147483648,minLatEnd=           139295504,minLonStart=                   0,minLonEnd=          2147483647,slopeLatm=        0.8789836764,slopeLonm=        0.0000000000,id=7759,c1=37000,c2=37001,tj=9612,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=37000,c2r=0,r=0,lonmwidth=2147483647,latmheight=-2008188144) child: GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-22 23:11:31.167,minLatStart=          -248678368,minLatEnd=           139295502,minLonStart=                   0,minLonEnd=          2147483647,slopeLatm=        0.0399135835,slopeLonm=        0.0000000000,id=37000,c1=37117,c2=37118,tj=-2147483648,numsegments=1,startTimeJump=56829944,endTimeJump=85244912,ws.start=56829944,ws.end=85244912,c1r=37117,c2r=0,r=0,lonmwidth=2147483647,latmheight=387973870) child.getEndMaxLatm() > maxLatmAtChildEnd
D/AndroidRuntime(11481): Shutting down VM
W/dalvikvm(11481): threadid=1: thread exiting with uncaught exception (group=0x4001d7e0)
E/AndroidRuntime(11481): FATAL EXCEPTION: main
E/AndroidRuntime(11481): junit.framework.AssertionFailedError
E/AndroidRuntime(11481):        at junit.framework.Assert.fail(Assert.java:47)
E/AndroidRuntime(11481):        at junit.framework.Assert.fail(Assert.java:53)
E/AndroidRuntime(11481):        at com.rareventure.gps2.database.GpsLocCacheRow.checkDataHack(GpsLocCacheRow.java:225)


TODO 1: FIX GETSCORE~!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
TODO 2: what are we going to do about overflow. It's a constant worry with the slope. Maybe a safe multiply?

Ok, so the problem is this.
We store the values by a start time, position, a slope and an end postion. What if we get rid of slope and replace it
with end position? That would be a lot better. We could calculate slope that way. We wouldn't have to worry about
things jumping out of place, except for when we do the least squared data. But think about that... we have a start,
end and a slope. We don't want it to wrap. At least in latm. In lonm, I suppose it can. But there is also another rub,
because eventually, you know that it's going to happen that lonm will overflow, too. So maybe an end point is safer.

This is too much of a headache to leave alone, although, you know it might create problems. But you see the deal, right?
I mean we often are looking at end and start and... hmm. Do you see, though? There is a difference between a slope that
wraps the world once and one that wraps it 3 times. See, so we'd have to store it a weird way to make it work, and that
would probably be worse than just a slope. So I think that we can just have a maximum and minimum slope. And just expand
the area if the correct slope is not good. Yea, that would fix a lot of problems.

So let's say that we have an infinite time jump. And two points really far away from each other just happen to match
up so that we could create a gps point that links the two. You see the issue?

There is a jump in distance. Time jump was really not meant for large distance separation. It really only makes sense
when you are looking straight down on it?

I think so, because if you have this:


          / A
         /



     / B
    /

You'll never want that gap filled in.

So only if A and B overlap given a certain granularity should we even consider having a time jump.

Then we'll have to have a maximum slope, too.

So what is the granularity? Well imagine that each cache point represents one point. I don't think this matters too much.
Because anytime there is any gap at all, technically, you shouldn't have a line there. It really depends on the granularity
of the screen and the size of the dot we use to display the point.

No, I think that we want a zero distance gap. Because you neveer cross the area between two areas, you never crossed it.
I mean this case is an edge, I think... ah but you see, this messes with the slope decision, and that is it.

That is what determines if A can be added to B with a certain slope. So these things are a little related.

So the point is that it doesn't depend on the pixel granularity, but the opposite. Because we don't want to join points
that are too far apart, regardless of the gap.

No, no gap.

FACT: since a gps cache point can cover a long distance, (in case of a plane trip) we can't have any gap. Because if
      we did, then we would have a false long plane trip between two ones going half way. Like this:
                 (Italy)
BA --------------  -------------- Japan (or somewhere with the same slope)
        A                    B

We don't want to display this as

BA ------------------------------ Japan 

So that should solve a lot of problems with the slope becoming too great. However, it doesn't fix everything., because we don't
want to start and end in a way that will cause the end to be higher than a certain max integer level, giving some spare bits.
And I think that is it. If the slope would cause the end time position to be too high, we lower it.

So it's all in the least squares data that we need to solve this.

Now I'm confused. Because ok, there is a gap between points, a distance and time gap. But we are always looking down at it, with
0 slope. Now, in general not considering time jump, it's AOK that there are gaps, because that is the resolution of the point.
But with a time jump. So maybe it means the edge of the point. Now that is where the line starts to draw. So think about it,
it's the line that we are worried about. 

Ok, currently we are saying along the line, no gaps. So that means from the center of the first guy to the center of the second
guy. But considering that with zero slope, we would accept any point along the way, (even regardless of timejump, (except for
the normal timejump rules, of course)) We are only concerned with the areas between the start and end. However, I'm not
sure about that, either. What about the end? Do we treat it like the start? Or something else?

Ok, so what if the thing about time jump is that it can't be used to choose slope? Hmm... so slope is special. Like we can't
have a slope of too much and be, ok that's cool. It's not. There can only be slope if a non-timejumped line follows a certain
path. 

Now the thing is that a slope has a direction and an angle. I wonder if we just calculate the slope for ... hold on.. is there
a problem here? Because we aren't dealing with individual points, but with children.
And children have time jumps of their own, and they also have their own zoom resolutions, etc.

So a huge child and a small point. Can we handle this condition? What about two huge childs?

Well, we don't want a gap. We know the edges of the child. So in this situation, wait, we have two children. They both are
boxes. Lopsided boxes, remember. So if there is a time jump, *any* time jump, we need to make them touch when looking straight
down through time? 

Or maybe we have this line from start to the finish, or a lopside box. Now the rule is that the line must always be going 
through a child. But of course that is not always accurate, either. The truth is that with a time jump and children they is
no way to determine whether it exactly will match. 

The thing is with time jump is that the idea of having a single line is not correct. There is a blurring patch of data.
If you have two cities next to each other and you never go between them, you should not have a line between them.
Time jump shuold allow you to start over, I suppose. Well kind of. Because starting over would affect the color. 
Right, that's why time jump and slope don't mix well. 

Don't you see, the reason we have time jump at all is because of the colors. We can't dot the lines. Of course a certain
granularity in points would fix this, but.. So wait. what does this mean? Can we get away with having a zero slope when
there are time jumps?

So we add a point, lets say from BA to Italy.
What happens? There is a zero time jump data, and we add that as normal. Then there is a big time jump.
Now, we would add the same point! We only split it up if we have a time jump to be added. And of course, we wouldn't want
to do it for the time jump point. I think that will work!

24 tj is 23 which is also it's sibling


point 12
has two time sections,
one links to 24 (which has a tj to 45)
and the other directly to 45
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:25:11.331,minLatStart=           -25600619,minLatEnd=           -25595181,minLonStart=           -54575808,minLonEnd=           -54570441,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=25,c1=23,c2=24,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=438008,ws.start=0,ws.end=438008,c1r=0,c2r=24,r=0,lonmwidth=5367,latmheight=5438)
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:25:11.331,minLatStart=           -25600619,minLatEnd=           -25595181,minLonStart=           -54575808,minLonEnd=           -54570441,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=-2147483648,c1=23,c2=24,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=438008,ws.start=0,ws.end=17090,c1r=0,c2r=45,r=0,lonmwidth=5367,latmheight=5438)



ws.start=292005,ws.end=438008


GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:25:11.331,minLatStart=           -25600619,minLatEnd=           -25595181,minLonStart=           -54575808,minLonEnd=           -54570441,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=25,c1=23,c2=24,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=438008,ws.start=0,ws.end=17090,c1r=0,c2r=45,r=0,lonmwidth=5367,latmheight=5438)
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:25:11.331,minLatStart=           -25600619,minLatEnd=           -25595181,minLonStart=           -54575808,minLonEnd=           -54570441,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=25,c1=23,c2=24,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=438008,ws.start=292005,ws.end=438008,c1r=0,c2r=24,r=0,lonmwidth=5367,latmheight=5438) child 24


GpsLocCacheRowData(startTimeMs=2010-04-20 06:20:48.112,endTimeMs=2010-04-20 06:25:11.331,minLatStart=           -25600612,minLatEnd=           -25599218,minLonStart=           -54573609,minLonEnd=           -54572547,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=45,c1=34,c2=42,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=17090,ws.start=0,ws.end=17090,c1r=0,c2r=42,r=0,lonmwidth=1062,latmheight=1394)
GpsLocCacheRowData(startTimeMs=2010-04-20 06:20:48.112,endTimeMs=2010-04-20 06:25:11.331,minLatStart=           -25600612,minLatEnd=           -25599220,minLonStart=           -54573609,minLonEnd=           -54572549,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=24,c1=43,c2=44,tj=45,numsegments=1,startTimeJump=17090,endTimeJump=438008,ws.start=292005,ws.end=438008,c1r=-24,c2r=-1073741848,r=0,lonmwidth=1060,latmheight=1392)

E/CacheViewer(15014): 25,23,24,-2147483648,1,0,438008,2010-04-20 05:33:26.470,2010-04-20 06:24:37.263,-25600619,-25595181,-54575808,-54570441,0.000000,0.000000
E/CacheViewer(15014): 24,52,53,45,1,17090,438008,2010-04-20 06:20:48.112,2010-04-20 06:25:11.331,-25600612,-25599220,-54573609,-54572549,0.000000,0.000000
E/CacheViewer(15014): 45,34,42,-2147483648,1,0,17090,2010-04-20 06:20:48.112,2010-04-20 06:25:11.331,-25600612,-25599218,-54573609,-54572547,0.000000,0.000000


ok so the thing is that we have a chain of time siblings. So I think considering that a chain is just a single chain, and not a graph or anything crazy
like that, if the last time sibling of the guys match, we do nothing. However, we will still need to choose the right time sibling for the resulting 
combined node. So 


E/CacheViewer(15412): 196,208,195,-2147483648,1,0,11393,2010-04-20 05:33:26.470,2010-04-21 04:16:12.124,-26026418,-25595165,-54586354,-54500350,0.000000,0.000000
 E/CacheViewer(15412): 208,205,206,209,1,2250,11393,2010-04-20 05:33:26.470,2010-04-21 04:16:12.124,-26026418,-25595167,-54586354,-54500352,0.000000,0.000000
 E/CacheViewer(15412): 209,194,195,-2147483648,1,0,2250,2010-04-20 05:33:26.470,2010-04-21 04:16:09.941,-26026011,-25595167,-54586333,-54500352,0.000000,0.000000

 E/CacheViewer(15412): 195,-2147483648,-2147483648,210,1,0,11393,2010-04-21 04:16:07.740,2010-04-21 04:16:09.941,-26025533,-26025533,-54586301,-54586301,-0.217174,-0.014539
 E/CacheViewer(15412): 210,-2147483648,-2147483648,-2147483648,1,0,2250,2010-04-21 04:16:09.941,2010-04-21 04:16:12.124,-26026011,-26026011,-54586333,-54586333,-0.186441,-0.009620









E/CacheViewer(15597): 196,208,195,-2147483648,1,0,11393,2010-04-20 05:33:26.470,2010-04-21 04:16:12.124,-26026418,-25595165,-54586354,-54500350,0.000000,0.000000
E/CacheViewer(15597): 209,194,195,-2147483648,1,0,2250,2010-04-20 05:33:26.470,2010-04-21 04:16:09.941,-26026011,-25595167,-54586333,-54500352,0.000000,0.000000
E/CacheViewer(15597): 195,-2147483648,-2147483648,210,1,0,11393,2010-04-21 04:16:07.740,2010-04-21 04:16:09.941,-26025533,-26025533,-54586301,-54586301,-0.217174,-0.014539

E/CacheViewer(15597): 1,165,166,14,1,1478277,7776000000,2010-04-20 05:33:26.470,2010-04-21 04:16:12.124,-26026418,-25595167,-54586354,-54500358,0.000000,0.000000
E/CacheViewer(15597): 14,163,164,144,1,657012,1478277,2010-04-20 05:33:26.470,2010-04-21 04:16:12.124,-26026418,-25595167,-54586354,-54500358,0.000000,0.000000
E/CacheViewer(15597): 144,161,162,25,1,438008,657012,2010-04-20 05:33:26.470,2010-04-21 04:16:12.124,-26026418,-25595169,-54586354,-54500360,0.000000,0.000000
E/CacheViewer(15597): 25,159,160,70,1,17090,438008,2010-04-20 05:33:26.470,2010-04-21 04:16:12.124,-26026418,-25595171,-54586354,-54500362,0.000000,0.000000
E/CacheViewer(15597): 70,193,178,196,1,11393,17090,2010-04-20 05:33:26.470,2010-04-21 04:16:12.124,-26026418,-25595169,-54586354,-54500354,0.000000,0.000000
E/CacheViewer(15597): 196,208,195,-2147483648,1,0,11393,2010-04-20 05:33:26.470,2010-04-21 04:16:12.124,-26026418,-25595165,-54586354,-54500350,0.000000,0.000000
E/CacheViewer(15597): 208,205,206,209,1,2250,11393,2010-04-20 05:33:26.470,2010-04-21 04:16:12.124,-26026418,-25595167,-54586354,-54500352,0.000000,0.000000
E/CacheViewer(15597): 195,-2147483648,-2147483648,210,1,0,11393,2010-04-21 04:16:07.740,2010-04-21 04:16:09.941,-26025533,-26025533,-54586301,-54586301,-0.217174,-0.014539
E/CacheViewer(15597): 209,194,195,-2147483648,1,0,2250,2010-04-20 05:33:26.470,2010-04-21 04:16:09.941,-26026011,-25595167,-54586333,-54500352,0.000000,0.000000
E/CacheViewer(15597): 210,-2147483648,-2147483648,-2147483648,1,0,2250,2010-04-21 04:16:09.941,2010-04-21 04:16:12.124,-26026011,-26026011,-54586333,-54586333,-0.186441,-0.009620


0 child1rep
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 04:16:12.124,minLatStart=           -26026418,minLatEnd=           -25595167,minLonStart=           -54586354,minLonEnd=           -54500352,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=208,c1=205,c2=206,tj=-2147483648,numsegments=1,startTimeJump=2250,endTimeJump=11393,ws.start=2250,ws.end=11393,c1r=205,c2r=206,r=0,lonmwidth=86002,latmheight=431251)
1 child1rep
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 04:16:09.941,minLatStart=           -26026011,minLatEnd=           -25595167,minLonStart=           -54586333,minLonEnd=           -54500352,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=209,c1=194,c2=195,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=2250,ws.start=0,ws.end=2250,c1r=194,c2r=195,r=0,lonmwidth=85981,latmheight=430844)


GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 04:16:12.124,minLatStart=           -26026418,minLatEnd=           -25595167,minLonStart=           -54586354,minLonEnd=           -54500352,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=208,c1=205,c2=206,tj=-2147483648,numsegments=1,startTimeJump=2250,endTimeJump=11393,ws.start=2250,ws.end=11393,c1r=205,c2r=206,r=0,lonmwidth=86002,latmheight=431251)
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 04:16:09.941,minLatStart=           -26026011,minLatEnd=           -25595167,minLonStart=           -54586333,minLonEnd=           -54500352,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=209,c1=194,c2=195,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=2250,ws.start=0,ws.end=2250,c1r=194,c2r=195,r=0,lonmwidth=85981,latmheight=430844)

ok, so the issue is this:
orig:

      0-b
    196
    /  \
  194   195
  0-b     0-b



r1
      a-b
    196
    /  \
  208   195
  a-b     0-b


r2             0-a
            196
       0-a  /  \   0-a
          209   210
         /  \     
        194  195
       0-a     0-b

Now, 195 and 210 want to be time siblings. This is bad, because their times overlap.
We can't really get rid of the 0-b for 195, since 209 needs it for 0-a.

So the weird thing is lets say were a-b. Then we'd hit 196 and return 195
But for 0-a, we'd hit 209 and then 195, as well.

So, unless we want to rearrange everything and put time ranges on links, we
have to copy 196.

I don't like the solution, but I don't see another one. 

But the weird thing is that, ok, from 196 we reach 195, right?

So why from 196 a-b, we can just hit it to 195.
But from 0-a, we need to go through 209 first?

Well, the idea is that the original 196 rejected point 210.
So before we get to the original 196 (now 209) for 0-a, we
need to check if it fits in 210.
Then we go to 195. 

So, given that, it's very odd that we can combine 196 anyway.
Because the new 196 contains point 210.
The old one does too, I suppose, but in a different way.
So the weird thing here is that.

Which is why I never found this before... but now I have
another question. Why is that this guy has a zero slope
for the 0 tj one?

Also, should we disallow this unholy union of 196
beforehand in addPoint()?
Because they aren't really the same. And we can't really
allow them to be joined anyway? Hmm..

So here is my question. In this extremely unlikely scenario,
the parent cannot be combined.. This is true. 
But we are splitting up the parent into two rows.

Oh, I remember, we have code to find dead parents, based on
the time split. Ok, so 

FACT: We don't have to worry about splitting up a node, because
if we do, we find all the parents and update their replacement.
This also means we can only create splits in addPoint(), not in 
updateRowAndChildren()

I think our original plan is best then. It's simpliest in terms
of weird scenarios that may come up.

Ok.
whew!


startTimeJump=17090,endTimeJump=438008,ws.start=17090,ws.end=292005,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=


E/CacheViewer(16205): 252,230,195,211,1,2250,5063,2010-04-20 05:33:26.470,2010-04-21 04:16:17.528,-26027373,-25595161,-54586424,-54500346,0.000000,0.000000
E/CacheViewer(16205): 230,246,229,-2147483648,1,3375,5063,2010-04-20 05:33:26.470,2010-04-21 04:16:17.528,-26027373,-25595163,-54586424,-54500348,0.000000,0.000000
E/CacheViewer(16205): 195,-2147483648,-2147483648,-2147483648,1,0,11393,2010-04-21 04:16:07.740,2010-04-21 04:16:09.941,-26025533,-26025533,-54586301,-54586301,-0.217174,-0.014539

Look at that time jump discrepancy!

--

Ok, autozoom. How to?

We just do it.

Ok, the way we did it before doesn't make a lot of sense to me. So, what are we going to do?
Well, lets see. We have these nodes, and we just keep going down as long as the time matches.
Now the only problem is to know when to stop. Because even the highest, biggest node we could
take a sliver and say, that's the autozoom area. So, the thing is, we're kind of finding where
the points aren't. Cause we start at the top level, and then check out the sub points. If we
find they're down both, that means something different than if they are down one. I mean
we could keep striking down, finding all points. But we might be going across the world. So
I think the thing is that we have to take a look at the size of the cache points compared to
the total area they represent. And once this ratio gets small enough, we stop, and return
the total area they represent. In other words, were coool.

So, wait. We have to only go down the children that are the biggest. Because that's the key.
But we don't need to find the absolute biggest, I don't think. Just assure that it isn't the 
smallest. Hmm.. no. Because if we get a really small one, we're screwed then. But we can still
afford to be a little haphazard. I mean, we don't want to resort everytime. We could use a 
tree map. Or... no a tree map would be best.

So for autozoom, I had some more ideas. We go down to a certain level, whatever that we would
like to guess. Then use that as a template and go down in other branches as well to find the
total extent of the area. If the ratio is not good enough, we start over and try again.
I think caching it is a little silly because we already have a cache.

So lets do that then


D/HACK    ( 1798): score1 is 9223372036854775807 score2 is 1292632090 currScore is 0 
child1 is GpsLocCacheRow(startTimeMs=2010-04-21 17:15:41.194,endTimeMs=2010-04-21 17:25:25.545,minLatStart=           -32524284,minLatEnd=           -32524284,minLonStart=           -58328921,minLonEnd=           -58328921,slopeLatm=       -0.1765890718,slopeLonm=       -0.1648478359,id=4747,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=1000,endTimeJump=7595,widthLonm=0,heightLatm=0) 
child2 is GpsLocCacheRow(startTimeMs=2010-04-21 17:25:25.545,endTimeMs=2010-04-21 17:35:35.263,minLatStart=           -32627474,minLatEnd=           -32627474,minLonStart=           -58425250,minLonEnd=           -58425250,slopeLatm=       -0.1711069643,slopeLonm=       -0.0694616213,id=4748,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=1000,endTimeJump=7595,widthLonm=0,heightLatm=0) ws is WorkSegment(start=1000,end=7595)

lstb2:
endTime	1271842738458	
latmPerMs	0.0	
lds	LonmDataSet  (id=830103286120)	
lonmPerMs	0.0	
maxLatm	-32524280	
minLatm	-32850253	
newPointTimeJump	0	
time	1271841341194	


lds.width= (int) 208696

curr score child2:
 (int) 3828711
1271841341194
0.0
com.rareventure.gps2.database.LonmDataSet@4601ef90
0.0
-25593656
-32524284
0
1271712806470

Math.abs((lds.getWidth()) * (maxLatm - minLatm) - otherChild.getHeightLatm() * otherChild.getWidthLonm());

3828711 * (-32524284 - (-25593656)) - 



208696 * (-32524280 - -32850253) -   6930628 * 3828711

Ok, overflow, we get it. darn overflow again.

Now what we need to do is this. The thing is that if we add a point... you know what it is..
it's not the score that matters, (unless we want to consider overlap.. Cause think, suppose
that you do have overlap. You'll be perpetuating the dragon, man. Cause, yea, you don't want
it to flip the wrong way. Maybe the score is good enough. Well, we could consider the number
of points stored, or the max depth or something. What about the number of points stored? I
mean that's the real issue here. Well, again, we have the overlap issue. 

Ok, like if the size doesn't keep on increasing, maybe we can just add the point. Because 
otherwise, you know what? It's the same. If we have to search two children rather than one
to consider one point on the world, it's even worse. 

So think, the big problems are not balance of size. They are:

1. number of points in each direction
2. overlap

Cause overlap causes you to search through both points. So maybe we should do it non-fuzzily

So first, if overlap increases, no way.
Second, number of points difference.

That's the thing, so if we are creating overlap, we will always need to search both trees
from now on to reach that point. 

I know that a little overlap may be ok. But it's so difficult to tell between good overlap
and bad. What if we ask, can we add the point without changes to one? If we can, then
we know the overlap is bad. 
Hmm, but we could maybe expand it in time, and then we'd still have some weird craziness.
But time is ok, I think. Right? Ok, same problem same damn conclusion. Let's just try
it with area

Ok, so the question is now, how can we dab that finger on the screen. So right now we 
have the code to find the position in drawer. I like this, I suppose. We don't want to
have to cache everything multiple times. But, so when we dab that finger, we need to tell
the drawer? Is that the way we want to do it? Or what. 

Ok, so when we are zoomed out far, we need to know how to handle this.

Ok, so the thing is:

without chungking
endTimeMs	0	
latmHeight	30415	
latmStart	22282525	
lonmStart	114159900	
lonmWidth	27464	
startTimeMs	0	

with chungking:
endTimeMs	0	
latmHeight	30414	
latmStart	22282367	
lonmStart	114157924	
lonmWidth	27464	
startTimeMs	0	

Chungking
UserLocationRow(id=16,LATM=22295251,LONM=114174122,CREATED_ON=0,NAME=4368756E676B696E67206C69627261727974656C20000000000000000000000000000000000000000000000000000000)

8/20

D/HACK    (32530): localStBox is SpaceTimeBox(startTimeMs=2010-08-18 11:24:35.434,endTimeMs=2010-08-18 13:15:19.035,latStart=22783171,latHeight=56476,lonStart=108282928,lonWidth=1738)
D/GpsTrailer(32530): drawing bitmap to Point(0, 0)


GpsLocCacheRow(startTimeMs=2010-08-19 20:12:41.565,endTimeMs=2010-08-19 20:16:07.804,minLatStart=            22797366,minLatEnd=            22797366,minLonStart=           108365364,minLonEnd=           108365364,slopeLatm=        0.0057457611,slopeLonm=       -0.0120442789,id=203350,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=7483779,endTimeJump=16838502,widthLonm=0,heightLatm=0)



1282231475434

1282231475434 = Thu Aug 19 23:24:35 HKT 2010, 2010-08-19 23:24:35.434
1282101875434 = Wed Aug 18 11:24:35 HKT 2010, 2010-08-18 11:24:35.434

8/22

ok, so we need to figure out what is going on with the cache.

It's hard to follow, ok, so heres the deal...

Ok so, what we want to do is find the cache items that contain the 
point and what we are searching for... like what if.... what if we 
just display the squares?

Ok, so the issue that we are worried about is simply if there is
too much overlap or the nodes are unbalanced... we know this is
happening, because we see stuff like:

D/GpsTrailer( 1043): overlaps true swim down false --- GpsLocCacheRow(startTimeMs=2013-04-02 22:46:04.138,endTimeMs=2013-04-04 02:25:35.321,minLatStart=            22816698,minLatEnd=            22834114,minLonStart=           108321102,minLonEnd=           108353231,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=1742402,c1=1743841,c2=1743842,tj=-2147483648,numsegments=1,startTimeJump=85244912,endTimeJump=7776000000,widthLonm=32129,heightLatm=17416)
D/GpsTrailer( 1043): overlaps true swim down true --- GpsLocCacheRow(startTimeMs=2013-04-04 02:25:35.321,endTimeMs=2013-04-07 19:41:51.517,minLatStart=            22773486,minLatEnd=            22833992,minLonStart=           108314980,minLonEnd=           108420098,slopeLatm=        0.0000000000,slopeLonm=        0.0000000000,id=1744402,c1=1749761,c2=1749762,tj=-2147483648,numsegments=1,startTimeJump=85244912,endTimeJump=7776000000,widthLonm=105118,heightLatm=60506)


So the question is, what do we want to do? The thing is that when we have overlap at the upper levels it's going to cause overlap at the bottom ones, and that will just increase. Maybe we should just try preventing it and see what happens. Yes, let's just do that...

So how the heck are we going to do overlap?

It's lopsided boxes we are talking about.

Darnit.

It's difficult.

it's error prone. 

and I don't even know if it will work.

I suppose we could just do the space time boxes
but you got to realize that then we are messed up
if we have slope. Right?

We could also just measure the start and the end...
You know it's not perfect, but it should work.
Alright, that's the ticket. We can't solve every problem
that is laid before us.

ok, so here is the deal. What are we going to do about the rounding error of the
slope? Ok, I think I know... sort of. You know, it's tricky. Because we are using
this least squares thing, which gives us a slope. Then we use that slope to 
determine the extent of the box. Now, what I recommend is that... well think.
When we are determining the size of the child... when ??? 

Well, here is the real rub. Right now we are adding 2 to the end point of all 
the point guys. Well that's going to cause problems...
It's just that we need to be able to say,, ok it fits or it doesn't.

I'm going to bed. darn it. We'll deal with this tomorrow.

8/23

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=           -25599487,minLatEnd=           -25598250,minLonStart=           -54570449,minLonEnd=           -54568792,slopeLatm=        0.0026635628,slopeLonm=       -0.0037183356,id=4,c1=2,c2=3,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=1657,heightLatm=1237) 
GpsLocCacheRow(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:34.358,minLatStart=           -25595189,minLatEnd=           -25595189,minLonStart=           -54574735,minLonEnd=           -54574735,slopeLatm=       -0.0025539817,slopeLonm=       -0.0008227026,id=5,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=0,heightLatm=0)

Oh yea, when we add a new guy, we are mushing the overlap. Because if the guy doesn't accept it, then... well. wait.
So if i

Wait, so let me understand this... we can add it to child x, but we don't want to because it changes the score.
So what do we do? We create a parent for row, and place the child as a sibling to it. Now, suppose that sibling
was a child smack dab in the middle of child 1. Then it would still be a child of the row.

Now, I know what your thinking. What about balance?
Balance is fine, yo. Look man, if the row contains the point already, we need to add it to it. Period. Otherwise,
we are screwed. We can't not let the row contain it if it overlaps. Right?
Because otherwise, we got major overlap. 

So the thing is, though. What are we doing... I know what you are thinking. Maybe you are right, but it will be
a pain. The idea is to divide and separate. Look, ok. Look, we're probably going to need to do this.

I mean think about it. Yes, we might be able to create something that works. But, you know. it probably won't
be ideal.

And will have lots of problems.

But the thing is, that whenever we split something up, well, you know.

Ok, the current problem is that a row says, yo, I can handle the point, and in fact, if you don't give it to
me, there will be overlap, but I don't want it. Because if you give it to me, my children will be worse off
than before. 

But the thing is, we have other trees, other time jumps that are pointing to the sub trees. So the thing is,
if we have a node, and say, lets muck with the children. We are mucking with more than that row, but other
rows as well, outside of the tree.

So maybe the thing is, we need to be able to find parents. Period. I think we have something like that already.

But here we go, this looks like almost a total rewrite. How sad.

But here we go, anyway.

So what we need to do is this. We add a point to a row. It has the following responses:
1. must add (I control that space, or at least part of it)
2. don't need to add

and 
a. want to add.
b. don't want to add.

I think this "wanting" can be scraped. We are going to be moving points up and down now. So we don't need this
wanting anymore.

Now what happens here is that we will want to move children over. Oh my god, what a pain this will be.

Because we could go down several levels, and say, ok, this child. So what we'd need to do is search the child1
and find it's children that we want to move to the other child, based on their proximity to the other child.

Then once we do that. The children may be unbalanced. So what do we do... we rebalance them, and cause the 
same issue again. and again. and again.

It might make sense to "liberate" and the points under the parent and simply re-add them. But in that case,
we may need to do a major rebalance. And that would be bad. Because we can't re-add 10,000 points.

Ok, so adding the point would make the tree even more inbalanced. It would expand one over the other even
more. So what does that mean?

I mean that is just the case. Spilled milk. If the user keeps going in one direction, and never goes back,
than this will happen. There is nothing we can do about it, unless we push from one to the other. 

Well, think about this. It's not true. Because if the user is moving from one point to the other, we
can create a proper tree. The issue is that the new point is deep inside one of the children, but we refuse
to add it. Now that is the trick. Because one point leads to the next. If a node is on the border of a point,
then we are messed.

so in the current case, they are just points. So is the point in the center of them?

GpsLocationRow(timeMs=2010-04-20 05:53:50.120,latm=           -25595189,lonm=           -54574735,id=3)

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=           -25598252,minLatEnd=           -25598252,minLonStart=           -54570449,minLonEnd=           -54570449,slopeLatm=        0.0002234014,slopeLonm=       -0.0004448258,id=2,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=0,heightLatm=0)
GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-20 05:53:50.120,minLatStart=           -25598139,minLatEnd=           -25598139,minLonStart=           -54570674,minLonEnd=           -54570674,slopeLatm=        0.0041095852,slopeLonm=       -0.0056572966,id=3,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=0,heightLatm=0)

no, it's far away from them both. Well, actually were not sure, becasue we don't have the ending values. We should add them. 

So, what is happening?

Well, here is the original row:

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=           -25599487,minLatEnd=           -25598250,minLonStart=           -54570449,minLonEnd=           -54568792,slopeLatm=        0.0026635628,slopeLonm=       -0.0037183356,id=1,c1=2,c2=3,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=1657,heightLatm=1237)

--

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=-25598252,minLatEnd=-25598252,minLonStart=-54570449,minLonEnd=-54570449,slopeLatm=0.000223,slopeLonm=-0.000445,id=2,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=0,heightLatm=0,endMinLat=-25598139,endMaxLat=-25598138,endMinLon=-54570675,endMaxLon=-54570674)
GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25598139,minLatEnd=-25598139,minLonStart=-54570674,minLonEnd=-54570674,slopeLatm=0.004110,slopeLonm=-0.005657,id=3,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=0,heightLatm=0,endMinLat=-25595190,endMaxLat=-25595189,endMinLon=-54574735,endMaxLon=-54574734)


This crossing of points... it's ugly and meaningless.. You know? Cause we have a minimum
and a maximum..

I mean we have a slope.

I think the issue here is that I'm thinking of a 3d structure. It's not, it's a 2-d structure.
With a large enough time jump, we can ignore the 3d part of it. We can ignore the speed, 
and the timing, and just concentrate on the location of the points. Ah, so now we have an
elonginated diamond. 

And as long as the start of one point is timejump away from the end of the next point,
we're good? Well, no, it is 3d. Gosh darnit.

Ok so the thing is that we have these groups of points all going the same direction at the
same place but not the same time. Whether we split it up or keep it together, it is the same
problem. We don't know how to display it. So what we need to do is figure that out. But that
is independent of how we handle the cache. 

Ok, so lets do it.

We'll just take the average color, or something like that. Make it colorful.

Ok, so that's fine. Now we just need to figure out how to handle extra points, etc.

Now the thing we are worried about is that the children must be fully contained by the
parent. But if we keep the start and end we can say the whole thing is contained.

Right, so then we have the rounding issue. So lets see if I can figure this out.

Let's see. So if we have slope from 0 to 50 and the thing extends actually to some
number lets say 79. Then the question is, suppose that 79 rounds from like 78.5
so we choose 79. Then when we go back to 50. I don't think there is a problem.

No there isn't, as long as the point is before the end point
FACT:
1. create a slope based on the point
2. round it at the end.

Since the end is always going to have a greater distance than the point we start at
(since the end is further along and as the distance incraeses, the slopes diverge),
if the end is under .5 then the midpoint must be.

Ok, so that solves that issue.

So then the only other thing we have to worry about is overlap. So we either get really
good at detecting it, prove it isn't an issue (hard) or always worry about it.

Because we could have an x. just because one child doesn't accept, then we allow another to.

Hmm, the other issue is that we don't go all the way done to determine if a parent can
accept a child and not overlap. That may be an issue as well.

Cause if we do go all the way down... we are using a lot of time. Well, are we?

We can do a pre one and a post one, like we used to. That should work good enough.
So that handles that problem.

Then we need to detect overlap. I wonder what we can do?

So the question is if we skew one, can we make sure that the lines still cross?

I think that should work...because imagine everything is a bunch of plates. If we
slide them around, nothing changes on each plate. They just move.

So then we can skew the other one. But we have to be exact, right? If we can be,
that would be best. well, we can, because we can use doubles and we have the
end and the start. But when we skew... What are we doing, except adding slopes?
ok, so that's great. 

But how can we be perfect? I mean we'll be a tiny bit off, no matter what, right?
True, unless we multiply the time differences. Can we do that? Yes. So the deal is
this. We need the a common denominator. Right?

Cause of this

new lat slope for second box = (elat2-slat2)/(et2-st2) - (elat1-slat1)/(et1 - st1) 

new lat slope for second box = ((elat2-slat2)*(et1 - st1) - (elat1-slat1)*(et2-st2))/ ((et1 - st1) * (et2 - st2)) 

Yes, yes, that would be perfect. A perfect slope. Sure. but then what...
well that gives us the slope. Then we need to determine if it crosses the starting
square at any time. 

So it becomes a 2d problem. Ahh.ahhh.ahh. I see, we can eliminate time now!

Because for lon, we have:

new lon slope for second box = (elon2-slon2)/(et2-st2) - (elon1-slon1)/(et1 - st1) 

new lon slope for second box = ((elon2-slon2)*(et1 - st1) - (elon1-slon1)*(et2-st2))/ ((et1 - st1) * (et2 - st2)) 

So now we can say:
lon = ((elon2-slon2)*(et1 - st1) - (elon1-slon1)*(et2-st2))/ ((et1 - st1) * (et2 - st2))  * (t - st2) + slon2
lat = ((elat2-slat2)*(et1 - st1) - (elat1-slat1)*(et2-st2))/ ((et1 - st1) * (et2 - st2)) * (t - st2) + slat2

So be it. So as you can see, we can eliminate the denominator, which gives
lon = ((elon2-slon2)*(et1 - st1) - (elon1-slon1)*(et2-st2)) * t2 + k
lat = ((elat2-slat2)*(et1 - st1) - (elat1-slat1)*(et2-st2)) * t2 + j
where t2 = t * ((et1 - st1) * (et2 - st2))

So if of course it just goes one direction or another, well...
So what we want is to know in terms of t2 when it crosses this square. 

and then of course in terms of t.

So the t has four lines, and there are four slope lines. But only two of them
matter. And that changes the 'k'. 

WE MUST TEST THIS!!!

ok..ok..ok..

But anyway, we have 4 lines. If any of those lines crosses the square in a point
that is inside the square, between the start and end times we know we have a trouble.

So sounds like a big rickity structure.. Definitely.

Any wrong variable and we are messed. Such is the nature of this stuff. 

So what can we do? 

I think we need to build up from small. Like maybe we can do the slope stuff properly.
But we need to make a "Line" and ask, when does this "Line" cross another "Line"? at
what "t"? But if you think about it, it's really a 3d thing. Yea. But whatever.

So the only thing is that we will need to test this. So lets see. We have a lopsidedbox
So the trick here is this is serious stuff. Now we have a child and we havea  lopsidedbox.
And we have a spacetimebox but that is a different thing. 

So the lopsided box... hmmm.. The idea behind it is to add children so we can find the
extent of an actual row if, bla, bla, bla...

Ok, so we'll keep that.

So we'll just add a new method, overlaps() for lopsided box. We'll create a whole bunch
of tests. And that will be that.

so the point is we have a pretest, then a post test for overlapping. 

Now, are we certain that the method we use will not fail. I think it won't.

And there is the rub... because we don't want any overlap, not even a little to keep things
simple.

I was going somewhere with those equations, though. The idea is that we get the exact slope.
Ah, but we don't know about start and end. See, there is the trick.

Why am i so concerned with a tiny amount of overlap, though?

What is this goal suppose to achieve?

Hmm... suppose we do have a tiny amount of overlap. Is that going to hurt anything.
No... maybe I should take a rest. 

Hmm, I think the problem is rejecting a point because you think it overlaps, when it really
doesn't. Like what if we have a percentage... oh groan... 
Hmm... so the issue is that when we start and then go to an end, what happens.

The points are slammed against each other. So yes, overlap could be a problem.

And if a box half contains a point, what other choice do you have?

I'm totally confused.

Why we need to go down this route?

Ok, so what we can do is use a smaller square, of course! Allow a little overlap. Adjust each
line inward by one and that would solve this issue. 

And you're right, so the reason we are doing all this is because we don't act like every
person in the universe, no matter how much data we have of our own movements.

So this is what we do:

Yes, and that will automatically take care of the whole points slammed against each other. We
allow a tiny bit of overlap. Each box by one pixel, so that makes it 2. Is that enough?

Here is a question, what happens if you have a row, of data, and then all of the sudden a giant
long gps location crosses over it. Or crosses over several cache rows. What do you do? 
Because if it crosses multiple, you can't expand them all. So in that case, we'll need overlap,
we must have overlap. Wait, well, it depends. Doesn't it?

Hmm, I see... so it's not whether a point overlaps or not, really. Because sometimes you must have it.
It's more, how can we minimize this overlapping garbage.

Hmm...

But you see, the best response would be an elimination of points underneath it.

Darn, I feel this is hard.

Too hard.

We could try another way. A simple way of just caching graphs of data. And zooming in on those graphs.

What if we broke apart the point itself? So if it fits in both, we cut it in two? What's the difference?
That would give us a lot more options... Yes... I like it. The only issue is the way we are displaying
this stuff.. but we could monkey with that.

So how do we deal with this? Lets not worry about it for now..

ok.

So where are we?

If an item overlaps a cache row, we must add to it. 

So what do we do when we split up a point? 

I suppose we don't have to split it up as far as the parent is concerned.
Ah.

Ok, so the thing is that we dont want to make a fat right side. So here is the deal:

The precondition is that parent already handles the point. 
Here are the scenarios:

1. No children:
   expand the parent
2. One child can fit it in without changing:
    We must add to that child
3. Both children overlap point: 
   Split the node in the best possible way so the children will be closest to the same size
4. One child overlaps point, that child must expand, unless it grows too much... in that case,
   we need to split

Hmm, the problem is that we can't always avoid overlap with lopsided boxes.. I think that 
we need to go back to a looser system. 

So what we need to do is:

1. No children:
   expand the parent
2. One child can fit it in without changing:
    We must add to that child
3. Both children overlap point: 
   Split the node in the best possible way so the children will be closest to the same size
4. One child overlaps point, that child must expand
5. Neither child overlaps point:
   Add point to child which creates the best score,
   or expand parent if the original score is the best

Now I think that we take the cursory score

Ok, so lets say we go back to the way it was, with the slope and no worries about
overlap, etc. Then what we do is.

Like how do we decide when to split? Because spliting causes trouble. It can cause
trouble. Why? because we are increasing the number of points. Which means that if
we cover the entire area, we are looking up the same point twice... but if we overlap,
we are doing that anyway. Yea, but lets say we don't overlap, then, can we agree its
trouble? Yea... because suppose we broke a gps point into thousands of tiny little parts.
Then we just increased the cache rows to search through by thousands as well.

Yo, this is so hard. Perfection seems very difficult here. But we don't need perfection.
We just need to get rid of difficult problems. So the point is that we have a three way
overlap issue. c1, c2, and nc (possibly if we expand the parent)

So, we can even cut the piece, and give part of it to c1, and then expand the parent for
the rest.

Because, when we expand the parent, what we are doing is reducing the size of the children.
Right? It makes me wonder... why do we ever add to c1 or c2 when we don't have to?


Oh yea, I remember, we had rollback and commit stuff. Huh.

So I kind of get it. But it's a tricky thing. Because of the way the children are compared
to the parent. Could we force the children to be a certain size? Hahahaha. Yea right.
We have to deal with stuff like overlap at the below level. It's complicated.

Because the problem is that I don't know where to cut at the parent level.

What if we let the child decide? So we say, look, you need to stay within these bounds.
Where do we need to cut the row to do this. That's hard.

Remember that we only have to handle problems. Not make it perfect.

I wonder if I'm going the wrong direction. It almost feels that way, huh?

Because we are going to have to split up the pieces based on the tiniest cache nodes.

This won't help if we want to expand a child to a new node that it doesn't reach.

The other thing is it leaves a ton of parts of the point that aren't hooked up in the
tiniest of nodes. So after going down, we go up and then down again to place the 
new stuff.

So, I think the point is that we start at the bottom, and we choose to create the 
best possible situation? Hmm.. again... a pain.

So what if we just add the point to the best guy, and not consider splitting unless
it's a last resort? I think that should be ok. If not, we can deal with it. Maybe
even create dragon detection...

Alright..

So here's the deal. We go down, find all the points that are covered.

So how does this work with time jumps?

Because when we go down, we split based on whether we can add a point. Can we 
do the same thing, but at this initial stage instead? And can it be woven
into going down? 

Hmm... I see, maybe it is the same.. yea, I think so. Cause look at what you are
saying in a downward direction.

Yea, its the same... cool. We'll just do what we said. And we won't worry about 
super overlap... Grrr. I don't know. It means there will be overlap. Darn, goshdarnit.
I don't want it.

The problem is that even when we know there is overlap, we can never be sure of how to
fix it. Unless we work through all that math, which again is impossibly hard.

If only there was another metric. Because if 1000 points overlap the same place and
that place is ....

hmm.. I think I worry too much about overlap. Because it wasn't really the overlap that
caused the problem, it was the dragon.

Darn... I should just build it. If it doesn't work, we'll figure it out. That's the only
thing I can say.

The alternative is to get rid of all overlap. I mean why not? It's simple, look, we know
with a lopsided box that either the top or bottom will be the only thing in the child that
will bulge out of the parent. So we simply squish it. And then we figure out the point that
it could handle squished, and chop it.

Not so hard, really. 

So what we do is:

1. addPoint()
    addPoint will determine which children overlap, and make a decision in regards to which
    child should handle the point, ("the complete") point.
2. The child will get addPoint() called on it with a new class ExpandStatus, which tells the child the 
    maximum it can expand in each direction.
3. The child will return the amount of the point handled, and update ExpandStatus, which will
   notify the parent how much the child did expand (as well as update ExpandStatus for
   the other children)
4. If there is still part of the point to be covered, the parent will decide what to do with
it (either add it to the second child, or create a new child for the point) and this will
   repeat until the options the parent has for handling the point are exhausted.
5. It then will return to its parent how much of the point was handled, and how it did expand.

8/28

there is an issue. again. and again. and failure is coming closer than ever. Should i give
up?

The issue is that suppose we have 3 points. We can't always box it in a way so that two are
contained by a super point and that super point and the other point is contained by
the parent.

I suppose we could split the point so we always could

I feel like I'm missing something. Like there is an easier way to look at this issue.

If I start from the floor and find the nearest point... no that's not going to help.

Ok, we have to, I think, handle each specific way of dealing with the point, and seeing
which way improves the data the most. Yea, that's really the only way of doing it. However,
that means we have to go all the way down, so we need the cursory check.

So we perform a cursory check to see what happens to child in a cursory fashion for the
scenarios

1. add point to child1
2. add point to child2
3. expand parent and combine child1 and child2

Then we choose simply based on the best score for these choices as well as whether
they can handle the entire point.

8/29

Ok, so we realize that we need to have overlap.

This is because, consider the following new information

It is no longer a lopsided box, but two cubes connected together by a tunnel.
This is because the timejump represents the time height. If a point arrives
within that timejump, it is expected to be part of that item.
Now also, you must realize, that the items themselves really have no shape
in the classical sense. This is because suppose we have a point that extends
at the same slope and angle as an already existing cache piece. It would then
need to grow higher and higher... There is something weird about that.

So we need to decide and define whether a point should turn into a ghost point
when presented with two caches that it could fit into. Becuase, if you state that
the cache must be complete. In other words, if a point should fit into it, then it
must, then we necessarily must have ghost points.

See it depends. The items certainly are lopsided boxes in terms of the final result.
It's just when adding them do they become these other figures. So there are multiple
types of overlap, overlap for searching, and overlap for adding points. Because 
of course we *could* add a point to a cache doesn't mean we have to. But what does
time jump mean if we ignore it? 

I don't think it is a big deal.

but one thing that bothers me is that for a large time jump, it's next to useless
to separate these guys by time. So just because an item would grow bigger than its
sibling, if we are separating it and adding it to another node because of a time
separation, then we shouldn't really do it.

8/30

We have all these points. But think about it this way, we have these areas.
And the more we have to lookup for each area the worse. 

So what is the value of the lopsided box? I mean suppose everything is a box.
Except that then a child cannot coexist if it's just a long skinny line.

One other thing is this overlap problem won't exist at the low time levels.
At the low time levels, everything becomes quite easy. Because all the boxes
can be split up by time. 

So it's only at the high time levels this becomes a problem.

Now, the low and high time levels coexist to a point. 

At the high levels there is the most potential for a problem.

And since we're dealing with segments, two cache rows could rightfully each hold
a part. But that doesn't matter. Because, think. Suppose that we have, for a certain
data range, ... no won't work, first child can grow and the nmust acquire new points.

Look, lets think of this from a 2d, no time perspective.

In this case, we have lines and squares. Thick fat lines. 

Once we create divisions at the high level, we would be advised
to stay within those bounds. Ok,
it's just that we can add a long point, but only add the part that's in our bounds.
Fine.

I wonder if we could use a grid based system instead. Yet that would be a lot of 
wasted space. True.

I wonder if we could have a background node. No, because foreground nodes can expand,
unless we transfer ownership. It's a problem of kingdoms.

What if we just use a single line to draw up ownership.

There would be no more growing then. Children would never grow. They would just be
split. We could use a plane rather than a line for 3d space if necessary.
So we'd start with the world. Add a point, and split with a randomish line for that
point.

Then, when we add another point, it would go to whatever child based on that line.
The child would split in two.

8/30

Area != Shape.

The idea here is the area of coverage doesn't conform to the shape of the cache.

I suppose that is ok. Then we could just use space time boxes for the size, and
shape it anyway we want... based on the shape of the children. And cutting would be 
simple. Hmm.. But wha't sth

A different idea is needed.

This crossing of segments. It needs to be represented somehow. It seems so awkward.

At the deepest level, it's not awkward and not a problem. The segments are skinny and
pointed. The overlap is tiny. But as you blur and expand out, you get bigger
and bigger children. Also the two child per item restriction increases this issue
by a lot. 

8/31/10

Ok, so we figure that we can do this based on increase in area.

So the idea is as follows. We choose what we cursorily think is the best path.
Then we sloppily go through the rest, the idea is that we can summarily dismiss
paths that are at least as big as the best.

I wonder if we can get away with just one level of area checking.

Because otherwise, we are asking the guy to score itself. In a cursory check,
none the less.. very similar to... the time I was in nanning. So when did we leave nanning?

Ok, so we can get the volume change of the two children. But what of the parent?
Because we are joining child1 and child2 together. And we are doing kind of 
a cursory search

E/GpsTrailer( 2315): Bad child data!!! 

GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25598252,minLatEnd=1894252270,minLonStart=-109453742,minLonEnd=-16863242,slopeLatm=-1734.062351,slopeLonm=-6.018739,id=1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=-1,c2r=-1073741825,r=0,lonmwidth=92590500,latmheight=1919850522,endMinLat=-2147483648,endMaxLat=-227633126,endMinLon=-116818572,endMaxLon=-24228072) child: 
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=-25598252,minLatEnd=-25598252,minLonStart=-54570449,minLonEnd=-54570449,slopeLatm=-1682.024938,slopeLonm=-1606.494700,id=-1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0,endMinLat=-876393378,endMaxLat=-876393378,endMinLon=-867161172,endMaxLon=-867161172) Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMinLonm()) >= minLonmAtChildEnd + getWidthLonm() 

rowcopy:
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=-25598252,minLatEnd=-25598252,minLonStart=-54570449,minLonEnd=-54570449,slopeLatm=-1682.024938,slopeLonm=-1606.494700,id=1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0,endMinLat=-876393378,endMaxLat=-876393378,endMinLon=-867161172,endMaxLon=-867161172)

newparent:
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=-25598252,minLatEnd=-25598252,minLonStart=-54570449,minLonEnd=-54570449,slopeLatm=-1682.024938,slopeLonm=-1606.494700,id=-1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0,endMinLat=-876393378,endMaxLat=-876393378,endMinLon=-867161172,endMaxLon=-867161172)

newnode:
GpsLocCacheRowData(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25598139,minLatEnd=-25598139,minLonStart=-54570674,minLonEnd=-54570674,slopeLatm=-1185.227664,slopeLonm=-1132.001942,id=-1073741825,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0,endMinLat=-876394854,endMaxLat=-876394854,endMinLon=-867160156,endMaxLon=-867160156)





E/GpsTrailer( 2315): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25598252,minLatEnd=1894252270,minLonStart=-109453742,minLonEnd=-16863242,slopeLatm=-1734.062351,slopeLonm=-6.018739,id=1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=-1,c2r=-1073741825,r=0,lonmwidth=92590500,latmheight=1919850522,endMinLat=-2147483648,endMaxLat=-227633126,endMinLon=-116818572,endMaxLon=-24228072) child: GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=-25598252,minLatEnd=-25598252,minLonStart=-54570449,minLonEnd=-54570449,slopeLatm=-1682.024938,slopeLonm=-1606.494700,id=-1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0,endMinLat=-876393378,endMaxLat=-876393378,endMinLon=-867161172,endMaxLon=-867161172) Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMaxLonm()) >= minLonmAtChildEnd + getWidthLonm() 




E/GpsTrailer( 2315): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25598252,minLatEnd=1894252270,minLonStart=-109453742,minLonEnd=-16863242,slopeLatm=-1734.062351,slopeLonm=-6.018739,id=1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=-1,c2r=-1073741825,r=0,lonmwidth=92590500,latmheight=1919850522,endMinLat=-2147483648,endMaxLat=-227633126,endMinLon=-116818572,endMaxLon=-24228072) child: GpsLocCacheRowData(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25598139,minLatEnd=-25598139,minLonStart=-54570674,minLonEnd=-54570674,slopeLatm=-1185.227664,slopeLonm=-1132.001942,id=-1073741825,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0,endMinLat=-876394854,endMaxLat=-876394854,endMinLon=-867160156,endMaxLon=-867160156) Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMinLonm()) >= minLonmAtChildEnd + getWidthLonm() 
E/GpsTrailer( 2315): Bad child data!!! GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25598252,minLatEnd=1894252270,minLonStart=-109453742,minLonEnd=-16863242,slopeLatm=-1734.062351,slopeLonm=-6.018739,id=1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=-1,c2r=-1073741825,r=0,lonmwidth=92590500,latmheight=1919850522,endMinLat=-2147483648,endMaxLat=-227633126,endMinLon=-116818572,endMaxLon=-24228072) child: GpsLocCacheRowData(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25598139,minLatEnd=-25598139,minLonStart=-54570674,minLonEnd=-54570674,slopeLatm=-1185.227664,slopeLonm=-1132.001942,id=-1073741825,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0,endMinLat=-876394854,endMaxLat=-876394854,endMinLon=-867160156,endMaxLon=-867160156) Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMaxLonm()) >= minLonmAtChildEnd + getWidthLonm() 








E/GpsTrailer( 2509): Bad child data!!! 


GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25599487,minLatEnd=-25598250,
minLonStart=-54570449,minLonEnd=-54568792,slopeLatm=0.002663,slopeLonm=23.672137,id=1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=-1,c2r=-1073741825,r=0,lonmwidth=1657,latmheight=1237,endMinLat=-25596228,endMaxLat=-25594991,
endMinLon=-25604038,endMaxLon=-25602381) child: 

GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=-25598252,minLatEnd=-25598252,minLonStart=-54570449,minLonEnd=-54570449,slopeLatm=0.000223,slopeLonm=-0.000445,id=-1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,ws.start=0,ws.end=7776000000,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0,endMinLat=-25598139,endMaxLat=-25598139,
endMinLon=-54570674,endMaxLon=-54570674) 


Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMinLonm()) >= minLonmAtChildEnd + getWidthLonm() 



GpsLocCacheRowData(startTimeMs=2010-04-21 04:16:09.941,endTimeMs=2010-04-21 04:16:17.528,minLatStart=-26026040,minLatEnd=-26026006,minLonStart=-54586333,minLonEnd=-54586321,slopeLatm=-0.179913,slopeLonm=-0.012258,id=367,c1=384,c2=385,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=194670,ws.start=0,ws.end=3375,c1r=0,c2r=385,r=0,lonmwidth=12,latmheight=34,endMinLat=-26027405,endMaxLat=-26027371,endMinLon=-54586426,endMaxLon=-54586414) doesn't contain

GpsLocCacheRow(startTimeMs=2010-04-21 04:16:09.941,endTimeMs=2010-04-21 04:16:12.124,minLatStart=-26026011,minLatEnd=-26026011,minLonStart=-54586333,minLonEnd=-54586333,slopeLatm=-0.186441,slopeLonm=-0.009620,id=384,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=194670,widthLonm=0,heightLatm=0,endMinLat=-26026418,endMaxLat=-26026418,endMinLon=-54586354,endMaxLon=-54586354)












-- next bug

D/GpsTrailer( 3223): Gps loc 787 added 786
D/GpsTrailer( 3223): Processing gps loc row GpsLocationRow(timeMs=2010-05-10 21:47:52.854,latm=            41876761,lonm=            12480453,id=788)
E/GpsTrailer( 3223): Bad child data!!! 


GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 21:47:52.854,minLatStart=-178832571,minLatEnd=6778037,minLonStart=179652106,minLonEnd=335746781,slopeLatm=0.070946,slopeLonm=-0.138736,id=1495,c1=31189,c2=31190,tj=39,numsegments=1,startTimeJump=438008,endTimeJump=657012,ws.start=438008,ws.end=657012,c1r=-1495,c2r=-1073743319,r=0,lonmwidth=156094675,latmheight=185610608,endMinLat=-52089634,endMaxLat=133520974,endMinLon=-68195744,endMaxLon=87898931) child: 
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 21:42:51.143,minLatStart=-177661150,minLatEnd=6778035,minLonStart=-178589212,minLonEnd=-24253221,slopeLatm=0.070290,slopeLonm=0.061794,id=-1495,c1=31189,c2=31190,tj=-2147483648,numsegments=1,startTimeJump=438008,endTimeJump=657012,ws.start=438008,ws.end=657012,c1r=31189,c2r=31190,r=0,lonmwidth=154335991,latmheight=184439185,endMinLat=-52111038,endMaxLat=132328147,endMinLon=-68214684,endMaxLon=86121307) 

Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMinLonm()) >= minLonmAtChildEnd + getWidthLonm() 

D/AndroidRuntime( 3223): Shutting down VM
W/dalvikvm( 3223): threadid=1: thread exiting with uncaught exception (group=0x4001d7e0)
E/AndroidRuntime( 3223): FATAL EXCEPTION: main
E/AndroidRuntime( 3223): junit.framework.AssertionFailedError
E/AndroidRuntime( 3223):        at junit.framework.Assert.fail(Assert.java:47)
E/AndroidRuntime( 3223):        at junit.framework.Assert.fail(Assert.java:53)
E/AndroidRuntime( 3223):        at com.rareventure.gps2.database.GpsLocCacheRow.checkDataHack(GpsLocCacheRow.java:224)
E/AndroidRuntime( 3223):        at com.rareventure.gps2.database.cachecreator.GpsLocCacheRowData.updateLstb(GpsLocCacheRowData.java:137)
E/AndroidRuntime( 3223):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPoint(AddPointStatus.java:784)
E/AndroidRuntime( 3223):        at com.rareventure.gps2.database.cachecreator.AddPointStatus.addPointToTop(AddPointStatus.java:85)

newParent
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 21:42:51.143,minLatStart=-177661150,minLatEnd=6778035,minLonStart=-178589212,minLonEnd=-24253221,slopeLatm=0.070290,slopeLonm=0.061794,id=-1495,c1=31189,c2=31190,tj=-2147483648,numsegments=1,startTimeJump=438008,endTimeJump=657012,ws.start=438008,ws.end=657012,c1r=31189,c2r=31190,r=0,lonmwidth=154335991,latmheight=184439185,endMinLat=-52111038,endMaxLat=132328147,endMinLon=-68214684,endMaxLon=86121307)

newNode
GpsLocCacheRowData(startTimeMs=2010-05-10 21:42:51.143,endTimeMs=2010-05-10 21:47:52.854,minLatStart=41875607,minLatEnd=41875607,minLonStart=12478929,minLonEnd=12478929,slopeLatm=0.003825,slopeLonm=0.005051,id=-1073743319,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=438008,endTimeJump=657012,ws.start=438008,ws.end=657012,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0,endMinLat=41876761,endMaxLat=41876761,endMinLon=12480453,endMaxLon=12480453)

GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-05-10 21:28:41.325,minLatStart=-173185188,minLatEnd=6778031,minLonStart=-173070455,minLonEnd=-24253225,slopeLatm=0.067784,slopeLonm=0.058703,id=-1495,c1=31093,c2=31094,tj=-2147483648,numsegments=1,startTimeJump=438008,endTimeJump=657012,ws.start=438008,ws.end=657012,c1r=31093,c2r=31094,r=0,lonmwidth=148817230,latmheight=179963219,endMinLat=-52170224,endMaxLat=127792995,endMinLon=-68266470,endMaxLon=80550760)

GpsLocCacheRowData(startTimeMs=2010-05-10 21:28:41.325,endTimeMs=2010-05-10 21:37:50.440,minLatStart=41875994,minLatEnd=41875994,minLonStart=12481375,minLonEnd=12481375,slopeLatm=0.000537,slopeLonm=-0.003418,id=-1073743319,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=438008,endTimeJump=657012,ws.start=438008,ws.end=657012,c1r=0,c2r=0,r=0,lonmwidth=0,latmheight=0,endMinLat=41876289,endMaxLat=41876289,endMinLon=12479498,endMaxLon=12479498)



child end time 1273499272854
child end min lon  (int) 12480453
lonmperms 0.0627787634730339
start time 1271712806470

12480453-(1273499272854-1271712806470)*0.0627787634730339

-99671697.5736622


child1 ebd min lonm -68214684
child1 end time 1273498971143


-68214684-(1273498971143-1271712806470)*0.0627787634730339


-180347893.530156
179652106.469844

9/2

E/AndroidRuntime( 1812): junit.framework.AssertionFailedError: curr row is [GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 03:51:03.223,minLatStart=-25738928,minLatEnd=-25471602,minLonStart=-54607959,minLonEnd=-54531585,slopeLatm=-0.001026,slopeLonm=0.000429,id=1,c1=69,c2=27,tj=72,numsegments=1,startTimeJump=85244912,endTimeJump=7776000000,ws.start=85244912,ws.end=7776000000,c1r=69,c2r=0,r=0,lonmwidth=76374,latmheight=267326,endMinLat=-25821234,endMaxLat=-25553908,endMinLon=-54573541,endMaxLon=-54497167) -25738928 -54607959 1271712806470


So somehow 72 got added, as a time sibling of 1 but in a dead way.

We are on gps point 4 where eveything loks good
 on 5

We are on 9

id is 15, things are lopsided

9/3/10

Ok, now, this idea of taking something out of one tree and placing it in another..
It might help.

But what really irks me is the idea of balance in a vertical fashion. In other words,
depth. So what we do is as follows.

We look at depth to determine score. The child count, in other words.

The thing is that we were ok with overlap before... remember? We thought that 
if a particular item overlaps well... that we were ok. you know

But the thing is that we can't determine where to add a point until we get down
to the depth. So it's not really the area for this first child, but the area
for the entire depth that matters. 

Yet, we don't want overlap if we don't need it. But then again, if a point
is completely covered. the net area change is zero. 

So we need to have a rollback and commit.

What we need to do is first do a cursory examination to find the best child choice..
Now wait, when do we add as a parent. Ok... ok.. so that's easy to find out. So
here we go.

We find out what the value is if we add a parent. Then we find the best child.
Now this is the minimum. So what we need is a "i give up" value.

So we add the parent. Then we find the cursory results for the children. Then
for the best child, we start crunching. We find the solution for the best child.
Then we look at the other child. If it's minimum is higher than the best solution
for the best child, we give up right there. Otherwise we compute it and find out 
which one is better, using the "i give up" value of either the parent or the best
child, which ever is less.

Now, what is the value of the parent?

The value of the parent would be the size of the row with the current row and the
point added.. that is the value of the parent.

The value of the children would be, the change in size of each child, and 
all it's descendents down the line, and finally the change in size of the row
itself.

9/4/10

Ok, ok... I'm thinking we should combine these two things.

9/6

com.rareventure.gps2.database.SlopeChooser$Point@e9841092
-25598252
1271712806470
com.rareventure.gps2.database.SlopeChooser$Point@e66950e3
-25595189
1271714030120

Here is the current best

com.rareventure.gps2.database.SlopeChooser$Point@e9841092
-25598252
1271712806470
com.rareventure.gps2.database.SlopeChooser$Point@e66950e3
-25595189
1271714030120

3 top
10,1,1,1


GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:20:49.126,minLatStart=-25599406,minLatEnd=-25593646,minLonStart=-54574129,minLonEnd=-54568902,slopeLatm=-0.000429,slopeLonm=-0.001650,id=10,c1=11,c2=51,tj=54,numsegments=1,startTimeJump=312776,endTimeJump=717834,ws.start=312776,ws.end=313754,c1r=-10,c2r=-1073741834,r=0,lonmwidth=5227,latmheight=5760,endMinLat=-25600626,endMaxLat=-25594866,endMinLon=-54578819,endMaxLon=-54573592)

 child: GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:20:48.112,minLatStart=-25599406,minLatEnd=-25593646,minLonStart=-54574129,minLonEnd=-54568902,slopeLatm=-0.000429,slopeLonm=-0.001650,id=-10,c1=11,c2=51,tj=-2147483648,numsegments=1,startTimeJump=312776,endTimeJump=717834,ws.start=312776,ws.end=313754,c1r=11,c2r=51,r=0,lonmwidth=5227,latmheight=5760,endMinLat=-25600625,endMaxLat=-25594865,endMinLon=-54578817,endMaxLon=-54573590)

Util.makeContinuousFromStartLonm(minLonmAtChildEnd, child.getEndMaxLonm()) >= minLonmAtChildEnd + getWidthLonm() 
-54578818   (int) -54573590 >  (int) -54578818 + 5227 = -54573591


9/7
child that is missng:
GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-21 03:43:32.690,minLatStart=-25598139,minLatEnd=-25593516,minLonStart=-54579869,minLonEnd=-54570672,slopeLatm=-0.000950,slopeLonm=0.000887,id=828,c1=822,c2=823,tj=-2147483648,numsegments=1,startTimeJump=77245899,endTimeJump=78559470,widthLonm=9197,heightLatm=4623,endMinLat=-25673493,endMaxLat=-25668870,endMinLon=-54509550,endMaxLon=-54500353) 

E/CacheViewer( 2658): 1,740,849,853,1,77255232,7776000000,2010-04-20 05:33:26.470,2010-04-21 03:43:32.690,-25598252,-25525642,-54638682,-54570443,-0.000943,0.000878
E/CacheViewer( 2658): 849,847,20,828,1,78559470,7776000000,2010-04-20 05:41:52.286,2010-04-21 03:43:32.690,-25598139,-25594504,-54577618,-54570672,-0.000950,0.000887

77245899
78559470

E/CacheViewer( 2658): 1,740,849,853,1,          77255232,7776000000,2010-04-20 05:33:26.470,2010-04-21 03:43:32.690,-25598252,-25525642,-54638682,-54570443,-0.000943,0.000878
E/CacheViewer( 2658): 849,847,20,828,1,         78559470,7776000000,2010-04-20 05:41:52.286,2010-04-21 03:43:32.690,-25598139,-25594504,-54577618,-54570672,-0.000950,0.000887
E/CacheViewer( 2658): 828,822,823,-2147483648,1,77245899,78559470,2010-04-20 05:41:52.286,2010-04-21 03:43:32.690,-25598139,-25593516,-54579869,-54570672,-0.000950,0.000887


gps loc 16, 849 and 828 gets created

I believe this is where 849 came from:

D/HACKVTU ( 3057): 21~0,-21~0,-1073741845~0,(-2147483648),(-2147483648),77245899,77255232,21,4,20,-2147483648,1,2022072,7776000000,2010-04-20 05:41:52.286,2010-04-21 03:43:32.690,-25598139,-25593516,-54579869,-54570672,-0.000950,0.000887
D/HACKVTU ( 3057): 21~1,-21~1,-1073741845~1,(-2147483648),(-2147483648),77255232,78559470,21,4,20,-2147483648,1,2022072,7776000000,2010-04-20 05:41:52.286,2010-04-21 03:43:32.690,-25598139,-25593516,-54579869,-54570672,-0.000950,0.000887
--->>>D/HACKVTU ( 3057): 21~2,4~0,20,(-2147483648),(-2147483648),78559470,7776000000,21,4,20,-2147483648,1,2022072,7776000000,2010-04-20 05:41:52.286,2010-04-21 03:43:32.690,-25598139,-25594504,-54577618,-54570672,-0.000950,0.000887
D/HACKVTU ( 3057): 21~3,4,20,(-2147483648),(-2147483648),2022072,77245899,21,4,20,-2147483648,1,2022072,7776000000,2010-04-20 05:41:52.286,2010-04-20 06:15:43.691,-25598139,-25595043,-54572945,-54570673,-0.000199,-0.002521


------



D/GpsTrailer( 8377): Gps loc 3 added 2
E/CacheViewer( 8377): id,child1,child2,tjs,numsegments,startTimeJump,endTimeJump,startTimeMs,endTimeMs,minLatStart,minLatEnd,minLonStart,minLonEnd,slopeLatm,slopeLonm
E/CacheViewer( 8377): 1,2,3,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:53:50.120,-25599406,-25598251,-54570449,-54568901,0.002503,-0.003503
E/CacheViewer( 8377): 2,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:41:52.286,-25598252,-25598252,-54570449,-54570449,0.000223,-0.000445
E/CacheViewer( 8377): 3,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:41:52.286,2010-04-20 05:53:50.120,-25598139,-25598139,-54570674,-54570674,0.004110,-0.005657
D/GpsTrailer( 8377): Processing gps loc row GpsLocationRow(timeMs=2010-04-20 06:15:34.358,latm=           -25598520,lonm=           -54575808,id=4)
D/dalvikvm(  611): GC_EXPLICIT freed 4313 objects / 423640 bytes in 80ms
D/HACKVTU ( 8377): start
D/HACKVTU ( 8377): id,c1,c2,rep,tjs,wss,wse,id,child1,child2,tjs,numsegments,startTimeJump,endTimeJump,startTimeMs,endTimeMs,minLatStart,minLatEnd,minLonStart,minLonEnd,slopeLatm,slopeLonm
D/dalvikvm( 8377): GC_FOR_MALLOC freed 9254 objects / 398552 bytes in 34ms
D/HACKVTU ( 8377): 1~0,-1~0,-1073741825~0,(-2147483648),(-2147483648),0,717834,1,2,3,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 06:15:34.358,-25599406,-25595615,-54571392,-54568900,0.000350,-0.002732
D/HACKVTU ( 8377): 1~1,2,3~0,(-2147483648),(-2147483648),717834,7776000000,1,2,3,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 06:15:34.358,-25598252,-25594997,-54571629,-54569387,-0.000106,-0.002539
D/HACKVTU ( 8377): -1~0,2,3,(-2147483648),(-2147483648),0,717834,-1,2,3,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:53:50.120,-25599406,-25598251,-54570449,-54568901,0.002503,-0.003503
D/HACKVTU ( 8377): 3~0,-3~0,-1073741827~0,(-2147483648),(-2147483648),717834,7776000000,3,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:41:52.286,2010-04-20 06:15:34.358,-25598139,-25595052,-54572913,-54570673,-0.000188,-0.002539
D/HACKVTU ( 8377): 3~1,(-2147483648),(-2147483648),(-2147483648),(-2147483648),0,717834,3,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:41:52.286,2010-04-20 05:53:50.120,-25598139,-25598139,-54570674,-54570674,0.004110,-0.005657
D/HACKVTU ( 8377): -3~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),717834,7776000000,-3,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:41:52.286,2010-04-20 05:53:50.120,-25598139,-25598139,-54570674,-54570674,0.004110,-0.005657
D/HACKVTU ( 8377): -1073741827~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),717834,7776000000,-1073741827,-2147483648,-2147483648,-2147483648,1,717834,7776000000,2010-04-20 05:53:50.120,2010-04-20 06:15:34.358,-25595189,-25595189,-54574735,-54574735,-0.002554,-0.000823
D/HACKVTU ( 8377): -1073741825~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),0,717834,-1073741825,-2147483648,-2147483648,-2147483648,1,0,717834,2010-04-20 05:53:50.120,2010-04-20 06:15:34.358,-25595189,-25595189,-54574735,-54574735,-0.002554,-0.000823



D/AddPointStatus( 8377): inserting id -1~0 to r.id, 2
D/AddPointStatus( 8377): inserting id -1073741825~0 to r.id, 3
D/AddPointStatus( 8377): inserting id -3~0 to r.id, 4
D/AddPointStatus( 8377): inserting id -1073741827~0 to r.id, 5
D/AddPointStatus( 8377): inserting id -1~0 to r.id, 6
D/AddPointStatus( 8377): inserting id -1~0 to r.id, 7
D/AddPointStatus( 8377): inserting id -1~0 to r.id, 8
D/AddPointStatus( 8377): inserting id -1073741825~0 to r.id, 9
D/AddPointStatus( 8377): inserting id 1~1 to r.id, 10

-----------

D/GpsTrailer( 8563): Gps loc 3 added 2
E/CacheViewer( 8563): id,child1,child2,tjs,numsegments,startTimeJump,endTimeJump,startTimeMs,endTimeMs,minLatStart,minLatEnd,minLonStart,minLonEnd,slopeLatm,slopeLonm
E/CacheViewer( 8563): 1,2,3,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:53:50.120,-25599406,-25598251,-54570449,-54568901,0.002503,-0.003503
E/CacheViewer( 8563): 2,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:41:52.286,-25598252,-25598252,-54570449,-54570449,0.000223,-0.000445
E/CacheViewer( 8563): 3,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:41:52.286,2010-04-20 05:53:50.120,-25598139,-25598139,-54570674,-54570674,0.004110,-0.005657
D/GpsTrailer( 8563): Processing gps loc row GpsLocationRow(timeMs=2010-04-20 06:15:34.358,latm=           -25598520,lonm=           -54575808,id=4)
I/ActivityManager(  111): Start proc com.google.android.apps.maps:BackgroundFriendService for service com.google.android.apps.maps/com.google.googlenav.friend.android.BackgroundFriendService: pid=8574 uid=10014 gids={3003, 1015}
D/dalvikvm( 8574): GC_FOR_MALLOC freed 2417 objects / 168152 bytes in 34ms
D/HACKVTU ( 8563): start
D/HACKVTU ( 8563): id,c1,c2,rep,tjs,wss,wse,id,child1,child2,tjs,numsegments,startTimeJump,endTimeJump,startTimeMs,endTimeMs,minLatStart,minLatEnd,minLonStart,minLonEnd,slopeLatm,slopeLonm
D/dalvikvm( 8563): GC_FOR_MALLOC freed 9459 objects / 405800 bytes in 40ms
D/HACKVTU ( 8563): 1~0,2,3~0,(-2147483648),(-2147483648),717834,7776000000,1,2,3,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 06:15:34.358,-25598252,-25594997,-54571629,-54569387,-0.000106,-0.002539
D/HACKVTU ( 8563): 1~1,-1~0,-1073741825~0,(-2147483648),(-2147483648),0,717834,1,2,3,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 06:15:34.358,-25599406,-25595615,-54571392,-54568900,0.000350,-0.002732
D/HACKVTU ( 8563): 3~0,-3~0,-1073741827~0,(-2147483648),(-2147483648),717834,7776000000,3,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:41:52.286,2010-04-20 06:15:34.358,-25598139,-25595052,-54572913,-54570673,-0.000188,-0.002539
D/HACKVTU ( 8563): 3~1,(-2147483648),(-2147483648),(-2147483648),(-2147483648),0,717834,3,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:41:52.286,2010-04-20 05:53:50.120,-25598139,-25598139,-54570674,-54570674,0.004110,-0.005657
D/HACKVTU ( 8563): -3~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),717834,7776000000,-3,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:41:52.286,2010-04-20 05:53:50.120,-25598139,-25598139,-54570674,-54570674,0.004110,-0.005657
D/HACKVTU ( 8563): -1073741827~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),717834,7776000000,-1073741827,-2147483648,-2147483648,-2147483648,1,717834,7776000000,2010-04-20 05:53:50.120,2010-04-20 06:15:34.358,-25595189,-25595189,-54574735,-54574735,-0.002554,-0.000823
D/HACKVTU ( 8563): -1~0,2,3,(-2147483648),(-2147483648),0,717834,-1,2,3,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:53:50.120,-25599406,-25598251,-54570449,-54568901,0.002503,-0.003503
D/HACKVTU ( 8563): -1073741825~0,(-2147483648),(-2147483648),(-2147483648),(-2147483648),0,717834,-1073741825,-2147483648,-2147483648,-2147483648,1,0,717834,2010-04-20 05:53:50.120,2010-04-20 06:15:34.358,-25595189,-25595189,-54574735,-54574735,-0.002554,-0.000823
D/AddPointStatus( 8563): inserting id -3~0 to r.id, 4
D/dalvikvm( 8574): GC_FOR_MALLOC freed 3991 objects / 255432 bytes in 62ms
D/AddPointStatus( 8563): inserting id -1073741827~0 to r.id, 5
D/AddPointStatus( 8563): updating 3~1
D/AddPointStatus( 8563): inserting id -1~0 to r.id, 6
D/AddPointStatus( 8563): updating 3~1
D/AddPointStatus( 8563): inserting id -1~0 to r.id, 7
D/AddPointStatus( 8563): inserting id -1~0 to r.id, 8
D/AddPointStatus( 8563): inserting id -1073741825~0 to r.id, 9
D/dalvikvm( 8215): GC_EXPLICIT freed 879 objects / 61648 bytes in 93ms
D/AddPointStatus( 8563): inserting id 1~1 to r.id, 10
D/AddPointStatus( 8563): updating 1~0
D/GpsTrailer( 8563): misses: 2, hits: 18
D/GpsTrailer( 8563): Gps loc 4 added 3
E/CacheViewer( 8563): id,child1,child2,tjs,numsegments,startTimeJump,endTimeJump,startTimeMs,endTimeMs,minLatStart,minLatEnd,minLonStart,minLonEnd,slopeLatm,slopeLonm
E/CacheViewer( 8563): 1,2,7,10,1,717834,7776000000,2010-04-20 05:33:26.470,2010-04-20 06:15:34.358,-25598252,-25594997,-54571629,-54569387,-0.000106,-0.002539
E/CacheViewer( 8563): 10,8,9,-2147483648,1,0,717834,2010-04-20 05:33:26.470,2010-04-20 06:15:34.358,-25599406,-25595615,-54571392,-54568900,0.000350,-0.002732
E/CacheViewer( 8563): 8,2,3,-2147483648,1,0,717834,2010-04-20 05:33:26.470,2010-04-20 05:53:50.120,-25599406,-25598251,-54570449,-54568901,0.002503,-0.003503
E/CacheViewer( 8563): 9,-2147483648,-2147483648,-2147483648,1,0,717834,2010-04-20 05:53:50.120,2010-04-20 06:15:34.358,-25595189,-25595189,-54574735,-54574735,-0.002554,-0.000823
E/CacheViewer( 8563): 2,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:41:52.286,-25598252,-25598252,-54570449,-54570449,0.000223,-0.000445
E/CacheViewer( 8563): 3,-2147483648,-2147483648,-2147483648,1,0,717834,2010-04-20 05:41:52.286,2010-04-20 05:53:50.120,-25598139,-25598139,-54570674,-54570674,0.004110,-0.005657
E/CacheViewer( 8563): 2,-2147483648,-2147483648,-2147483648,1,0,7776000000,2010-04-20 05:33:26.470,2010-04-20 05:41:52.286,-25598252,-25598252,-54570449,-54570449,0.000223,-0.000445
E/CacheViewer( 8563): 7,4,5,-2147483648,1,717834,7776000000,2010-04-20 05:41:52.286,2010-04-20 06:15:34.358,-25598139,-25595052,-54572913,-54570673,-0.000188,-0.002539
E/CacheViewer( 8563): 4,-2147483648,-2147483648,-2147483648,1,717834,7776000000,2010-04-20 05:41:52.286,2010-04-20 05:53:50.120,-25598139,-25598139,-54570674,-54570674,0.004110,-0.005657
E/CacheViewer( 8563): 5,-2147483648,-2147483648,-2147483648,1,717834,7776000000,2010-04-20 05:53:50.120,2010-04-20 06:15:34.358,-25595189,-25595189,-54574735,-54574735,-0.002554,-0.000823
D/GpsTrailer( 8563): Processing gps loc row GpsLocationRow(timeMs=2010-04-20 06:15:43.691,latm=           -25598525,lonm=           -54575797,id=5)
D/dalvikvm( 8223): GC_EXPLICIT freed 1112 objects / 77176 bytes in 76ms

9/8

Ok, so the problem is that row 1 and -1 both point to 3. 3 has a state0 in array index 1. So array index 0 becomes a new row, row id 6. Row id 6 doesn't exist in the 
cache of already done rows. So we redo it.

Heres your problem:
				if(child.getEndMaxLatm() > maxLatmAtChildEnd ) 
child end max latm (int) -25601518
maxlatmatchildend  (int) -25601850

rowcopy:
GpsLocCacheRowData(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 03:58:21.200,minLatStart=-25598252,minLatEnd=-25342789,minLonStart=-54641792,minLonEnd=-54570443,slopeLatm=-0.003228,slopeLonm=0.000878,id=661,c1=754,c2=563,tj=758,numsegments=1,startTimeJump=77377305,endTimeJump=79300404,ws.start=77377305,ws.end=77378319,c1r=0,c2r=753,r=0,lonmwidth=71349,latmheight=255463,endMinLat=-25858727,endMaxLat=-25603264,endMinLon=-54570917,endMaxLon=-54499568)

is a toprow

D/GpsTrailer( 3452): Processing gps loc row GpsLocationRow(timeMs=2010-04-21 03:58:21.200,latm=           -25858
727,lonm=           -54555187,id=19)

This guy is over the end

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 03:51:03.223,minLatStart=-25598252,minLatEnd=-25433160,minLonStart=-54641791,minLonEnd=-54570444,slopeLatm=-0.002098,slopeLonm=0.000878,id=753,c1=657,c2=751,tj=-2147483648,numsegments=1,startTimeJump=77377305,endTimeJump=77378319,widthLonm=71347,heightLatm=165092,endMinLat=-25766610,endMaxLat=-25601518,endMinLon=-54571301,endMaxLon=-54499954)

Here in latmslopechooser, is the point not being accounted for:
-25601518 1271793063223
It has the right end time, and the right latm

Ok, first best slope is 0 to 6, which makes sense as the left slope

The right slope is -25433160, 1271712806470 (index 1) to 


No, that one failed,


it's now #3 to #5

0 com.rareventure.gps2.database.SlopeChooser$Point@e9841092
-25598252
1271712806470
1 com.rareventure.gps2.database.SlopeChooser$Point@e9869dfe
-25433160
1271712806470
2 com.rareventure.gps2.database.SlopeChooser$Point@e67c495b
-25598139
1271713312286
3 com.rareventure.gps2.database.SlopeChooser$Point@e6796fbe
-25411168
1271713312286
4 com.rareventure.gps2.database.SlopeChooser$Point@e2b0c019
-25766610
1271793063223
5 com.rareventure.gps2.database.SlopeChooser$Point@e2bf4f25
-25601518
1271793063223
6 com.rareventure.gps2.database.SlopeChooser$Point@e2b9a949
-25858727
1271793501200
7 com.rareventure.gps2.database.SlopeChooser$Point@e2b483a4
-25671756
1271793501200


com.rareventure.gps2.database.SlopeChooser$Point@e9841092
-25598252
1271712806470
com.rareventure.gps2.database.SlopeChooser$Point@e2b9a949
-25858727
1271793501200

0 to 6 wins

remember, 5 is our boy (the one outside of the resulting box)
here is minDist end -25602931.7485691
So as you can see, it doesn't reach it

So what we need to do is figure out how it's coming up with 
the min dist when it calculates 0 to 6

The right side max point is currently #3

Pretty darn close...

The rightsidemaxpoint is our boy (who's coming up short for the mindist)
 (double) 255795.2514308542
Yup thats the number and yup its short. So what the heck?

The slope is backwards... what the heck?
How'd this survive?

So this is the slope?  (double) -0.00322790596114517
here is the time distance (which is negative) (long) -80256753
and here is "basispoint2"  (int) -25601518

So what we got is
(-0.00322790596114517) * (-80256753) + (-25601518) - (-25598252) 
=
255795.251430854

and if we reverse the slope we get:
(0.00322790596114517) * (-80256753) + (-25601518) - (-25598252) 
=
-262327.251430854

What a weird thing....

Ok so what the heck is going on?

We are going backwards in time. So the further backwards we go, the more we push the
point we end up positive (in the original calcuation).
Now the deal is, the further backwards we go, the more distance we add.
Ok, so that sounds right. I mean it slices toward the positive as we go down
towards point 1. 
Now, we are adding basispoint2.x and basispoint2.x is our bad boy, number 5,
So the further positive it is.. the further positive it is... like if it 
was zero, then .. it looks totally wrong there.
We should be adding it. Not subtracting it.
Oh, we are. And then we subtract out the original.

Which should be right.

Ok, so lets see. 

ok, this is how we determined that mindist didn't reach it
255795.2514308542 -25858727
Ah, so it does reach it. -25858727 is part of point #6 which is higher,
so of course it won't stretch as far.

So it's a weird backwards calcuation, but it works

ok, here is the new slope
 (double) -0.00322790596114517
the same

max latm at start  (int) -25342789


 -25342789 - (-25598252)
Got:  -25342789 - (-25598252)

255463  -- 0x3e5e7  -- 0762747  -- 111110010111100111
what? the min dist is now smaller

So somehow the mindist got smaller.

oh. we are using the other child. oh great!

ok, you can see the gap from what the other child was and what it 
has become.
And the parent fit over what it has become like a glove.

So here it comes, boys. What we need to do, is see, what the heck
is the actual problem here... because the other child should be
updated. It should be changed.

Oh, wait again! Now it's a different row entirely, its the child
thats making the screw up!

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 03:51:03.223,minLatStart=-25598252,minLatEnd=-25433160,minLonStart=-54641791,minLonEnd=-54570444,slopeLatm=-0.002098,slopeLonm=0.000878,id=753,c1=657,c2=751,tj=-2147483648,numsegments=1,startTimeJump=77377305,endTimeJump=77378319,widthLonm=71347,heightLatm=165092,endMinLat=-25766610,endMaxLat=-25601518,endMinLon=-54571301,endMaxLon=-54499954) 

No, it's the same row, 661

Yes, it's testing with the original.
Now it's starting to amke sense.

So the id of the chosen child is 753 and the id of the row is 754! Surprise surprise!
That's a new guy.

Now the original row at this time (when we're commiting, which is before we write to the db), is 
GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 03:51:03.223,minLatStart=-25598252,minLatEnd=-25433159,minLonStart=-54641791,minLonEnd=-54570443,slopeLatm=-0.002098,slopeLonm=0.000878,id=661,c1=754,c2=563,tj=758,numsegments=1,startTimeJump=77377305,endTimeJump=79300404,widthLonm=71348,heightLatm=165093,endMinLat=-25766610,endMaxLat=-25601517,endMinLon=-54571301,endMaxLon=-54499953) 

Here is the chosen child of the adr
 (com.rareventure.gps2.database.GpsLocCacheRow) GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 03:51:03.223,minLatStart=-25598252,minLatEnd=-25433160,minLonStart=-54641791,minLonEnd=-54570444,slopeLatm=-0.002098,slopeLonm=0.000878,id=753,c1=657,c2=751,tj=-2147483648,numsegments=1,startTimeJump=77377305,endTimeJump=77378319,widthLonm=71347,heightLatm=165092,endMinLat=-25766610,endMaxLat=-25601518,endMinLon=-54571301,endMaxLon=-54499954)

So the chosen child is shaped right... well, its shaped like the old child

The updatedChild (sigh) has an id of 753 and is right. It looks good.

The weird thing is it doesn't even have a row replacement.
Well 754 is the time sibling parent of 753. Ok, so this is starting
to make sense. Then the item is probably failing on 754, which wasn't adjusted.
Yes, that is true. Ok now it all makes sense.

Yes, so when we do the adr thing, we are getting the time sibling, and updating
that. This revelation is making me nervous.

I wonder if we can check this stuff again somehow with some test. This was hard to find.
And it has profound implications.

So here is the thing. What we have here is a tree, that's linked to itself. It might 
be good if we always accessed the tree in a way where we always get the correct time sibling.

Then we start to shred it up and place it in this meta thing. 

I'd like not to change it too much, since we're not even sure if it'll work. But then again,
we can't prove that until we are certain there aren't any major bugs. Like this problem really
messed up the tree. And I'm not sure what to do about it.

Well, we could rename everything again. That'll work.

9/9

Ok, so we start out with the tree.

Then we start adding a point. When we do that, we are creating changes to little parts of
each row. Soemtimes, we actually change where we are linking to, and other times, we are
linking to the same place but need to rechange it.

So the point here is that the rowdata's are not really rows. Rows link to time siblings.

Rowdatas link to other row datas which are pieces of rows. So row's are made out of rowdatas.

So here's the trick..

Rows that link to other rows 

FACT: rows that link to the same child always have separate time jump ranges

Ok.. so we know this, and this is something we can check for.

So we start out with rows, and time siblings, etc. Then we create rowdatas, and rowdatas
are different in that they represent a slice of a row. In some cases the backing row doesn't
exist. Ah.. so we should change rowdatas to a new class, rowdata2, and that class is no
longer a row.

Now rows don't contain rowdatas. However, we make changes to all these sections of rows.

Also note that rowdatas that don't have any links keep their parents links. 

So we need to have a container o

9/10

			GpsLocCacheRowData2 child1 = child1DS.getRowDataForTimePeriod(workLoad.currWorkSegment);
			
			//we need to split up the work according to the child even if it doesn't accept it,
			//since we will need to check the next time jump sibling, too, to see if it accepts it
			workLoad.adjustWork(child1.getEndTimeJump());
			

			if(child2DS == null)
				Assert.fail("row "+dataSet.row+", child1 "+child1+" child2 null, workLoad currSegment is "+workLoad.currWorkSegment.start);

			GpsLocCacheRowData2 child2 = child2DS.getRowDataForTimePeriod(workLoad.currWorkSegment);

			//same reasoning as child one, above
			workLoad.adjustWork(child2.getEndTimeJump());
	


//			workLoad.adjustWork(granualize(timeJump1));
//			workLoad.adjustWork(granualize(timeJump2));
			//TODO 1: HACK to speed failure by not granulizing the time jump
			workLoad.adjustWork(granualize(timeJump1));
			workLoad.adjustWork(granualize(timeJump2));

9/11/10

We go through each dataset as follows:

For rowdata-less data, we create or find ds parents...



For rowdata-less spots, we create rowdatas. We create rowdatas 

Then we do the standard combinations. For each rowdata, we find it's parent(s), if we
don't already know (and make it live if necessary)

9/12/10

Parents needs to know id of child to save.
Child needs to know extent of parent to know where to put tj links.

So we should have two passes.

First pass, determine combinations, and create rowdatas:
1. Find parents that aren't live and make them so. Each piece of child
should have an individual parent. 
2. Combine data and make rowdatas. Rowdatas will store children DS

We need tj siblings. Otherwise the tree gets fragmented from bottom to top
whenever a change in path happens between time jumps 

So remember that initially, all rowdatas are split up finely so they don't cover
two children and are pointing directly at their DS

---

So what do we do about new children?
We have two options.. combinable DS's or that's it.

------

9/13/10

Ok so that is done..

Now here is the next issue is that we need to create new rd's for
gaps. Ok, so when we create a rd initially, we do not populate
child ds's in any shape or form.

But afterwards, we create or get the child ds if we are past the 
give up score.

Ok. so that's fine.

9/15

I can't figure out why we need a list for child1 and child2.

If we simply have a single DS and make sure these are the same,
we can combine child1 and child2 and not have to worry about it.

If we have brand new DS's, we need to combine these, and they
better be pointing to the same children DS's, or something
is wrong.

9/16

So I think that the we may need lists of child or at least some
mechanism. This is because suppose we have a mega big rowdata
with children that have timesiblings. When we add a point to separate
worksegments for this big rowdata, it can be combinable even though
it has different children.. it may not have changed at all in either case.
When we create rowdatas, we link to the actual timesibling, so if we created two rowdatas
for the megabig row, they would both be pointing to separate timesiblings.

I suppose we could combine the ds's into one for the children. But in that 
case, we'd have two row ids that could be cannibalized.

There is a disconnect here.. So right now we are concerned, because we usually
have a parent per DS. But if we combine these guys, I mean, what would make sense would be
no parents.

Ok. Here is the plan.

In the addPoint() code. One rowData, two children (ie. child1 and child2).
Since the workload splits everything based on the minimum timejump of children
and parents, we can be assured that there will be at most only 1 child1 and 1
child2 for parents.

In the squish code, we allow parents to have multiple children. Each parent
points to *all* it's *rd*'s, not DS's. This is because DS's can't be used
to always contain all the rd's of a squished parent (due to the problem
as explained above right after 9/16 about the megabig row)

Each rd will link to its DS, that way we can determine who get's what row id.
That's the point of the DS's.

Instead of links to parents, I think we should just keep track of top rows.
We can also keep track of dead parents. Do dead parents possibly have live
parents and will that fuck things up?

No, because a dead parent is dead. It means that any parents of it aren't
changing it. Aren't splitting it. Aren't touching it, or their references to
it.

So we start at the absolute top and work our way down. Except that... two
parents can point to the same child ds.. what about rd? Of course. Because
we can combine rd's. So fuck it. Let's keep the parent records there.

So it will be parent rd's, child rd's, and links between the two in both
directions. Then we combine. Children first. By DS.

Because, the idea is that for DS's we can't join rowdata's across them.

IDEA: That's so interesting, the idea that we could combine rows that overlap
a substantial amount. A sort of restructoring. So if we do that, we 
have 4 children. So we create two children to contain these four.
A bubble up approach in a way. Like we have substantial overlap, so
we know we have 4 children. Either those children have overlap or they
don't. If they do, we go down further. Until we get to the case where
the parents overlap but the children don't. Then we swap children to
get a better outlook and bubble up to where it doesn't matter anymore.
This isn't a silver bullet, however. It won't defeat dragons.

Only rowdatas can join each other.

* Update squish
* Update sasve

9/17/10

There are certain rules that must be followed for children to be
formed. What are they? We need to know to verify that we have
time sibling lines, with no two time siblings pointing to the
same row.

Rule 1: rows will never be combined unless they are part of the same
ds, or both part of sequential brand new ds's that contain the exact
same children (ie. were initially formed by the same ds)

If they are from the same ds, children will always be part of the same
chain to begin with. The only exception is when a row creates
a new row for a child, but that row *must always link back to the chain again*

So it seems that we should not combine parent rows unless:

1. Their children belong to the same DS, or,
2. The DS's are time siblings of each other.

In particular, we do NOT timesibling link when we have a brand new 
child (ie. replace the parent) situation.

So if the children are separate for brand new rows, then what do we need
to do to combine them?

The children of a brand new node are separate, but their children will
point to the same DS. When combining rowdatas for a particular DS, we
do not consider the parent(s). Therefore, they will be fine. The only
thing is to reset the parents and add the additional children.

9/17

			//if we have exhausted the current parent
			boolean justStartingNewParentDs;
			if(pri == -1)
			{
				pdi++; //go to the next one
				pri = 0;
				justStartingNewParentDs = true;
				
				if(pdi == parents.size())
					Assert.fail("ran out of parents for "+this.row+", ri "+ri+", pos "+timeJumpPos);
			}
			else
				justStartingNewParentDs = false;
			
			
			//
			//adjust the parent based on the starting pos
			//
			GpsLocCacheRowDataSet parentDataSet = parents.get(pdi);
			GpsLocCacheRowData2 parentRowData = null;
			GpsLocCacheRowData2 rowData = rowDatas.get(ri);
						
			boolean firstTime = true;
			while(pri < parentDataSet.rowDatas.size())
			{
				parentRowData = parentDataSet.rowDatas.get(pri);
				if(parentRowData.getEndTimeJump() > timeJumpPos)
					break;
				
				//after the finding the first rowdata, there should be no gaps and all starting positions should be covered
				if(justStartingNewParentDs && !firstTime)
					Assert.fail("parent row datas have a time jump gap that is missing the current position, parent "+parentDataSet.row+
							" pos "+timeJumpPos+", us: "+row);
				pri++;
				firstTime = false;
			}
			
			if(pri == parentDataSet.rowDatas.size())
			{
				pri = -1;
				continue;
			}
				
			//so pri and pdi should be pointing at the correct items. In addition, parentRowData should be set
			


9/18/10

	public class TreeUniverseViewer
	{
		HashMap<GpsLocCacheRowData2,String> rowDataToRdId = new HashMap<GpsLocCacheRowData,String>();
		private HashSet<ArrayList<GpsLocCacheRowData>> displayedRdArrays = new HashSet<ArrayList<GpsLocCacheRowData>>();
		public void viewTreeUniverse(GpsLocCacheRowDataSet ds) 
		{
			Log.d("HACKVTU", "start");
			Log.d("HACKVTU", "id,c1,c2,rep,tjs,wss,wse,"+GpsLocCacheRowData2.commaSeparatedViewHeaders());
			populateRowDataToIdFor(ds);
			
			viewTreeUniverse2(rowData);
		}
	
		private void viewTreeUniverse2(ArrayList<GpsLocCacheRowData> rowData) {
			Collections.sort(rowData, TIME_JUMP_DESC_COMPARATOR );
			if(displayedRdArrays.contains(rowData))
				return;
			
			displayedRdArrays.add(rowData);
			
			StringBuffer simple = new StringBuffer();
			for(GpsLocCacheRowData rd : rowData)
			{
				simple.append(getSimpleData(rd, rowDataToRdId)).append(":");
				Log.d("HACKVTU", getCommaSeparatedViewForRd(rd, rowDataToRdId));
			}
			simple.delete(simple.length()-1, simple.length());
//			Log.d("HACKVTS", simple.toString());
			
			for(GpsLocCacheRowData rd : rowData)
			{
				viewTreeUniverse2(rd.child1Replacement);
				viewTreeUniverse2(rd.child2Replacement);
				viewTreeUniverse2(rd.getTimeJumpSiblingFkz());
				
			}
		}
	
		private void viewTreeUniverse2(int rowId) {
			ArrayList<GpsLocCacheRowData> rowArray = idToRow.get(rowId);
			
			if(rowArray == null) return;
			
			viewTreeUniverse2(rowArray);
		}

		private void viewTreeUniverse2(GpsLocCacheRow row) {
			if(row == null) return;
			ArrayList<GpsLocCacheRowData> rowArray = idToRow.get(row.id);
			
			if(rowArray == null) return;
			
			viewTreeUniverse2(rowArray);
		}

		private String getSimpleData(GpsLocCacheRowData rd, HashMap<GpsLocCacheRowData, String> rowDataToRdId) {
			String child1Id = getRdId(rd.getChild1(), rd.getChild1Fkz(), rowDataToRdId);
			String child2Id = getRdId(rd.getChild2(), rd.getChild2Fkz(), rowDataToRdId);
			String replacement = getRdId(rd.replacement, Integer.MIN_VALUE, rowDataToRdId);
			String tjs = getRdId(null, rd.getTimeJumpSiblingFkz(), rowDataToRdId);
			
			return String.format("id=%s c1=%s c2=%s r=%s tjs=%s wstj=%d wetj=%d",
					rowDataToRdId.get(rd),
					child1Id,
					child2Id,
					replacement,
					tjs,
					rd.workSegment.start,
					rd.workSegment.end);
		}

		private String getCommaSeparatedViewForRd(GpsLocCacheRowData rd, HashMap<GpsLocCacheRowData, String> rowDataToRdId) {
			String child1Id = getRdId(rd.getChild1(), rd.getChild1Fkz(), rowDataToRdId);
			String child2Id = getRdId(rd.getChild2(), rd.getChild2Fkz(), rowDataToRdId);
			String replacement = getRdId(rd.replacement, Integer.MIN_VALUE, rowDataToRdId);
			String tjs = getRdId(null, rd.getTimeJumpSiblingFkz(), rowDataToRdId);
			
			return String.format("%s,%s,%s,%s,%s,%d,%d,%s",
					rowDataToRdId.get(rd),
					child1Id,
					child2Id,
					replacement,
					tjs,
					rd.workSegment.start,
					rd.workSegment.end,
					rd.toCommaSeparatedView());
					
		}
		
		private String getRdId(GpsLocCacheRow rd, int fk, HashMap<GpsLocCacheRowData, String> rowDataToRdId)
		{
			if(rd == null)
			{
				return "("+fk+")";
			}
			
			String v = rowDataToRdId.get(rd);
			if(v != null)
				return v;
			
			return rd.id+"";
		}
	
		private void populateRowDataToIdFor(GpsLocCacheRowDataSet ds) {
			if(ds == null)
				return;
			
			int i = 0;
			
			int id0 = ds.row.id;
			
			for(GpsLocCacheRowData2 rd : ds.rowDatas)
			{
				if(!rowDataToRdId.containsKey(rd))
				{
					rowDataToRdId.put(rd, id0+"~"+(i++));
					if(rd.child1Replacement != null)
						populateRowDataToIdFor(idToRow.get(rd.child1Replacement.id));
					if(rd.child2Replacement != null)
						populateRowDataToIdFor(idToRow.get(rd.child2Replacement.id));
					if(rd.replacement != null)
						populateRowDataToIdFor(idToRow.get(rd.replacement.id));
					if(idToRow.containsKey(rd.getTimeJumpSiblingFkz()))
						populateRowDataToIdFor(idToRow.get(rd.getTimeJumpSiblingFkz()));
				}
			}
		}
	} //end TreeUniverseViewer
	

9/20

Ok, so when we addPoint() we are only creating rowDatas when we make changes to
the top row.

True.

Which means we are left with Squish. But squish is currently running, so the 
parent may not be squished yet.

So the thing here is this. Ok, we when we makeParentAlive we don't squish it,
because we make it in such a way that it's good. Wait, what are parents for?

Ah, they're used to determine time sibling links, and that is it.

Ok, in squish we don't set up parent to child links, but we do set up
child to parent links by virtue of makeParentAlive.

Ok, so we squish the parent
hahaha

addPoint() will create parents for all the rows attached to points

ok squish will squish two rowdatas that are time siblings of each other

So now we have these zombie rows, which we will save. If we squish
them, of course their parents will be created. Maybe that is for the 
best. Because then everything seems the same.

In general we don't create rowdatas unless we change something..
well not exactly true.. so maybe we should just create a single
huge rowdata, because we already know it would squish.

Shoot... I don't want to navigate through all these lists.
Lists of children, lists of parents...
Time siblings everywhere. It's all because of the darn
live tree.

I think that, when we squish, we need to set up the parent
links when we fill in the gaps. So in that case,
we'll need to squish the children first. It still means
that we will need to navigate children, right? 

Ok, so there are two directions we can go:

1. child to parent
2. parent to child

We will always need to do 1. EIther that or squish
zombie parents.

Anyway, for 1, we will have to navigate down the 
real tree. Because, well, wait... When we create new
nodes, can't we parentize the children then?
Ah, but originally they are dead. No, I doubt that.
Because if we do parantize them, and create row ds's,
then the only reason not to have a ds is if there really
isn't a live parent for that... no, because you could go
the wrong way down ... yes, we would have to make the whole
tree live then.

So yea, we'd have to navigate down the live tree, which is 
a pain

For 2, we need to squish parents first. So we're only dealing 
with the empty rows, right? The non parent rows. So if we lets
say record all the parent DS's and start there and squish downwards,
we should be ok. Then if we run into a empty row with no parent,...
only the thing is that we need to assign a child.... like yea,
we can't assign parents when going down the tree if we haven't
created the children yet.

So I guess we need to follow the live combined with dead tree.
Darn. (and do #1, child to parent)

9/21

the parent is 1, x-y

ri = 1

worksegment of row is x-y

--

id is -1
right child is 3 (with error)
(com.rareventure.gps2.databasex.GpsLocCacheRow) GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-20 06:15:34.358,minLatStart=-25598139,minLatEnd=-25595052,minLonStart=-54572913,minLonEnd=-54570673,slopeLatm=-0.000188,slopeLonm=-0.002539,id=3,c1=6,c2=7,tj=-2147483648,numsegments=1,startTimeJump=717834,endTimeJump=7776000000,widthLonm=2240,heightLatm=3087,endMinLat=-25598520,endMaxLat=-25595433,endMinLon=-54578047,endMaxLon=-54575807)

It doesn't have a time sibling...
hmm


ok 3 is parented by 1, ri 1
ds 1 ri 0 flies off somewhere else.

So the problem occurs when we are saving a new id
So the rowdata has been replaced for -1, but 

GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-20 06:15:34.358,minLatStart=-25598139,minLatEnd=-25595052,minLonStart=-54572913,minLonEnd=-54570673,slopeLatm=-0.000188,slopeLonm=-0.002539,id=3,c1=6,c2=7,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=2240,heightLatm=3087,endMinLat=-25598520,endMaxLat=-25595433,endMinLon=-54578047,endMaxLon=-54575807)

GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-20 06:15:34.358,minLatStart=-25598139,minLatEnd=-25595052,minLonStart=-54572913,minLonEnd=-54570673,slopeLatm=-0.000188,slopeLonm=-0.002539,id=3,c1=6,c2=7,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=2240,heightLatm=3087,endMinLat=-25598520,endMaxLat=-25595433,endMinLon=-54578047,endMaxLon=-54575807)


----------

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=-25598252,minLatEnd=-25598252,minLonStart=-54570449,minLonEnd=-54570449,slopeLatm=0.000223,slopeLonm=-0.000445,id=2,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=0,heightLatm=0,endMinLat=-25598139,endMaxLat=-25598139,endMinLon=-54570674,endMaxLon=-54570674) -25598252 -54570449 1271712806470
-25598139 -54570674 1271713312286
-25598252 -54570449 1271712806470
-25598139 -54570674 1271713312286
-25598252 -54570449 1271712806470
-25598139 -54570674 1271713312286


row 1 has no time sibling but doesn't cover 0 through y (only x through y)


Here is parent:

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25599406,minLatEnd=-25598251,minLonStart=-54570449,minLonEnd=-54568901,slopeLatm=0.002503,slopeLonm=-0.003503,id=4,c1=2,c2=3,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=717834,widthLonm=1548,heightLatm=1155,endMinLat=-25596


Here is the child:

GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-20 06:15:34.358,minLatStart=-25598139,minLatEnd=-25595052,minLonStart=-54572913,minLonEnd=-54570673,slopeLatm=-0.000188,slopeLonm=-0.002539,id=3,c1=6,c2=7,tj=-2147483648,numsegments=1,startTimeJump=717834,endTimeJump=7776000000,widthLonm=2240,heightLatm=3087,endMinLat=-25598520,endMaxLat=-25595433,endMinLon=-54578047,endMaxLon=-54575807) 

Well, then, the time sibling is messed up, in that there is none, for the child. And no, the link shouldn't be there at all.
So when we are saving "4" we should not be linking to 3.


----

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25599406,minLatEnd=-25598251,minLonStart=-54570449,minLonEnd=-54568901,slopeLatm=0.002503,slopeLonm=-0.003503,id=-1,c1=2,c2=3,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=717834,widthLonm=1548,heightLatm=1155,endMinLat=-25596343,endMaxLat=-25595188,endMinLon=-54574735,endMaxLon=-54573187)

curr worksegment: WorkSegment(start=0,end=717834)

So this weird guy is still pointing to 3 and it shouldn't because 3 is from x to y
The "id" is -2

So the problem is that when we try to save "-2" (see the sheet) it doesn't properly
link to the correct child of 3. Because when we create a new poof" node, we keep the children
dead. So, 2 and 3 are dead. However, 3 becomes alive by an alternate path.
So now that 3 is alive, we need to somehow get "-2" pointing at the correct rowdata of
the splitted up #3.


Ah, so the point is the whole "leave the children where they were pointing before" when
child1List and child2List works as long as the children are dead.

Once they become alive, we need to fix the parent. Now, can we just fix it in
makeParentAlive? Yes, because every time range for tj periods have only one parent.
And makeParentAlive finds all of them.

---

9/22/10

So it starts out at 0-x and we are at x!


GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:53:50.120,minLatStart=-25599406,minLatEnd=-25598251,minLonStart=-54570449,minLonEnd=-54568901,slopeLatm=0.002503,slopeLonm=-0.003503,id=7,c1=2,c2=6,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=717834,widthLonm=1548,heightLatm=1155,endMinLat=-25596343,endMaxLat=-25595188,endMinLon=-54574735,endMaxLon=-54573187)

Here is the parent, 0-x

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:15:34.358,minLatStart=-25599406,minLatEnd=-25595615,minLonStart=-54571392,minLonEnd=-54568900,slopeLatm=0.000350,slopeLonm=-0.002732,id=9,c1=7,c2=8,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=717834,widthLonm=2492,heightLatm=3791,endMinLat=-25598520,endMaxLat=-25594729,endMinLon=-54578299,endMaxLon=-54575807)

---

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=-25598252,minLatEnd=-25598252,minLonStart=-54570449,minLonEnd=-54570449,slopeLatm=0.000223,slopeLonm=-0.000445,id=2,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=0,heightLatm=0,endMinLat=-25598139,endMaxLat=-25598139,endMinLon=-54570674,endMaxLon=-54570674) 

has no parent for timejump 0!

DS 2 <-- DS 7 <-- DS -8 <-- DS 9 RD 0

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:15:34.358,minLatStart=-25599406,minLatEnd=-25595615,minLonStart=-54571392,minLonEnd=-54568900,slopeLatm=0.000350,slopeLonm=-0.002732,id=-1,c1=7,c2=8,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=717834,widthLonm=2492,heightLatm=3791,endMinLat=-25598520,endMaxLat=-25594729,endMinLon=-54578299,endMaxLon=-54575807) (DS -8)

So the problem is that DS 7 isn't squished yet. makeParentsAlive! works fine for dead parents. But live ones that aren't squished yet...
Can we squish them?

The whole thing is kind of weird... when we squish a child, what do we do with the parents?

Well, the first thing to realize is that when we squish a parent, we check for the following conditions for the child, thereby we can handle 
squishing a parent with unsquished children:
					//1. Their children belong to the same DS, or,
					//2. The DS's are time siblings of each other, or,
					//3. Both the DS's are brand new and point to the same children

Yes, but there is the bug. We, as parents aren't notified that our children are being killed and replaced.

So, we could notify them, and change them...



I have a question about this whole thing. DS -8 doesn't point to DS 7 directly through the rowdata. This is true.
But the rule is that if there are no children, we default to the row. Do we want to change this rule?
Yea, I think so. It's a pain the way it is. But the whole makeParents alive before squish doesn't quite 
work right either since there are still gaps.

So I think that after adding the points, we fix the tree by

1. fixing gaps, and
2. pointing all parents and all childrens correctly.

Then we squish.

Then we save.

But why can't we squish first?
Do we care?
Squishing is hard with gaps. We'd have to squish
a gap, squish a row and a gap and squish row and row.

True, but we should already being doing this, but I give you that
it is error prone.

Because when we have a gap, we need to find the child to figure out if we can
even squish it. 
ok
good enough.




	/**
	 * Squishes a single data set
	 */
	private void squishDataSet(GpsLocCacheRowDataSet dataSet) 
	{
		if(dataSet.isSquished)
			return;
		dataSet.isSquished = true;
		
		//combine the rows together.. and save later. Note that we don't try to combine these
		//steps since two parents need to access the same child in some cases, and that
		//would be hairy because the child needs info from the parent (the extent of each rowdata)
		// and the parent needs to know the child ids before saving. So this interlocked marriage
		//of parent and child causes some craziness if we have two parents.
		
		LopsidedSpaceTimeBox2 rowLstb = new LopsidedSpaceTimeBox2();
		rowLstb.addRow(dataSet.row);
		rowLstb.finishCalculation();
		
		if(!dataSet.advanceToNextPeriod())
			Assert.fail("row doesn't even have one period, sad (and illegal)");
		
		GpsLocCacheRowData2 startRowData;
		
		long startRowStartTimeJumpMs;

		//while we are going through similar groups of ranges within the dataset
		while(true)
		{
			startRowData = dataSet.getCurrPeriodRowData();
			
			startRowStartTimeJumpMs = dataSet.getCurrPeriodStartTimeJump();
			
			LopsidedSpaceTimeBox2 currLstb;
			
			if(startRowData != null)
				currLstb = startRowData.lstb;
			else
				currLstb = rowLstb;
		
			//while we are advancing endTimeJumpPos
			while(dataSet.advanceToNextPeriod())
			{
				GpsLocCacheRowData2 nextRowData = dataSet.getCurrPeriodRowData();
				
				if(nextRowData != null)
				{
					//ok here we do something completely different.
					//if startRowData is null, it and nextRowData is not, it means that the start is a gap
					//and it is going to have to be filled. So we create a new rowData for it.
					if(startRowData == null)
					{
						startRowData = dataSet.createRowDataForTimePeriod(new WorkSegment(startRowStartTimeJumpMs, dataSet.getCurrPeriodStartTimeJump()),
							null, null, true);
					
						//All rowDatas need their parents to be alive. 
						//This is because a row data will one day become a new row with a brand new id. So 
						//for any parent that was included in that time range, we will need to update it's child id's
						//so.... (note that this does not include the first un-rowdata'ed range, if any. This is because
						// that range will be canabilized -- the row will be updated and the row id will be reused)
						makeParentsAlive(dataSet.row.id, startRowData);
						
						//NOTE that we do not need to make parents alive for rowdatas that are already present.
						//This is because they were found based on a parent, and because we slice timejumps as
						//much as possible, there is no way they can have more than one parent
					}
					//**if rows are combinable based on lstb**
					if(nextRowData.lstb == null)
					{
						if(!rowLstb.equals(currLstb))
							break;
					}
					else if(!nextRowData.lstb.equals(currLstb))
						break; //if the end row data is different from the start
					
					//We should not combine rows unless:
					//1. Their children belong to the same DS, or,
					//2. The DS's are time siblings of each other, or,
					//3. Both the DS's are brand new and point to the same children

					GpsLocCacheRowDataSet currChild1DataSet = nextRowData.getFirstChild1DataSetz();
					GpsLocCacheRowDataSet currChild2DataSet = nextRowData.getFirstChild2DataSetz();
					
					GpsLocCacheRowDataSet lastChild1DataSet = startRowData.getLastChild1DataSetz();
					GpsLocCacheRowDataSet lastChild2DataSet = startRowData.getLastChild2DataSetz();
					
					boolean areCombiningBrandNewRows1 = false;
					boolean areCombiningBrandNewRows2 = false;
					
					if(currChild1DataSet != lastChild1DataSet  // rule #1
						&& lastChild1DataSet.row.getTimeJumpSiblingFkz() == currChild1DataSet.row.id //rule #2 
						&& !(areCombiningBrandNewRows1 = lastChild1DataSet.canCombineWith(currChild1DataSet))) //this is rule #3, brand new children
						break; // rules are not satisified for child 1
						
					if(currChild2DataSet != lastChild2DataSet  // rule #1
						&& lastChild2DataSet.row.getTimeJumpSiblingFkz() == currChild2DataSet.row.id //rule #2 
						&& !(areCombiningBrandNewRows2 = lastChild2DataSet.canCombineWith(currChild2DataSet))) //this is rule #3, brand new children
						break; // rules are not satisified for child 2
		
					//ok, now we know we can combine					

					if(areCombiningBrandNewRows1)
					{
						//in this case we keep one of the ds's and destroy the other for the child
						//we have to combine the rowdata to do this
						
						if(startRowData.child1List.size() > 1 || nextRowData.child1List.size() > 1)
							Assert.fail("brand new children should always have a single child only");
						
						startRowData.getLastChild1().replaceBrandNewRow(nextRowData.getFirstChild1());
					}
					else 
						startRowData.addAdditionalChild1(nextRowData.getFirstChild1());
						
					if(areCombiningBrandNewRows2)
					{
						//in this case we keep one of the ds's and destroy the other for the child
						//we have to combine the rowdata to do this
						
						if(startRowData.child2List.size() > 1 || nextRowData.child2List.size() > 1)
							Assert.fail("brand new children should always have a single child only");
						
						startRowData.getLastChild2().replaceBrandNewRow(nextRowData.getFirstChild2());
					}
					else 
						startRowData.addAdditionalChild2(nextRowData.getFirstChild2());
						

					//now that we've finished with the children, we update the current row
					//the rule still is, for the current row at least, that they cannot be combined 
					//unless they are part of the same DS
					startRowData.replaceRowData(nextRowData);
					
					//kill the current "nextRowData"
					dataSet.deleteCurrRow();
				}
				else //next data is null
				{
					if(currLstb != rowLstb && !currLstb.equals(rowLstb))
						break; //if the start row data is different from the row (and the end data is empty)
					
					
					if(startRowData != null)
						startRowData.updateTimeJumpEnd(dataSet.getCurrPeriodEndTimeJump());
				}
				
			}//while evaluating the current start node
			
			if(dataSet.isCurrPeriodAtEndOfDataSet())
				break;
			
		}// while we are squishing rowdatas together
		
		//we want to create a row data for the last bit of the ds to make it easier when we save it
		if(startRowData == null)
		{
			startRowData = dataSet.createRowDataForTimePeriod(new WorkSegment(startRowStartTimeJumpMs, dataSet.row.getEndTimeJump()),
				null, null, false);
			makeParentsAlive(dataSet.row.id, startRowData);
		}
	}



Ok, so the point here is that a rowdata is either set or not. It either has all it's children links set up or not.



	/**
	 * 
	 * @param rowId
	 * @param rowData
	 * @param currParent
	 * @param parentDS the parent DS if known. If the row has a non negative id, this doesn't need
	 *  to be specified. Used for brand new "poof" rows
	 * @param startTimeJump
	 * @param isTopRow
	 * @return
	 */
	private GpsLocCacheRowData2 makeParentsAlive2(int rowId, GpsLocCacheRowData2 rowData, 
			GpsLocCacheRow currParent, GpsLocCacheRowDataSet parentDS, long startTimeJump, boolean isTopRow) {
		//it is guaranteed that all ancestors of a child row
		//contain the entire box within them. So we just search for one corner
		
		if(currParent == null)
			return null;
		
		
		if(parentDS == null)
			//check if there already is a live parent (if we treat a live parent as a dead one, we don't pick up any
			//changes to the children, thereby possibly incorrectly assigning the wrong parent)
			parentDS = idToDataSet.get(currParent.id);
		
		if(parentDS == null)
		{
			for(GpsLocCacheRow child : new GpsLocCacheRow [] { currParent.getChild1z(), currParent.getChild2z() })
			{
			
				//here we decide if we want to create it
				if(child.id == rowId)
				{
					//found a parent. Make it alive if it's not, and return it
					//we use the currparent start time jump, because we have already validated that it directly points to
					//the child. 
					parentDS = 
						getOrCreateRowDataSetForTimePeriod(isTopRow, currParent, currParent.getStartTimeJump());
					GpsLocCacheRowData2 parentRowData = parentDS.createRowDataForTimePeriod(
							new WorkSegment(currParent.getStartTimeJump(), currParent.getEndTimeJump()), null, null, false);
					
					//in this case, we are creating the parent *after* we create the child. So we let the child know it has a parent
					rowData.addParent(parentRowData);
					
					//we need to link the parent to the current child
					if(child == currParent.getChild1z())
						parentRowData.addAdditionalChild1(rowData);
					else
						parentRowData.addAdditionalChild2(rowData);
					
					return parentRowData;
				}
			}
			
			//if we are definitely not the parent
			if(!currParent.containsPoint(rowData.lstb.minLonm, rowData.lstb.minLatm, rowData.lstb.startTime))
				return null;
			
			//check both children to see if they are parents of rowData (and dataSetOfRowData)
			//Note that only one should be (this is regardless of the area of overlap that might be there) and 
			//we check both for assertion only
			GpsLocCacheRowData2 result = makeParentsAlive2(rowId, rowData, 
					getSiblingForTimeJump(currParent.getChild1z(), startTimeJump), parentDS, 
					startTimeJump, false);
			GpsLocCacheRowData2 result2 =	makeParentsAlive2(rowId, rowData, 
					getSiblingForTimeJump(currParent.getChild2z(), startTimeJump), parentDS,
					startTimeJump, false);
			
			if(result == null && result2 == null)
				return null;
			
			if(result != null && result2 == null)
				return result;
			
			if(result2 != null && result == null)
				return result2;
			
			Assert.fail("two childrens are parents of the node "+result+", "+result2+", and the target "+rowData);
			return null;
			
		} //if the parent was dead

		//if we are here, the parent is alive

		
		//we squish here so that all the gaps are filled in, and we can get the correct rowdata to set the parent
		//if this row turns out to be the actual parent, squish will not automatically set us as the child of the this row
		// as it does not do that for any row
		squishDataSet(parentDS);
		
		GpsLocCacheRowData2 parentRowData = parentDS.getRowDataForTimeJump(startTimeJump);
		
		int childIndex = parentRowData.isParentOf(rowData);
		
		//if it's the parent
		if(childIndex != 0) 
		{
			//we add it as such
			rowData.addParent(parentRowData);

			//we need to link the parent to the current child
			if(childIndex == 1)
				parentRowData.addAdditionalChild1(rowData);
			else 
				parentRowData.addAdditionalChild2(rowData);
			
			return parentRowData;
		}
		
		if(rowData.lstb != null)
		{
			if(!parentRowData.containsPoint(rowData.lstb.minLonm, rowData.lstb.minLatm, rowData.lstb.startTime))
				return null;
		}
		else if(!parentRowData.containsPoint(rowData.dataSet.row.getMinLonm(), rowData.dataSet.row.getMinLatm(), 
				rowData.dataSet.row.getStartTimeMs()))
			return null;

		//it's not the parent, but one of it's children might be.
		for(GpsLocCacheRowData2 child : parentRowData.child1List)
		{
			if(child.containsTimeJump(startTimeJump))
			{
				return makeParentsAlive2(rowId, rowData, child.dataSet.row, child.dataSet, startTimeJump, false);
			}
		}
		for(GpsLocCacheRowData2 child : parentRowData.child2List)
		{
			if(child.containsTimeJump(startTimeJump))
			{
				return makeParentsAlive2(rowId, rowData, child.dataSet.row, child.dataSet, startTimeJump, false);
			}
		}
		
		//nope, not a parent of it
		return null;
	}



Ok, we need to decide to go down or up for fixDirtyTree(). I was thinking of going up. If we go down... well, we can't
because we don't know all the parents yet. We only know the children.

So then we need to work from the DS.
Maybe we got this already down.

9/24

Screw these crazy links. They are so annoying.

So we have:

LinkPool

So, how do we do this? We have a parent and all it's child1 guys, and all it's child2 guys.

Then from the perspective of the child, we have all it's parent guys.

So let's say we combine two parents. Then, all the links have to be combined.

So think, from the parent, we say we have x-y covered by child a and
y-z covered by child b

from child a, we have x-y parent g, and child b, we have y-z parent g.

So what we need, to have child a eat child b, is to take this link
from child a and combine it with b. So the thing is, that, see.
parent g has a link to the '2' link. Which needs to be removed.
The thing is this. We have these links, right? And then we change one,
we are actually changing the link from b to g to go to a to g. At this
point, we can combine the links. 

The question I have is whether we should even bother combining. Maybe we 
don't bother. What's the point? We combine and keep the links the way
they are.

So then we have to consider *puke* makeParentsAlive and fixDirtyTree

So we go through each DS. Make row datas where they don't exist.

Ok, so what about dead stuff. Now lets think a little more. We could
go back to DS to DS rather than rowdata to rowdata. 

In this case, we wouldn't have to mess with links at all when combining
rowdata. The only issue here is that we still would have to worry
about combining links because we combine DS's, too for poof rows.

We are going around in circles, aren't we?

But anyway, for dead stuff, we just don't have links. Dead means dead,
which means do not change.

Whenever a child becomes alive, the system must create all links to
other live children.

Ok, so lets say we do DS to DS. Then we only need to combine links for
poof rows.

So, we would have:
parent
child
childIndex
start
end

So that wouldn't be too different.

In any case, it seems like it's going to be a mess.

Even if we have these middlelink data stuff, it's still going to be a mess.

What if we had a range and a tree. 

In any case it's going to be a mess. Yea, forget it.


	public void replaceRowData(GpsLocCacheRowData2 rowData) {
		updateTimeJumpEnd(rowData.getEndTimeJump());
		
		for(GpsLocCacheRowData2 parent : rowData.parents)
		{
			if(parents.contains(parent))
			{
				boolean child1;
				//remove the parents link to the other child
				if(!(child1 = child1List.remove(rowData)) && !child2List.remove(rowData))
					Assert.fail();
				
				if(child1)
				{
					if(!parent.child1List.contains(this))
					{
						parent.child1List.add(this);
						Collections.sort(child1List,TIME_JUMP_COMPARATOR);
				}
				else
				{
					if(!parent.child2List.contains(this))
					{
						parent.child2List.add(this);
						Collections.sort(child1List,TIME_JUMP_COMPARATOR);
					}
				}
			}
		}
	}

Ok so I got that the rowdatas are linked up. But when we create a new one and aren't sure the
children are dead

---

9/25

parentRowData child1 is 0-x
rowData is 0-y

parent is id 1, with two children


it seems like we are doing different things for alive and dead guys
in makePArentsAlive. Why are we still creating ds's for live but for dead?

still 7, stil lnot dead

I wonder if we did it in a time based fashion. This fix dirty tree stuff.
So we start at 0, and go down. Since every time of every node must have a 
parent, there should be no gaps if we do it this way. Basically we go through
from top to bottom, and follow both children... Hmm, we would need to store
the dead guys as "top" nodes to go through as well. No it wouldn't work because
there could be gaps.
 
Ok, so we could go fix links whenever they aren't there. I mean, we see there is
no child, we fix it. We start at the top of the tree. Then we go down to
the bottom.. Ah... because when we fix it, we create a parent for it.
But then, that parent may not be created. Why not hack our way out of this issue?


Should a brand new node have a tj? Yes, it could.
We need to make a seaprate id for ds's to figre this out

So do we keep the tj when creating a brand new node? No. It is a separate entity,

paren t row is 1
rowdata is WorkSegment(start=717834,end=1304238) which has a child1 of our friend.

It has two children in child1

WorkSegment(start=717834,end=1304238)

Yup, somehow both chidl1 and child2 point to this guy

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:15:43.691,minLatStart=-25598252,minLatEnd=-25594992,minLonStart=-54571662,minLonEnd=-54569386,slopeLatm=-0.000108,slopeLonm=-0.002526,id=18,c1=14,c2=16,tj=9,numsegments=1,startTimeJump=717834,endTimeJump=1304238,widthLonm=2276,heightLatm=3260,endMinLat=-25598525,endMaxLat=-25595265,endMinLon=-54578072,endMaxLon=-54575796)
GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:15:43.691,minLatStart=-25598252,minLatEnd=-25594983,minLonStart=-54571670,minLonEnd=-54569396,slopeLatm=-0.000115,slopeLonm=-0.002522,id=1,c1=2,c2=3,tj=18,numsegments=1,startTimeJump=2022072,endTimeJump=7776000000,widthLonm=2274,heightLatm=3269,endMinLat=-25598544,endMaxLat=-25595275,endMinLon=-54578068,endMaxLon=-54575794)

so 1 has a gap to 18

startTimeJump=2022072,endTimeJump=7776000000
startTimeJump=717834,endTimeJump=1304238


ok, right now we are updating tj links in fixDirtyTree
Yet, in squish, we create more potential tj links. 


GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:15:34.358,minLatStart=-25598252,minLatEnd=-25594997,minLonStart=-54571629,minLonEnd=-54569387,slopeLatm=-0.000106,slopeLonm=-0.002539,id=-1,c1=2,c2=3,tj=-2147483648,numsegments=1,startTimeJump=1304238,endTimeJump=2022072,widthLonm=2242,heightLatm=3255,endMinLat=-25598520,endMaxLat=-25595265,endMinLon=-54578048,endMaxLon=-54575806)
GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:15:34.358,minLatStart=-25598252,minLatEnd=-25594997,minLonStart=-54571629,minLonEnd=-54569387,slopeLatm=-0.000106,slopeLonm=-0.002539,id=-1,c1=2,c2=3,tj=-2147483648,numsegments=1,startTimeJump=717834,endTimeJump=1304238,widthLonm=2242,heightLatm=3255,endMinLat=-25598520,endMaxLat=-25595265,endMinLon=-54578048,endMaxLon=-54575806) 



GpsLocCacheRow(startTimeMs=2010-04-20 06:15:34.358,endTimeMs=2010-04-20 06:15:43.691,minLatStart=-25598520,minLatEnd=-25598520,minLonStart=-54575808,minLonEnd=-54575808,slopeLatm=-0.000536,slopeLonm=0.001179,id=-1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=717834,endTimeJump=1304238,widthLonm=0,heightLatm=0,endMinLat=-25598525,endMaxLat=-25598525,endMinLon=-54575797,endMaxLon=-54575797)


One of the brand new rows has a dead parent


GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:15:34.358,minLatStart=-25598252,minLatEnd=-25594997,minLonStart=-54571629,minLonEnd=-54569387,slopeLatm=-0.000106,slopeLonm=-0.002539,id=1,c1=2,c2=3,tj=9,numsegments=1,startTimeJump=717834,endTimeJump=7776000000,widthLonm=2242,heightLatm=3255,endMinLat=-25598520,endMaxLat=-25595265,endMinLon=-54578048,endMaxLon=-54575806)

---

curr row is 8:
GpsLocCacheRow(startTimeMs=2010-04-20 05:53:50.120,endTimeMs=2010-04-20 06:15:34.358,minLatStart=-25595189,minLatEnd=-25595189,minLonStart=-54574735,minLonEnd=-54574735,slopeLatm=-0.002554,slopeLonm=-0.000823,id=8,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=717834,widthLonm=0,heightLatm=0,endMinLat=-25598520,endMaxLat=-25598520,endMinLon=-54575808,endMaxLon=-54575808) 

----

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:20:47.134,minLatStart=-25598252,minLatEnd=-25596410,minLonStart=-54572620,minLonEnd=-54570039,slopeLatm=-0.000833,slopeLonm=-0.001253,id=2,c1=19,c2=20,tj=-2147483648,numsegments=1,startTimeJump=2031405,endTimeJump=7776000000,widthLonm=2581,heightLatm=1842,endMinLat=-25600618,endMaxLat=-25598776,endMinLon=-54576178,endMaxLon=-54573597)



---------


GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:21:40.008,minLatStart=-25599406,minLatEnd=-25593648,minLonStart=-54574201,minLonEnd=-54568898,slopeLatm=-0.000427,slopeLonm=-0.001625,id=-1,c1=46,c2=47,tj=-2147483648,numsegments=1,startTimeJump=9333,endTimeJump=50882,widthLonm=5303,heightLatm=5758,endMinLat=-25600641,endMaxLat=-25594883,endMinLon=-54578903,endMaxLon=-54573600) 

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:21:40.008,minLatStart=-25599406,minLatEnd=-25593648,minLonStart=-54574201,minLonEnd=-54568898,slopeLatm=-0.000427,slopeLonm=-0.001625,id=-1,c1=46,c2=47,tj=-2147483648,numsegments=1,startTimeJump=51896,endTimeJump=52874,widthLonm=5303,heightLatm=5758,endMinLat=-25600641,endMaxLat=-25594883,endMinLon=-54578903,endMaxLon=-54573600) 

----

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:21:40.008,minLatStart=-25599406,minLatEnd=-25593648,minLonStart=-54574201,minLonEnd=-54568898,slopeLatm=-0.000427,slopeLonm=-0.001625,id=48,c1=46,c2=47,tj=30,numsegments=1,startTimeJump=9333,endTimeJump=312776,widthLonm=5303,heightLatm=5758,endMinLat=-25600641,endMaxLat=-25594883,endMinLon=-54578903,endMaxLon=-54573600)

start guy:
WorkSegment(start=9333,end=51896)
child1: WorkSegment(start=9333,end=51896)

but the data set is 
GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 06:21:40.008,minLatStart=-25599406,minLatEnd=-25593648,minLonStart=-54574201,minLonEnd=-54568898,slopeLatm=-0.000427,slopeLonm=-0.001625,id=-1,c1=46,c2=47,tj=-2147483648,numsegments=1,startTimeJump=9333,endTimeJump=50882,widthLonm=5303,heightLatm=5758,endMinLat=-25600641,endMaxLat=-25594883,endMinLon=-54578903,endMaxLon=-54573600)

next guy:
WorkSegment(start=51896,end=52874)
child1 of next guy (brand new with children)
WorkSegment(start=51896,end=52874)

So the question is, how did the rowdata change without the dataset

hackid 261

actual parent is row 53

parents are setup and everything

I wonder if it's getting deleted.
The funny thing is this fails in fixDirtyTree() which is called immediately afterwards.
Yea it has a parent and everything

It's a brand new row (261) with two elements. the second one is messed up.

Ok, so the problem is that currWorksegemnt is changing, but we formed the DS already.
So when we go to fixDirtyTree(), we are looking for a rowdata for the rest of the worksegment
but of course, there isn't any. I don't want to just fix that part, because we have this
weird dirtiness that we have to take care of.

So, what I want to do is fix it immediately after addPoint. But I don't know which
brand new node was created (there will only be one) to fix. 

Saving it seems tough becasue we go through a lot of different paths.

There are two brand new nodes created when going down two different paths

----

hackId = 2, brandnew there are no parents.
Not top row.

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-20 05:41:52.286,minLatStart=-25598252,minLatEnd=-25598252,minLonStart=-54570449,minLonEnd=-54570449,slopeLatm=0.000223,slopeLonm=-0.000445,id=-1,c1=-2147483648,c2=-2147483648,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=0,widthLonm=0,heightLatm=0,endMinLat=-25598139,endMaxLat=-25598139,endMinLon=-54570674,endMaxLon=-54570674)

Processing 3rd point:

---

c1 is a deleted child	

id 1:
GpsLocCacheRow(startTimeMs=2010-04-20 06:24:37.263,endTimeMs=2010-04-21 03:01:28.558,minLatStart=-25599465,minLatEnd=-25599221,minLonStart=-54572553,minLonEnd=-54572354,slopeLatm=0.000009,slopeLonm=0.000024,id=1,c1=4,c2=5,tj=-2147483648,numsegments=1,startTimeJump=0,endTimeJump=7776000000,widthLonm=199,heightLatm=244,endMinLat=-25598833,endMaxLat=-25598589,endMinLon=-54570803,endMaxLon=-54570604) 

This is worksegment of the rowdata for id 1: WorkSegment(start=631087,end=7776000000)
Of this rowdata, child1list, index 0: WorkSegment(start=631087,end=7776000000)
and seems ok
Here is index 1: WorkSegment(start=74177227,end=7776000000), it is the deleted one.

It's ok before fixDirtyTree() (only one child)
WorkSegment(start=631087,end=74177227)

Ok toprow is from 0 to max, and has no tj
So my question is that why isn't there a rowdata for 0 to 631087 and from 7477227 yet? We'll table this for now.

Ok , after fixDirtyTree, we now have 3 nodes, as expected.

Nevermind, there are 3 nodes after addPoint (for toprow #1), as expected. The second and third are equal

Ok, after squish, there are only 2, but it seems ok.

So the question is, before squish, what are the child1's of nodes 1 and 2 of #1?
Ok, children of nodes 1 and 2 look good before fixDirtyTree
Same for before squish

The child1 DS of the first and second rowdatas are the same.

Ok, so the DS in question which is getting messed up is ID 4
The children of ID 4 fir child1 are brandnew but who cares

The problems happens between squishes of #1 and #4. Or maybe during one
of these?

Ok, remember it's dataset 1 of id 1 child1z, #1 is messed up. And this
happens to have a dataset of 4 I believe?
Ok, so 0 gets extended, but 1 still remains a child of dataset 1, id 1 child1z

So both 0 and 1 of child1Listz dataset 1, id 1 point correctly back to
child1Listz, dataset 1, id 1

So the parent should no longer point to the removed child, but it still does
after the squish


--

Here is the row

GpsLocCacheRow(startTimeMs=2010-04-20 06:15:43.691,endTimeMs=2010-04-20 06:20:49.126,minLatStart=-25598545,minLatEnd=-25598524,minLonStart=-54575797,minLonEnd=-54575786,slopeLatm=-0.006833,slopeLonm=0.007216,id=33,c1=53,c2=54,tj=-2147483648,numsegments=1,startTimeJump=2564977,endTimeJump=7776000000,widthLonm=11,heightLatm=21,endMinLat=-25600632,endMaxLat=-25600611,endMinLon=-54573593,endMaxLon=-54573582) 

id 33

startTimeJump=2564977,endTimeJump=7776000000

Here is the actual parent:

GpsLocCacheRow(startTimeMs=2010-04-20 06:15:43.691,endTimeMs=2010-04-20 06:20:49.126,minLatStart=-25598545,minLatEnd=-25598523,minLonStart=-54575797,minLonEnd=-54575785,slopeLatm=-0.006833,slopeLonm=0.007216,id=20,c1=33,c2=34,tj=-2147483648,numsegments=1,startTimeJump=2564977,endTimeJump=7776000000,widthLonm=12,heightLatm=22,endMinLat=-25600632,endMaxLat=-25600610,endMinLon=-54573593,endMaxLon=-54573581) 

id 20, startTimeJump=2564977,endTimeJump=7776000000
Very simple. Looks good.

hackID of 33 is 480

So some blank rowdatas were craeted, this was probably done in fixDirtyTree() when messing with the kids

currtimejumpstart is 74113780

so it's pointing to a different rowdata, but same ds, but it's still like, nope, not *the* parent
Worksgement of child(33) the parent(20) is pointing to is : WorkSegment(start=2564977,end=74113780)

Our worksegment of the target is WorkSegment(start=74113780,end=7776000000)

The parent's(20) worksegment is: WorkSegment(start=2564977,end=7776000000)

So the parent, 20 should be pointing to both the one it is, and us as a second child1

So, I suppose in addPoint, we can check to see what happens when we add (start=74113780,end=7776000000),
because it should point, I would think.

So after addPoint() I think it's ok. So what the heck?

Confusing, idToDataSet.get(20) says it has no rowdatas, but we just created one?

So directly after adding it, rowData is set, so maybe we are reverting?

But if so, why aren't we reverting 33?

Oh yea, when we revert, how do we know they both don't point to the same place?
Yea it makes sense.

There are two datasets both with row id 20!

But the constructor is only called once for row id 20? For dataset?
So what the heck?

Bytheway, it's impposible for reverrting to have two paths revert
the same nodes, becasue for a given timejump there is no joins..
yes. that's true.

com.rareventure.gps2.database.cachecreator.GpsLocCacheRowDataSet@46077580

Ok, expressions and variables agree immiediately after we add the thingy
When we revert it goes to 0
Oh, we are reverting 20
GpsLocCacheRow(startTimeMs=2010-04-20 06:15:43.691,endTimeMs=2010-04-20 06:20:49.126,minLatStart=-25598545,minLatEnd=-25598523,minLonStart=-54575797,minLonEnd=-54575785,slopeLatm=-0.006833,slopeLonm=0.007216,id=20,c1=33,c2=34,tj=-2147483648,numsegments=1,startTimeJump=2564977,endTimeJump=7776000000,widthLonm=12,heightLatm=22,endMinLat=-25600632,endMaxLat=-25600610,endMinLon=-54573593,endMaxLon=-54573581)

ok, chasing after something that shouldn't be chased.
Ok, so fine, so after addPoint, 20 has zero rowdatas.

Ok, at 2335826 during fixdirtytree, it has 1 rowdata, from (start=2564977,end=7776000000),
and no children.
Ok, so lets check that, I suppose.

Ok, because we are "makingParentAlive" and there isn't any live link between 20 and 33,
we don't create this link.
Ah, the 256 vs 233 thing is because we have to make the whole thing alive.
Ok, so that's why there is no parent link. Now, is that a problem?

9/29

Row 20, WorkSegment(start=2564977,end=7776000000)

Child1 :
Row 33, WorkSegment(start=2564977,end=74113780)

Note the above row was created during makeParentsAlive
We did that during fixDirtyTree for a particular time. But
now we are at a different time and ahve the same issue.

--

id 17

 row doesn't have enough time jump power! GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-21 03:43:32.690,minLatStart=-25598139,minLatEnd=-25593516,minLonStart=-54579869,minLonEnd=-54570672,slopeLatm=-0.000950,slopeLonm=0.000887,id=495,c1=492,c2=493,tj=-2147483648,numsegments=1,startTimeJump=77245899,endTimeJump=78559470,widthLonm=9197,heightLatm=4623,endMinLat=-25673493,endMaxLat=-25668870,endMinLon=-54509550,endMaxLon=-54500353)

This is the row

GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-21 03:43:32.690,minLatStart=-25598139,minLatEnd=-25593516,minLonStart=-54579869,minLonEnd=-54570672,slopeLatm=-0.000950,slopeLonm=0.000887,id=495,c1=492,c2=493,tj=-2147483648,numsegments=1,startTimeJump=77245899,endTimeJump=78559470,widthLonm=9197,heightLatm=4623,endMinLat=-25673493,endMaxLat=-25668870,endMinLon=-54509550,endMaxLon=-54500353)

Here is tj start: 78559470

Here is the parent:
494,491,5,-2147483648,1,2022072,77245899,2010-04-20 05:41:52.286,2010-04-20 06:15:43.691,-25598139,-25595043,-54572945,-54570673,-0.000199,-0.002521,

And the other parent
13,491,12,-2147483648,1,717834,2022072,2010-04-20 05:41:52.286,2010-04-20 06:15:34.35
8,-25598139,-25595052,-54572913,-54570673,-0.000188,-0.002539,-25598139 

And the third:
492,491,5,-2147483648,1,77245899,78559470,2010-04-20 05:41:52.286,2010-04-20 06:15:43.691,-25598139,-25595043,-54572945,-54570673,-0.000199,-0.002521,-25598139 -54572945 1271713312286~-25598139 -54570673 1271713312286~-25598544 -54575795 1271715343691

So 494 and 13 shouldn't be parents

13 was the same in 15 as it is now.

13 was originally pointing to 4. We created a RD for it, and point it
to 4:0

And then we do this:
D/AddPointStatus( 2496): inserting id 4~717834-78559470 to r.id, 491

So that's how 491 was created.

Then we do this for the rest of 4.
D/AddPointStatus( 2496): updating 4~78559470-7776000000


Actually, its 495, not 491 which doesn't have enough timejump power, grr

D/AddPointStatus( 2496): inserting id 3~77245899-78559470 to r.id, 495

So here is after the destruction:

495,492,493,-2147483648,1,77245899,78559470,2010-04-20 05:41:52.286,2010-04-21 03:43:32.690,-25598139,-25593516,-54579869,-54570672,-0.000950,0.000887,-25598139 -54579869 1271713312286~-25598139 -54570672 1271713312286~-25673493 -54500353 127179261269

1,2,495,498,1,77245899,7776000000,2010-04-20 05:33:26.470,2010-04-21 03:43:32.690,-25598252,-25525642,-54638682,-54570443,-0.000943,0.000878,-25598252 -54638682 1271712806470~-25598252 -54570443 1271712806470~-25673493 -54500348 1
271792612690~-25673493 -54568587 

That's it

So 495 should have a tj but doesn't. It was based on 3
E/DataSet ( 2496): RD 3,0,2022072,77245899,4:0,5:0,22:0;443:0;447:0;453:0;459:0;465:0;473:0;481:0;487:0;427:0;-80:0;-2:0,null
E/DataSet ( 2496): RD 3,1,77245899,78559470,-14:0,-15:0,1:1;1:1
E/DataSet ( 2496): RD 3,2,78559470,7776000000,4:1,5:0,1:1

hmm same issue

So, still, even though we specify it, the time jump link isn't there. So we got to check it out
again.

Ah it,s there. 3 points to it. But nothing links to 3. 3 seems to be lost.

No there is no 3.
So what the heck happened to 3?

I mean there is one, but no one points to it. 3 has no parents!

Funny, after 15, a huge number of guys are pointing to it as their child.

No timesiblings though

E/CacheViewer( 4250): 1,2,3,430,1,76776272,7776000000,
E/CacheViewer( 4250): 430,426,3,429,1,74744867,76776272,
E/CacheViewer( 4250): 22,21,3,-2147483648,1,2022072,2031405,
E/CacheViewer( 4250): 443,38,3,-2147483648,1,2031405,2334848,
E/CacheViewer( 4250): 447,59,3,-2147483648,1,2334848,2335826,
E/CacheViewer( 4250): 453,88,3,-2147483648,1,2335826,2336840,
E/CacheViewer( 4250): 459,124,3,-2147483648,1,2336840,2387722,
E/CacheViewer( 4250): 465,168,3,-2147483648,1,2387722,2401725,
E/CacheViewer( 4250): 473,230,3,-2147483648,1,2401725,2564977,
E/CacheViewer( 4250): 481,291,3,-2147483648,1,2564977,2599045,
E/CacheViewer( 4250): 487,358,3,-2147483648,1,2599045,74113780,
E/CacheViewer( 4250): 427,425,3,-2147483648,1,74113780,74744867,

Ok, most of the guys probably just go to 3~0, but still..

---


GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-21 03:43:32.690,minLatStart=-25598139,minLatEnd=-25594504,minLonStart=-54577618,minLonEnd=-54570672,slopeLatm=-0.000950,slopeLonm=0.000887,id=3,c1=4,c2=5,tj=495,numsegments=1,startTimeJump=78559470,endTimeJump=7776000000,widthLonm=6946,heightLatm=3635,endMinLat=-25673493,endMaxLat=-25669858,endMinLon=-54507298,endMaxLon=-54500352) 

currworksegment WorkSegment(start=77245899,end=78559470)

---

So the new issue is that one of the datasets have children and the other do not.
However, considering that the other one that does not is basically claiming dead
children, they still may be able to be combined??? Or no?


Ok, so the rule is that we only combine if from the same ds (ie row) or a list of brand
new poof cases.

It's not a poof, so they need to be from the same ds.

Now, row id 1 has a child 3, so, deadly, it does point to the same.

GpsLocCacheRow(startTimeMs=2010-04-20 05:33:26.470,endTimeMs=2010-04-21 03:43:32.690,minLatStart=-25598252,minLatEnd=-25525642,minLonStart=-54638682,minLonEnd=-54570443,slopeLatm=-0.000943,slopeLonm=0.000878,id=1,c1=2,c2=3,tj=498,numsegments=1,startTimeJump=77245899,endTimeJump=7776000000,widthLonm=68239,heightLatm=72610,endMinLat=-25673493,endMaxLat=-25600883,endMinLon=-54568587,endMaxLon=-54500348) 

Ok, so the thing here was that before fixDirtyTree, start row was probably dead. next row was alive, but the change didn't
do anything to it's lstb. It changed 3's lstb, however. But we have a rowdata to support that.

It should be fine to add a tj. Tj's are timeless so to speak. Just because the new rowdata has a live link, and
the start row data has a dead link doesn't mean that they can't be combined. 

So after squish, we go to save. So squish is goign to squish and we'll have this node with incomplete children. When we save...
well.. the issue here is that ok, for dead children we have dead children, but they're all dead. 
Yea, saveSomeData expects them all to be there. So it's almost as if we have to check and if the row that a ds is pointing
to as a child is not dead, then we have to update all the children of all the rd's which defer their children to point
to the correct *children*, as in more than one child.

Ok, we could add this code to squish. I think that fixdirtytree would be better, though. 
So fixdirty tree starts from the top, and hits all the children for each timejump

But of course the dead parents..

I suppose it's possible to squish dead parents.. 

Ok, so what we are doing again, is anytime we have a live link, we must do it. So whenever
a ds points to a live ds, we have update all the children to point to that ds
So after we update, yes we'll have to update the parents. 
We do this because, A) we squish dead parents, and B) savesomedata expects

We already have make parents alive.

I think that, whenever we just have to fixdirtytree on the dead parents
and we'd be cool.

---

Oh, its 495 again... what is up with this id?
 GpsLocCacheRowData2(row.id=495,time is WorkSegment(start=77245899,end=78559470))

3,4,5,495,1,78559470,7776000000

E/CacheViewer( 6549): 1,2,3,498,1,77245899,7776000000

Ok, so the issue with this is that we could try to fix a row of the dirty tree,
and then we try to fix a parent, and reach over the top of it and attack it
from above. this is no good, because although it reports clean, it's really
not yet. So we need to do this the other way.

The thing is though...

So what if we find all the children first. The ones with dead childs. Then
we fix those, only. Then we find the ones with children that are all done,
and do those. So we add the parents into this pool of to dead items to fix.

We need to do this because we can't really go down and then up again or 
we get into all kinds of trouble. So we should be going up only. 

The thing is that a ds can have a dead child and an alive child for
real. And that is an ok thing. 

Which is fine. We only need to repair nodes when the ds points to an
alive node.

Ok, here is what we do. We save all dead parents for try later. Then
at the end we do them.

		//Now we have to fix dirty tree on all the parents. This is to catch the dead parents, which we normally wouldn't
		//get. Then we know the entire tree is completely linked
		//PERF: we could alternately save the ds of each dead parent we make so we can do this. 
		for(GpsLocCacheRowDataSet parentDS : currParentDSs)
		{
			fixDirtyTree(parentDS);
		}
	private ArrayList<GpsLocCacheRowDataSet> extraDSsToClean;
	
		extraDSsToClean = new ArrayList<GpsLocCacheRowDataSet>();
					if(extraDSsToClean.size() == 0 || extraDSsToClean.get(extraDSsToClean.size()-1) != parentData.dataSet)
						extraDSsToClean.add(parentData.dataSet);

Darn, it seems we are already doing this...

Ok, so when we fixDirtyTree for 1, we should be linking it to 495

Oh, so even if we created it, we are only making one of the links live.
So we need to make the other one live too

Ok, so for each child, I mean it could be half dead. So what we need to do
is, if there is no link, or even an incomplete link... well that shouldn't
ever happen. but anyway.. lets just fix em

----

GpsLocCacheRow(startTimeMs=2010-04-20 05:41:52.286,endTimeMs=2010-04-20 06:15:43.691,minLatStart=-25598139,minLatEnd=-25595043,minLonStart=-54572945,minLonEnd=-54570673,slopeLatm=-0.000199,slopeLonm=-0.002521,id=492,c1=491,c2=5,tj=-2147483648,numsegments=1,startTimeJump=77245899,endTimeJump=78559470,widthLonm=2272,heightLatm=3096,endMinLat=-25598544,endMaxLat=-25595448,endMinLon=-54578067,endMaxLon=-54575795)
has no parent

9/30/10

495,492,493,-2147483648,1,77245899,78559470

E/DataSet ( 1159): DS 492,f,f,f,492,491,5,-2147483648,1,77245899,78559470
E/DataSet ( 1159): RD 492,0,77245899,78559470,491:0,5:0,,null


E/DataSet ( 1159): DS 495,f,f,f,495,492,493,-2147483648,1,77245899,78559470,2010-04-20 05:41:52.286,2010-04-21 03:43:32.690,-25598139,-25593516,-54579869,-54570672,-0.000950,0.000887,-25598139 -54579869 1271713312286~-25598139 -54570672 1271713312286~-
25673493 -54500353 1271792612690~-25673493 -54509550 1271792612690~-25598139 -54579869 1271713312286~-25593516 -54579869 1271713312286~-25668870 -54509550 1271792612690~-25673493 -54509550 1271792612690~-25673493 -54500353 1271792612690~-25668870 -5450
0353 1271792612690~-25593516 -54570672 1271713312286~-25598139 -54570672 1271713312286~-25593516 -54570672 1271713312286~-25593516 -54579869 1271713312286~-25668870 -54509550 1271792612690~-25668870 -54500353 1271792612690~~
E/DataSet ( 1159): RD 495,0,77245899,78559470,,,1:0,null

495 is a timesibling of 3, that's its only link.
3 is second child of 1

492 is linked up to 495
492 WorkSegment(start=77245899,end=78559470) parents:[GpsLocCacheRowData2(row.id=495,time is WorkSegment(start=77245899,end=78559470))]

But the parents link disappears

Grr. .no it's 495 with the problem. Not 492!

Ok, what is this alternative idea?

We have a forgetit

GpsLocCacheRowDataSet(row.id=525) is now not havign a parent either

10/1/10

Ok, so do we need timejump granularity of 1 second? Because otherwise
we can never specifically say where you were at a given time.

Ok, so we calculate the following:

1. new parent: area to hold new node

So, when we add a point and make adjustments, we have to see a) how
many points does the row affect? Yea because we are taking an area..

Because if we add a new point, we need to go through it 
whenever we go down both routes. So it's like we are taking another step
for route 1 and route 2.

A max double is 1.7*10^308

The size of earth in lat and lon squared is 129600000000000000

We could use area. We could also use points. There are arguments for both.

But anyway, we are going down two routes. If we use area, I think
that we need to multiply parent * child1 + parent * child2 for this case

So currently

1. new parent
old: parent area
new: (new parent) * child1 + (new parent) * child2
2. add to child (1)
old: (new child size) - (curr child size)
new: 

I think that overlap has to be taken into account.

Ok, so the optimize for lopsided box when detecting overlap is a perf thing.
I mean we'll still consider the lopsided box, but suboptimally.

So what we do is, we keep track of num segment x depth.
And we update that everytime we add a point.

And we add the overlap penalty. So we are keeping a score

What if we do it in reverse.. like we said before, children first.
So we find the closest children, somehow, and add to the one that 
doesn't flip to the next zoom level. If none of them do, then we go
up one "level" and do it again. Yea, that makes sense, because we
want all the points to have the same zoom level

From the zero zoom level, I think that it should work. But it isn't, obviously.
The thing is that overlap shouldn't even come into play at that stage. 

10/2/10

1 depth of tree is not a matter. It is the size of the nodes at a given depth.

And therein lies one of the keys. If a node is below a certain size, it doesn't
matter if it contains 1000, 10000, or 1 million points. It is more advantageous
to have it the way it is, then to create a more "balanced" tree. 

If all points overlap, having a balanced tree doesn't really help much,
since they all have to be gone through, anyway.

Now let's say there is a typical situation of a road, and a person going
over it 100 times. In this case, at a high level we have all these identical
lines. So, when we zoom in, to the next level, we will need to negotiate
a tree....

100+50+25+13+7+4+2+1

of 200 points to get to the next level. But there isn't much we can do about
it. Except maybe split it up in half. So if the road line is 1 mile, then
we split it up into two 1/2 mile sections. So the point is that, we 
would like to reduce the size without overlap as much as possible. Of course
they will overlap. But the point is that when zoomed in, we won't have to
traverse all 200 points to get a lower zoom level. Because, if you consider 
that we will always be a rectangle, then what matters, well, what matters is
how you split up that line. It's difficult to codify, but I think a good enough
approach would be to split up based on square area of the corners of time.
Specifically, we would want to minimize overlap of the *minimum square 
surrounding the box*

There is this other idea I have been kicking around... which is to treat it
like google maps. In other words, because look, we are doubling the number of
points necessary. 

Ok, so what we'd do is have different planes of zoom levels. Then according
to the plane, we create areas, and in each of these areas, we have points.
So that, we could still have a tree like structure, since indexes are out of
the question, at least for the bottom zoom levels. Then we specify at the 
bottom, the points that are visible, and display those points. Easy.

Simple. Understandable. Less risk.

Then there is no containing of points within points. We could more easily have
different types of structures, rather than just lines. Blobs or spots, for 
example. Whatever we could think of.

We could even have squares of higher level zoom levels point to points of
lower levels if the number of points was low enough. Or vice versa as well
if there were too many points. Hmm...
The only thing is, well we are creating a tree of panels... yes it's true
but that would be a known quantity. As long as the user doesn't travel across
the earth, we would be fine. 

At the bottom level, we just specify the points that we contain. Or at
each level. Very simple. Ok, what about the edges? 

At the edges, we, well it's simple. If we create a structure and it overlaps
the edges, we add it to both the current and the adjacent panel.

Oh well, how sad that we worked so long and hard on this tree idea.
But I have to agree that this is a lot easier to understand and will get us
in a lot less trouble. 

It's just that we are so close with this first method. Before scraping it, I'd
like to make sure that we should. 

Because hold on, there is more to this than we first think, because of 
timelevels as well. True, so what we have here is, well, lets say for a single
point:

Let's say our first panel is... hmmm... as long as the tree works, I'd like
to keep it. There is something about it, I think that we can make it work like
this panel.

I mean, because if we get bogged down in the tree, we can stop. If we can
have some sort of marker that tells us how many points we will need to go 
through, we can stop and display the location of the tree where it is. 

I don't like the panel idea, because it just seems so flat and boring. 

Fine but the issue here is that we have blind roots of the tree that may
overlap with each other. When this happens, we have to go down both roots.
Which is true. And this makes it complex and error prone. Hell the whole
thing is.

It's true. Do you know how much debugging I had to go through just to get
it to this state? And we are talking about adding more complexity to it.
A lot more. The problem is that I can't define any of the answers to your
questions. 

The panel idea has it's drawbacks as well. Look, we have to create what

0 + 5 min + 1  hour + 6 hour + 1 day + 3 days + 1 week + 1 month + 1 year
+ inf

10 time points x 8 panels = 80 panels per one point

1, 10, 100, 1000, 10000, 100_000, 1_000_000, 10_000_000

The questions about the tree is:

1. how to prevent too much data?

2. how to prevent overlap?

Cause the thing is, the panel idea solves the overlap problem pretty darn well.

The other thing is that, we don't need to create 80 panels per one point.
Why? Because look, we can start with the universal panel and load those
points. When those points get to be too many, then we can split it up.
It's just that we do so granularly. It would be darn easy to check.. darn
easy to qa. 

Compared to the tree, where the question is:

3. For each and every zoom level and each and every time level, does the
tree perform well enough?

See, for the panel idea, we create subpanels to cut the points in 4, no
8 pieces. We always cut by 8. But we make everything granular. So that
we don't get these weirdly sized boxes. And this is more efficient, because
there is no need for a weird zoom level. But I suppose we could even 
experiment with doing that. 

So let's try the road experiment with the panel.

Well, for the zoom level of the artifact, we only have 1 point. 
When we zoom in, we have 100s of points, it's true. If it gets to the
point where we can not display the 100s of points, we link the cache to
the next higher artifact where we only have 1. When we zoom in far enough
where we don't have 100s of points, we can then again display the points
we have. Hmm... the only thing is that, the tree method, we could display
the same thing, only that we could then work on actually displaying the 100s
of points. So that if the user sat there long enough, we would actually display
what they want to see.

FACT: The reason we arent using the tree code: 
True, but the problem with the tree method is that if the user zooms past 
this bundle into an individual point, guess what, we will have a hard time
getting there, because we have to go through the cluster to get to the 
individual point. See.. it's bad. And we can still work on it with the panel
method. 

The funny thing is the panel is actually a tree. Darn man. I wish I thought
of this earlier.

So what we have here is a cache that can handle panels, and points.

We have 

Panel:
int id
int [3] start
int [3] end
int [8] subpanels
int ... points

We have two types of caches, for panels and for points, or maybe artifacts?

LopsideBox (within Artifact)
int id
int type
int [3] start
int [3] end
int num_points (*future*)

There is a problem here... what do we do when we have infinite time?
Because if we create children as specified, when we lower distance, we
lower time, too. Although in reality, time is independent of distance.

So then when we are zoomed in really far with infinite time... we have
a ton of points on a busily traffic'ed area.

I wonder, what if we ignored time when creating points. The only problem 
there is that then we have all these points that we cannot join up.

So then the option is that we still have point trees. But point
trees in time. Or what about a time graph... So we just store each
time the item appeared. Although it's really a range, or a set of ranges.

Then things get weird, because at a certain time, yea yea... I think that's
ok.

Then we no longer need lopsided boxes. Just lines, with widths, so flattened
lopsided boxes.

So then the only trick is what do with the time ranges. But if you think,
it's just the same problem as the panels, only easier, because there is
only one dimension.

So we don't have to worry about empty space for time because we do the 
same trick as with panels. That is we only create time range "subpanels"
if the number of time ranges is too high.

So there you have it. 

Ok...

So, we want to add to the children first, since I think they would
pick out a point better than the parent and sometimes they share.
Hmm, nah lets just make it simple.

FACT: Keep it Simple! (the tree thing was a fiasco!)

There is one creeping suspicion I have. And that is a certain level,
at a certain place, lets say "home" there will be thousands upon thousands
of points. And it won't matter how far you zoom in. In that place, it will
never make any sense to try to cache at infinite time. We'd have to zoom out
to the point where the points squish enough to be displayable. 

But the point is that at an individual time, we will want to display
points. So lets say we are at "home". 

And at the lowest level, or at, let's say a low level, we get too many points.
Well the current plan is to use the upper levels points and punt to the lower
level to display them. Which is fine. But what about when we are at the lowest
level. There definitely could be 1000s of points at the lowest level if 
the user sits in his easy chair every day for a year.

We can't punt here, because it would be nice to see when the user got up to
take a pee break, get a sandwich, etc. 

So what do we do? Well, we need to split up by time early. What if we split
up by time when we punt? So when we get to 100 points or so per panel, we 
split by time at that time. A time sibling! Of course!
So then we just look at the first and last point of each panel to determine
the times. But this isn't a time jump sibling, it's a time sibling. 

So when we punt, we don't clear out all the data, we just create a timesibling
and keep on adding. Points will never jump between time siblings, and that
way we can slowly display more detail, even in the face of 1000s of points.

The time sibling will go backwards, because we are probably more often
interested in fresh data.

Ok, so we got two things here:

1. We want to use tiny points at high zoom levels if possible, so that
we share them, because why not? That isn't a reason.
2. We don't want to create sub panels if we don't have to.

They are about the same, arent they. 

So when we create a hotspot, that is a point that is at the current zoom level
we will need to create sublevels. 

But actually, suppose that we even create a level 2 zoom, we would need to
be able to zoom in to level 1. 

So should we just create panels and have them independent of depth?
Yea, maybe.

So whenever we add a point to an existing cache item, we ... well...

Because if we have 100 points. I mean maybe we want to 

I mean ok, if we have a level 1, we can display it at panel 6, fine.
If we have a level 2.. we can still display it at level 6, but do
we create all the way down to level 2?
I mean I suppose, if it was possible to create a level 2 panel without the
intervening panels, that would make sense. 
Because the point is for level 5 panel you'd never want to create a level 1
point if there is a level 2 point available.

10/3/10

So here are some options:

1. Create a level 2 panel in the right place. We can link children panels
up anyway we want, so this would work.
2. Create a huge level 2 panel that we would move up to a level 5 when
necessary.. no this doesn't make sense.
3. Create the entire tree downwards. I don't like this, because it seems
a waste of space and memory

So let's say we have a level 1 point at level 6 and we combine to make
it a level 3 lets say. Then we want to make two level 1 panels (or one
if they are close enough) to store the level 1 points.

So in other words, we've handled up to level 3 when we add a level 3 
point. Whenever we change a point without changing panel info, we do nothing,
except deal with overlap which we will have to do later.

But then, we still need to handle levels below that,
it's like, so there isn't panels 5 and 4, thats ok. It's like they are there
in a ghost form. They are recreated when we handle panel 6.

So it's like this, as the point goes down each panel, we check each panel
to see if we can add to an existing point. If we find one, we basically need
to delete it and re-add it to the panels. And so we readd it to any existing
panels that are already there, but only go down to it's level. So if it's
new level is 3, then we go down to 3.

So the point splits at this point, any panels below this level that contained
the point still get the original one, whereas the panels above or equal to
level 3 get the new point. Then we simply add it to the children below
and repeat the process.

So here are the steps:

1. At top panel. Look for an existing point.
  a) if we found one, follow it done through the panels that contain it.

There is a problem. There is nothing to prevent a child from going across
a border that is not one of the 4 children of the immediate parent.
So, unfortunately, we have to somehow find all the children that extend
into the range of the point

10/4

Ok, so what about finding individual points. In other words, 
if we go to a specific time at a specific place, there can be a lot of
points to go through. We could create a time sibling tree to handle this.

So to find an individual place and individual time, we :

There is something that bothers me... we are basically dealing with time
twice. Once for overflow in the panel, and again at the point level.

See, in reality, the panels and points are the same, except that panels
can contain points. Points can contain time ranges, I suppose... well
we were saying they can contain time range panels.

Anyway, to find a specific place and time near "home" we
1. go through the area panels to the place, home.
2. go through the time sibling tree to find the individual panel
3. go through the points of the panel
4. go through the time ranges of each point to find the specific one
we want.

To find a specific place near "home" with infinite time, we 
1. go through the area panels to the place, home.
2. if the area panel overflows (yes it will), we use it's parent to
initially draw
3. get all the points and draw everything


AreaPanel
Point
TimePanel

We are displaying a panel... not a point.
So each panel should have the loc affect it once and only once.

Even if a point is removed and not readded to the same panel,
it should still not be touched again.

So the game is as follows:

1. We start at the top and try to add a point. Either
   we find an existing point to add it to, or,
   create a new one.
2. If we find an existing point, whereever we remove or add the
   new replacement... wait.. what if the replacement is at a higher
   zoom level then the one removed?

Also, the point may cover several panels... so..
ok, every panel covered by the point must be updated, once and only once
also, if updating the point spills over to another panel, then that panel
must also be updated once and only once.

But the thing is, that if only the point spills over and the loc is not
involved, then the panel is not going to be touched by the loc, and therefore
is a bystander in the process. It will not be touched twice, unless...

The same loc is added to another point of greater depth and that panel happens
to contain overspill from both. 

So points themselves have parent / child relationships. If a sub point of a 
point is not a child, it cannot be updated by the same loc.
The other thing to consider is that when creating a new point, it may happen
to overlap other sub points, which should be included in it.

And if we think of it that way, can we eliminate panels completely? 
So the question now becomes, when to create a super point?
Because if we do just when we have to, well we can go back and find probably
better overlaps then before.
Ah... but currently we are doing it this way, finding superpoints immediately.
And that makes a lot of sense.

So maybe after changing the points, we find a home for them?
Hmm, but the existence of the points... no.. the existence of the panels
depends on the points

So lets say that we just worry about the points. So we just use the panels
to find the points that may overlap... Huh.. then we modify them, and
replace them in the correct panel(s).

And considering the rule that every child point we find must match a parent
point or... well if there is a parent, it must match it.
Yes. of course.

So that leaves us to these modified points. Now the rule is that we must
go all the way done to depth level 0.

1 So we start at the top, and find the point with the highest depth level
to match the loc. If we match it, then it becomes the immediate *parent*.
2 Then we go find the next point that is a child of the immediate parent and
matches the loc. 

3 If there is none, we create a new point, and hook it up as an immediate child
of the parent, and exit, or
4 If there is one, we add it to the child.. 
  a) if the child becomes so fat that it becomes the same depth level 
     as the parent, the child is destroyed and all it's children must be
     linked to the parent
  b) Then, we set this guy as the immediate parent and find the matching
     child again, back to step 2

Now we have a list of changed points, the old ones and the new ones.
For each of these points:

----

The problem here is that the panel may have a point that gets too fat 
for the panel. So, in that case, we really don't want to add the loc
to the point. So if the point becomes too fat and bumps up a level,
we really need to create a new parent for the child but leave the 
child alone.

So we can only change a point as the result of a loc being added if
the resulting depth doesn't become too high. Well, if it does, we
create a parent for it. If that parent already exists, then it's
the responsiblity of the parent to handle the loc...

So a point can't change depth levels. If a point gets a loc that would
change it's depth level, we make a parent for it... but as soon as 
a parent states it cannot handle a loc, then all descendents of the point
must refuse to handle the point as well? Yes, it would make
sense, because a parent should always have the same loc's as the children
it represents. So even if you had a level 6 and a level 4 point, and 
the level 6 parent refuses the loc, we can't make a level 5 point that
handles it with the level 4 point as a basis.

So that leaves us with the following.

1. At the top most level,
2. Find all points that can match the loc. Find the best matching point
 that can match the loc. That becomes the *parent*, or
   a) if there are no matching parents, create a new point and exit
3. Descend to the children using the panels to find them. (or another way
depending on PERF) 
4. For each child that is an immediate child of the *parent*, find how
   well it matches
   a) CHOOSE the best matching child... only one may be chosen
   b) if there is no matching child that can at least stay below the parents
      depth, create a new point, set it as an immediate child of *parent*,
      also create a new point at level 0 (as a child of the parent) and exit
   c) "mop up" any other points. So after creating this monster point with
      child and parent, go ahead search again to see if we can create a 
      even bigger monster. If we can, replace the current monster if at 
      the same depth, or create another point that will be a parent of the
      current monster, and do this recursively
5. If a best matching child was chosen,
   a) if the resulting child created would be at a higher depth level than
      the original child, then create a new point, assign it as the immediate
      parent of the child (and *parent* as the immediate parent of it)..
      if there is a monster chain, hook it up to the child and the *parent*
   b) otherwise, modify and replace the child itself
   Finally set child as *parent* and return to step 3

So in the above,
1. one or more points can be modified, retaining the same depth and/or
2. a single new point can be added.

So all points must be added to panels of the depth or less.

So to start out with, we have one top level panel to start with.
Since it doesn't have a time range, and it has less than max points,
we just add the resulting level 0 point to it.

Hmm... see the thing is we don't want to create 1000s of panels to handle
a flight from BA to Rome. So we need to do something like this.

So then the point (sorry the overuse of this word) is, that suppose a point
gets a new parent. Then if we have a level 6 panel, we move that panel to
the parent. Then we have to create an additional panel to handle the child.

So the point is that each panel must always have the highest point in
the child to parent chain. Also, suppose child x is in a panel, and the parent
of it is not. It still doesn't matter. In that case, child x is removed
*and nothing is put in place of it*

Fine, so we have panel 6 and a point. Now we create a subpoint. Do we create
a new panel? Yes.. so the entire area of the point must be covered by at
least one panel. Now panel choice. Let's see... there really isn't any..
I mean, yes we will display a level 1 point in a level 6 panel, but
if a level 2 replaces it, we then move up to the level 2, and all the children
are moved out of that panel.. but unless we mop up children... well we should
know, even if we do mop up children. 

So we mop up all the children for this new super point.. but mop up how?
Yes, because we know we have to stay under the mother parent's depth.
And also, it sort of doesn't make sense for a parent to have only one child.
That would be weird. Right.

So what is the point of having multiple depths. Well, we should always have
as many depths to choose from as possible. That is the goal. Like if we 
have a depth of 6, and then a depth of 2, and we add a point, and it mops up
another point, we could create depths of 3 and depths of 4 and that would
be fine.

So the point being, a) when we create the points, we need to keep in mind
that we may be able to create muliple parents when the need arises, and
b) we need to make panels for each of the depths.

Yes, so lets say we have a level 2 panel and a level 6 panel and we 
create levels 3 and 4... So the thing is that we have to add the new point,
as well as all children points that overlap to these new panels... hmm...

Yes, that is the point. We have to copy the points from all immediate children
to the new panel. It shouldn't be a big deal, because we should be mopping up
points very quickly. So there shouldn't be many stragglers.

Ok, so after we mop up all the points, and create all the new points, we then
have to open up the hood and take a look to determine which panels we have
to create. We go through each point. For all modified points, we delete the
old ones and readd them. We do this because the old point might be replaced
by a new point. And that new point may not cover the same panel the old one
did. So during this stage... ok ... here is the main point

Another point is that we have to examine all adjacent points, even those
in different panels.

So the cardinal rule here is that a point in some representation has to
appear on every single panel that is covered by it. in other words,
if there is a parent which represents 1000 points at the lowest level,
then for each and every panel
1. the parent must be present, or
2  a cross section of children must be present, or
3. the panel doesn't overlap at all at it's chosen level.

Now here is the deal, the chosen level for a panel is always the highest
depth of the point chain that equal to or under the panel depth.

Then we go to that depth, and make the necessary changes for that panel.

So then, we will possibly have some children that are remaining that have
no panel. So then what we do is create the largest panel we can?
Yea I think so.

Hmm.. that changes things. The thing is that, look, is the reason we don't
create the largest panel? Should we always create the smallest panel?

No, I think it's a waste to create the smallest panel, because when you
zoom in, you have to zoom in to something. And if there is no level 5 panel
from a level 6 panel, where are you going to go? You can't go half way between
level 6 and level 1... That's true. 

So in that case, the panels, they only go down when necessary. Aha! Changes
a  lot. Simplifies a lot.

So when we start at level 6, ok, so it means that any panel we create will
not already have children. Aha! Ok.. Saying "Aha!" isn't making me come up
with a solution any faster, BTW.

So we still deal with the points the same, trying our best to create 
small and large parents to handle every single zoom level with as few points
as possible. 

Then, after the dust has settled. We take a look, and if there are any 
points that don't have all their ... hmm so if a point has half it's
area covered by a level 5 panel, then.. ok
..

so each point has to be represented somewhere by some panel. Right.
Then when we create a new panel for points that don't yet have a panel,
we place it under... well, what if it only covers half?
Because like you said, we can't mix panels when we display. So if it covers
half, we have to generate panels for the rest. In that case, we could
have panel gaps. Ok, that should be very unlikely that the children will
cover area outside their parent, but if it does happen (since we are allowing
this case to be flexible...) hmm should we allow it? Well anyway,
we should just create the parent panel anyway if we do.

Yes, so then each point is represented by a *depth*, and then all panels
at the depth are created 
(and any parents that aren't there must be also created and left blank to be 
clean for this edge case which we may not even support...)
So we assign a rep depth to each point based on it's panel. So the parent's
rep depth makes the child rep depth one less. Then we create or use the
panels that are already there to represent it.

So the point is also that when we create a parent, we delete the child
from all of the panels, then add them both as new points. So when we do
that, we simply look to the parent to find the maximum rep depth +1. Then
we add to a panel at the max rep depth. If one doesn't exist, we create it
(and all parent panels if needby or assert fail)

We also delete the child whenever its dimensions are modified so they can
be re-added. Very good.
So there you go...

Well, except one thing. When we add a panel, what about all the other points?
Aha! Yes, right-o. Because when we are forced to create a subpanel, we need
to copy all the relavant points from the panel higher up to it.

So after the delete step, we create the new panels, and copy all the points
to it. Then we can add the new points. So points can exist in multiple panels
So when we delete them, we need to remove them from all panels? Yes. because
we are modifying or modifying and/or adding new points.

So when we look at the parent to find the maximum rep depth, we have to consider
that the parent is in all the children.
Well not all the children. because if the parent has children, then they
must be represented, but hte parent will be in all the inbetween nodes..
hey, I wonder if we can carry along the nodes from parent to the child
without having... no, that would be a pain, we'd have to go through all
the parent nodes everytime... hmmm.. because otherwise, we need to link
directly to them. Because they could be cached... because when we pan,
we most likely would be retrieving nodes that could be partially covered
by the parent. The other thing is what if we pan off to where a subpanel
doesn't exist? Well, the only way that could happen is if the parent 
contains only nodes of itself or lower, so there is no problem, we just
scroll partially on the lower and partially on the upper panel.

So anyway, we have the panel, with the nodes, and we go all the way down on it.
So that if we have a point level 2, and a panel level 6, we go all the way
down to panel 2 (if they exist). So whenever we create a new panel, yes,
we copy all the existing nodes from the upper panel. If the upper panel's
node depth is too high, that is an assert error (because we should have 
created a subpanel for the children of that node already). Which brings up
another point. If a parent node covers an area that is *not* covered by
children, we still need to create a subpanel for it, and not give it any nodes.
Right, because when we scroll over to it, we can't display half of a larger
node. Bad... So the thing is when we pan, we can assert that there are no
points that we have to display that are higher than our zoom level.

So anyway we do this copy, right. So it's not a big deal, we just use
the maximum depth as the rep depth for each point. Most of them will be
brand new anyway, but for the ones that aren't this is true.

---

I think that we may want to directly link from parent to child. Here is the
thing, if we have one variable list, we might as well have two.

We still are going to need a damn time hierarchy for each point. 
Which means that we will basically need... hmmm... maybe not. Because of
course the bigger points will have less gaps in their time ranges. Yes...
that's right. Good.

So we have time hierarchies for panels as well as points still. Do we?
I wonder, if we have points having children, do we need children parent for
panels? A "tree hierarchy" is like an index, as well. I wnoder if we could
abstract it.

Yes, we need parents for panels, because if a panel contains no points...
Unless we do it by lat and lon.. but that's what we *are* doing.
It's just that, if we have the top panel, and we have the points from that...
I mean, we know where the other panels are, based on the points, we have
to look up the points anyway...

So then you are saying we start at the top, at the "parent" points.
And find all the parent points that overlap the objective, and then
go down to the children. Find all the children that overlap, and so
on and so on. Till we get to the bottom. In this case, we have no
need for the panels at all.. However, the issue here is volume.
We'd have to go through as many points as there are at the top level,
which may be a ton. We need a panel, cause we need an "index" into
the location of the points. 

And that is the key, that is all a panel is. An 2way tree based index.
Or close to it. If we had an index on startlat and start lon, and end lat
and end lon, would that be good enough? And maybe time, and depth?

How weird, simplifies everything, doesn't it. Yes, we still need the tree
of points, per se, because we need to show things at different depths, so
maybe cancel out the depth index. But...

No, I'm unsure how that would work...

I just wonder, what if we have no panels and cache the results in memory.
So yea, at first it takes a while to solve. But we save all the closest points
to where we're at. Then when we pan, we .. bla.. so complicated.

So then, we either have the condition, that the parent must contain the range
of all it's children, or, that we can define that range. Or we have these
parent to child links. It doesn't seem that big of a deal.

The time tree urks me though. It's just that if we have a road the user goes
down every day, there will be all these pieces of time. 100s, thousands maybe.
And we need to create these thousands of time ranges for each point on that
road, which may have 100s of points. So then, that equals hundred thousands of
time ranges for just 100 points. We could go in reverse, I suppose, no...
that would be bad as well. 

10/5

We'll have parent to child links although we don't need them. The reason
is that the search for children may be complicated since it'll involve 
multiple panels, we'll have to be smart about the "home" case, etc.

Ok, so we added points, now what about the panels??

Lets see, for every point, we have to 

So we have two options for panels:
1. Use fractions or doubles to determine extent.
2. Make them integer based and have the biggest panel a little bigger than
the world. I think that option 2 is best because we have to handle
the biggest panel differently anyway due to points overlaping east thru west

We can also have a different number of subpanels than 4. We could have 9 for
example... I'd like to be able to choose the number of subpanels. Because
this directly affects the depth
With 9, lets see.. it won't match exactly. Well that's kind of a pipe dream

9 * 3 ^ x = 360000000

log(9) + log(3) * x = log(360...)

x = (log(360000000) - log(9)) / log(3)

10/7

/is this a cross product?
   // Calculate the Right Vector. Use cross product of Out and Up.
   ViewRight[0] = -ViewOut[1]*ViewUp[2] + ViewOut[2]*ViewUp[1];
   ViewRight[1] = -ViewOut[2]*ViewUp[0] + ViewOut[0]*ViewUp[2];
   ViewRight[2] = -ViewOut[0]*ViewUp[1] + ViewOut[1]*ViewUp[0];

 
So then,

   rx = -p1y*p2z + p1z*p2y;
   ry = -p1z*p2x + p1x*p2z;
   rz = -p1x*p2y + p1y*p2x;

So then, let's test this

First, undefinable, (1,0) against (-1,0)

rz = -0 + 0;

Good

ok, Then (1,0) against (0,1)

rz = -1 + 0

ok

And (1,0) against (0,-1)

rz = 1 + 0

excellent.

Ok, a real test

(4,5) against (4,3)

-4*3 + 5 * 4 = 8 

Good. So positive is clockwise.

(4,5) against (4,6)

-4*6 + 5 * 4 = -4

Good. it seems right, ok, one more chance

(-4,5) against (-4,7)

-(-4)*(7) + (5)*(-4) = 8

Yea, its good. 

-- dist to point , and location on the line

r* = reference point
s* = slope
f* = result of function
p* = point

fx = sx * t + rx
fy = sy * t + ry

(ds = distance squared)
ds = (px-fx)^2 + (py-fy)^2

minimize over t

ds = (px - sx*t - rx)^2 + (py - sy*t - ry)^2

d(t/ds) = 2(px - sx*t - rx)*(-sx) + 2(py - sy*t - ry)*(-sy)
0 = 2(px - sx*t - rx)*(-sx) + 2(py - sy*t - ry)*(-sy)
0 = (px - sx*t - rx)*(-sx) + (py - sy*t - ry)*(-sy)
(px - sx*t - rx)*(-sx) = (py - sy*t - ry)*(sy)
-px*sx + sx^2*t + rx*sx = py*sy - sy^2*t - ry*sy
sx^2*t + sy^2*t = py*sy - ry*sy + px*sx - rx*sx
t(sx^2 + sy^2) = py*sy - ry*sy + px*sx - rx*sx

t = (py*sy - ry*sy + px*sx - rx*sx) / (sx^2 + sy^2)	

fx = sx * (py*sy - ry*sy + px*sx - rx*sx) / (sx^2 + sy^2) + rx
fy = sy * (py*sy - ry*sy + px*sx - rx*sx) / (sx^2 + sy^2) + ry

ds = (px - sx*t - rx)^2 + (py - sy*t - ry)^2

fx = (sx * py*sy - sx*ry*sy + px*sx^2 - rx*sx^2) / (sx^2 + sy^2) + rx
fx = (sx * (py*sy - ry*sy) + (px - rx)*sx^2) / (sx^2 + sy^2) + rx
fx = (sx * sy * (py - ry) + (px - rx)*sx^2) / (sx^2 + sy^2) + rx



x^2 = 2x

(x+x)^2 = 2*x*2

---

10/8/10

just to verify the slope of the fatness vs the normal line

   ViewRight[0] = -ViewOut[1]*ViewUp[2] + ViewOut[2]*ViewUp[1];
   ViewRight[1] = -ViewOut[2]*ViewUp[0] + ViewOut[0]*ViewUp[2];
   ViewRight[2] = -ViewOut[0]*ViewUp[1] + ViewOut[1]*ViewUp[0];

slope = l/w
fatness slope = -w/l

   ViewRight[0] = -l*0 + 0*(-w);
   ViewRight[1] = -0*l + w*0;
   ViewRight[2] = -w*(-w) + l*l;

Glad I checked it.. ok

So, lets take the cross product of w,l,0 and 0,0,1

   ViewRight[0] = -ViewOut[1]*ViewUp[2] + ViewOut[2]*ViewUp[1];
   ViewRight[1] = -ViewOut[2]*ViewUp[0] + ViewOut[0]*ViewUp[2];
   ViewRight[2] = -ViewOut[0]*ViewUp[1] + ViewOut[1]*ViewUp[0];


   ViewRight[0] = -l*1 + 0*0;
   ViewRight[1] = -0*0 + w*1;
   ViewRight[2] = -w*0 + l*0;

Which leaves

   ViewRight[0] = -l
   ViewRight[1] = w;

Haha ok. So let's check it again

   ViewRight[0] = -l*0 + 0*w;
   ViewRight[1] = -0*(-l) + w*0;
   ViewRight[2] = -w*w + l*(-l);

Ok, so I guess it's right.

l, -w I think
Yes, thats it, that's the fatness, (l, -w)



10/9/10


PointDiamond covering code for integers

		//		\ x /
		//		 \ /
		//	ok    +  ---> point diamond
		//		 / \
		//		/ x \
		//
		//  when we round, we need to get into the ok zone (assume the angles are 90 degrees)
		//
		// let dd be the distance in the y direction going down because of the rounding of latm (as a negative value)
		// let du be the distance in the y direction going up because of the rounding of latm
		// 
		// now, to get in the ok, zone, if y = d, then dist in x <= (-dd)/w*l or (du)/l * w
		// so, to determine which one is better (one could be really bad with a small l or w)
		// we can do some algebraic manipulation, and compare 
		// (-dd)*l*abs(l) to (du)*w*abs(w)
		// I don't think the abs comes into play. Because if it did, then having a small w and a large l could
		// return the dd as the best choice, which it obvious would not (since we divide by w)
		// I don't really understand, but I'm going to take away the abs
		
		//FACTS: 
		// 1. du/dd gets multiplied by the opposite direction it's going in if it's using the line, 
		//    otherwise by the same direction it's going in
		// 2. the bigger item should be the bottom of the division
		// 3. if using the line, should divide by the direction of du/dd. Otherwise should divide by
		//    the fatness
		// 4. The fatness line has the slope reversed and is negative. i.e if the slope is (w,l), then the
		//    fatness line is (l,-w) -- that will be clockwise
		
		int iLonm, iLatm, iLonmWidth, iLatmHeight, iFatnessMd;
		
		if(lonmWidth >= 0)
		{
			if(latmHeight >= 0)
			{
				//du is using fatness
				double du = Math.ceil(latm) - latm;
				//dd is using normal
				double dd = Math.floor(latm) - latm;
				
				//we want to go negative in lonm
				
				//if the adjustment going up in the lat direction provides a better diamond enclosure
				if(du * latmHeight * latmHeight < (-dd) * lonmWidth * lonmWidth)
				{
					//we use the fatness line
					iLonm = (int) Math.floor(lonm - du / lonmWidth * latmHeight); 
					iLatm = (int) Math.ceil(latm); 
				}
				else //use the normal line
				{
					iLonm = (int) Math.floor(lonm + dd / latmHeight * lonmWidth); 
					iLatm = (int) Math.floor(latm); 
				}
			}
			else //latmHeight < 0 && lonmWidth >= 0
			{
				//du is using fatness
				double du = Math.ceil(lonm) - lonm;
				//dd is using normal
				double dd = Math.floor(lonm) - lonm;
				
				//we want to go positive in latm
				
				//if the adjustment going right in the lon direction provides a better diamond enclosure
				if(du * lonmWidth * lonmWidth < (-dd) * latmHeight * latmHeight)
				{
					//we use fatness
					iLatm = (int) Math.ceil(latm - du / latmHeight * lonmWidth); 
					iLonm = (int) Math.ceil(lonm);
				}
				else
				{
					//we use normal
					iLatm = (int) Math.ceil(latm + dd / lonmWidth * latmHeight); 
					iLonm = (int) Math.floor(lonm); 
				}
			}
		} //lonmWidth >= 0
		else { //lonmWidth < 0
			if(latmHeight < 0)
			{
				//du is using fatness
				double du = Math.ceil(latm) - latm;
				//dd is using normal
				double dd = Math.floor(latm) - latm;
				
				//we want to go positive in lonm
				
				//if the adjustment going up in the lat direction provides a better diamond enclosure
				if(du * latmHeight * latmHeight < (-dd) * lonmWidth * lonmWidth)
				{
					//we use the fatness line
					iLonm = (int) Math.ceil(lonm + du / lonmWidth * latmHeight); 
					iLatm = (int) Math.ceil(latm); 
				}
				else //use the normal line
				{
					iLonm = (int) Math.ceil(lonm - dd / latmHeight * lonmWidth); 
					iLatm = (int) Math.floor(latm); 
				}
			}
			else //latmHeight >= 0 && lonmWidth < 0
			{
				//du is using fatness
				double du = Math.ceil(lonm) - lonm;
				//dd is using normal
				double dd = Math.floor(lonm) - lonm;
				
				//we want to go negative in latm
				
				//if the adjustment going right in the lon direction provides a better diamond enclosure
				if(du * lonmWidth * lonmWidth < (-dd) * latmHeight * latmHeight)
				{
					//we use fatness
					iLatm = (int) Math.floor(latm + du / latmHeight * lonmWidth); 
					iLonm = (int) Math.ceil(lonm);
				}
				else
				{
					//we use normal
					iLatm = (int) Math.floor(latm - dd / lonmWidth * latmHeight); 
					iLonm = (int) Math.floor(lonm); 
				}
			} //latmHeight >=0
		} //lonWidth < 0


Somehow we have the deal of having parent to kid links. That won't work if there are level 2 to level 1
links, since there could be thousands of kids for one parent.

I'm thinking that maybe we don't need parent children links. Just add the points willy nilly. We just
have to get the panels of points that may have the thing addable. Maybe look at the primary panel first,
and then if we cant find a match, check the nearby panels.

We won't have to worry about too many points to display, because time panels help with that.

Then the only concern is when a point is too big, and we display the whole thing in entirety, when really
we want just a subset for a particular time.

Now, I think we will defer this problem until we see it. There are several different ways to handle it.
It's a mess to deal with in the time panels. But without it.. all we need to do is find the point to
add it to.

Now, the thing is, can a point ... wait, ok, so what do we do?

For top level, we just search for a point, if we find it, we modify it, otherwise we add it anew.

Now, anytime we modify a point, we see if it is greater than depth level 0. (which it will be)
and then we create a subpoint. So the deal is... wait.. so are we creating a huge level 5 panel
or a level 1 size panel with a depth of level 5? Just enough to contain the point, or a lot more?

It's too complicated to create a small panel, so we'll create the largest.. but then... what if
we get a ton of points that can't be combined, which makes the level 5 panel way too big?

Besides, it would make sense to use as much of the separation as possible. Fine. That'll work
then, so we go deep. So we make or extend a panel to cover the new point.

Or we can go super deep and create tiny panels. But it seems like overkill. It's like each panel
should have a set number of points. That would work, except that we need to push points down
to make room. And we don't know what points to push. Because certainly we can push down the
points that we are combining, but once we go down more than one level, we can't do that anymore.

But the thing is, we will no longer need to push once we get to the bottom.

Well wait, because we are just creating an inbetween level. Instead of "pushing down", we
just smash a panel into it's children, and if it had children, smash those, too, all the
way down.

So I think we'll have a threshold of points. If we go over that threshold, we smash
the panel. But also, we smash the children if we need to create a subpanel..

It's like this.. panels have depth, but a panel can have a lower depth if there is few enough
points to justify the upper depth. 

So each panel has it's "depth" which is equal to the max depth of the points and the number
of descendent levels? No, just the max point depth. It also has the max depth it can 
represent, so a panel has "depth" and "max depth".

Then we start out with the first point added to the top level, creating a depth of 0
and a "max depth" of 6 (or whatever is the highest).

Then we add a new point, and see that it is combinable with the one already there.
So we smash the top level into a max depth of 5, with a depth of 1.
We create a new 6 with all the same points except the combined (which in this case
leaves a blank panel).

So then we get a third point. And that is also combinable with the monster 1/2 point
So we get

hmm so a panel has multiple max depths at once, depending on the area being covered.
So when we smash, we do a partial smash. Only the area that is covered by the points 
gets smashed.
Ok, lets try it.



So the extent of the combined points of the upper panel determines which subpanels to
check.

We have to test all panels, but only the latest time sibling? Or maybe the last two?
No, because if we were ever there before, we will want to combine. 

Ok.


//monster chain
		//find the best child to add to the current point
		// and create a monster chain of parent of parent and parent of parent of parent and so long,
		// by mopping up existing points that fit into the currently created point
		// as long as we stay under the parent depth and we are only looking at children that are children
		// of the parent, we can do this.
		ArrayList<Point> monsterChain = new ArrayList<Point>(1);
		
		while(true)
		{
			Point bestPoint = null;
			int bestScore = Integer.MAX_VALUE;
			
			for(Point p : parent.getChildren())
			{
				int score; 
				if(monsterChain.size() > 0)
					score = monsterChain.get(monsterChain.size()-1).getAddScore(p, modifiedParentPoint.getDepthLevel() - 1);
				else 
					score = p.getAddScore(r1,r2, modifiedParentPoint.getDepthLevel()-1);
				
				if(score < bestScore)
				{
					bestPoint = p;
					bestScore = score;
				}
			}
			
			if(bestPoint != null)
			{
				Point modifiedPoint;

				if(monsterChain.size() > 0)
				{
					modifiedPoint = monsterChain.get(monsterChain.size()-1).combineWithPoint(bestPoint);
					//if they are at the same depth level, replace it
					if(modifiedPoint.getDepthLevel() == monsterChain.get(monsterChain.size()-1).getDepthLevel())
						monsterChain.set(monsterChain.size()-1, modifiedPoint);
					else //otherwise we have to add a new guy to the chain
						monsterChain.add(modifiedPoint);
						
				}
				else 
				{
					modifiedPoint = bestPoint.combineWithLoc(r1,r2);
					monsterChain.add(modifiedPoint);
				}
			}
			else break;
		}
		
		//if we found no child to add the new point to
		if(monsterChain.size() == 0)
		{
			Point newPoint = new Point(r1,r2);
			newPoint.insertRow(GTG.db);
			modifiedParentPoint.addChild(newPoint);
			
			newPoints.add(newPoint);
			return;
		}
		
		for(int i = monsterChain.size()-1; i > 0; i--)
		{
			Point monsterParentPoint = monsterChain.get(i-1);
			Point monsterPoint = monsterChain.get(i);
			
			if(i != monsterChain.size()-1)
				monsterPoint.insertRow(GTG.db);
			
			monsterParentPoint.addChild(monsterPoint);
			newPoints.add(monsterChain.get(i));
		}
		
		modifiedParentPoint.addChild(monsterChain.get(0));
		
		modifiedParentPoint.updateRow(GTG.db);


	// a map of old -> new modification of points during one add segment call
	private HashMap<Point, Point> modifiedPoints;
	// new points added during one addSegment call
	private ArrayList<Point> newPoints;



solve t = (a*b - c*b + d*e - f*e) / (e^2 + b^2), g = e * t + f for g 

solve g = e * (a*b - c*b + d*e - f*e) / (e^2 + b^2) + f for g 

10/10

Ok, so when we modify a point, we have to figure out which subpanels can use our
already modified point, and which can't. If there are none that can't, we just 
update the current row (or delete it and create a new one)
Otherwise, we create a new row.

When ever we go down a level, we are always mucking with one point. So, we go 
down... if the previous point works, we're cool. If not, we save it as a new point.

So we save the merge point right at the start. If it raises the depth level, then
we add it as a new point, because by definition, there will have to be a panel
out there that requires the old one. Otherwise we just modify it.

So lets start over... ok

So when we start out, with 0 points, there is no level 6,5,4,3,2, or 1. Just a level 0.
Adding the first point, we still only have a level 0.

When we add the second point and it merges with the first, we now have a level 1 and a 
level 0. So the thing is, we go down to level 0, and say, it matches. Then we have a level 1
we have to create. So we create a level 1, merge the point, smash the level 0 to something smaller.

So for the third point, if the merge level increases, we create a level 2 from the level 1,
merge the point, smash level 1 into as many panels as necessary, and smash level 0 the same way.

Now we keep going that way until we get to a something like

-------6
------5
-----4
----3
---2
--1
-0

Now lets add a new point that doesn't merge under the big 6 point, and falls partially under
the 5 panel, and partially off of it
Then we do the following:

-----------6
------5 -----0
-----4
----3
---2
--1
-0

We do it, by, at level 6, we see, hey there is one, so we see it doesn't match anywhere, so we
add it

Then we go to level 5, and say, there is a panel that covers it, but there is this blank area.
So we add it to panel 5 and also create a new panel.
point. Since the point is level 0, it forces the panel to be a level 0 panel.

Then we add another point that matches with the previous, so we geon the streett:

-----------6
------5 -----1
-----4  ----0
----3
---2
--1
-0

We do that by first going to level 6, and say, hey, there is one. So we merge the point and
replace.
Then to level 5, and say, there isn't one. So the rule is, every area panel must go down to
0.

So if level 6 is 0, it's cool. If it's not, then we must create a child for it that is.
But also there is another rule. At every panel, we need to try to combine up to the 
level below the panels parent. 

So here are the rules

1. Start at the top panel. 
2. Try to merge the point under MAX_DEPTH
   ** if point doesn't merge, create a new one
3. If after merging the point, we are at level 0, exit
4. Take the point from 2 and find panels below it.
5. For each panel, or blank space: 
   a)if there is no panel below it, create it. All panels
 must be the same size, (ie. one size smaller than the max) 
      1) If the point is under max depth of the parent panel, then allow it to be added
         and go to 3
      2) Otherwise go to 2 for the current point
   b)If there is a panel
      1) If the point is under max depth of the parent panel, then allow it to be added and
         go to 3
      2) Otherwise go to 2 for the current point

10/11

Ok, a wrinkle. we have to share point information between siblings.

		//we are done with the point, but we will have to add it to our children
		}
		else // otherwise we will have to create a new point 
		{
			//first find a point to merge with the new location that fits within the 
			//maximum allowed depth
			Point bestPoint = findBestPointToMergeWithLoc(ap, r1, r2, ap.getMaxDepth());
			
			//if there isn't any
			if(bestPoint == null)
			{
				//create a new one
				
				//TODO 2: handle condition where a big long point has a lot of times in it, so
				// when we narrow down to a particular time, we can display a portion of the big
				// long point, rather than the whole thing
				TimePanel tp = new TimePanel(r1.getTime(),r2.getTime());
				tp.insertRow(GTG.db);
				modifiedBestPoint = new Point(r1,r2,tp);
				p.insertRow(GTG.db);
				
			}
			else
			{
				//PERF: we are already calculating this with findBestPointToMergeWithLoc()
				PointDiamond pd = new PointDiamond(r1, r2);
				pd.addPointDiamond(bestPoint.constructPointDiamond());
				
				//now we decide whether to insert it or update it. The trick here is that
				//if the depth level changes, it means we always insert it. This is because
				//if it does change, then the previous depth level needs to be shown *somewhere*
				//so the prior point is still needed.
				if(pd.getDepthIndex() != bestPoint.getDepthIndex())
				{
					TimePanel tp = bestPoint.getTimePanel().add(r1,r2);
					tp.insertRow(GTG.db);
					modifiedBestPoint = new Point(pd, tp);
				}
				else //we just update the point and call it a day
				{
					modifiedBestPoint = new Point(pd, bestPoint.getTimePanel());
					modifiedBestPoint.id = bestPoint.id; 
				}
			}
		}// if we weren't able to add the upper point to our panel
		
		//now we need to go through each panel the 

where code goes to die

Ok, we do this in two steps. The first is that we create a mapping from old to new
points. Then, when we are finished with that, we apply the change to the panels.

10/15

PointDiamond(minLatm=-25598252,minLonm=-54570449,latmHeight=113,lonmWidth=-225,fatnessMd=0)

mpd=PointDiamond(minLatm=-25595171,minLonm=-54574722,latmHeight=-3119,lonmWidth=4245,fatnessMd=61)

---
PointDiamond(minLatm=-25595189,minLonm=-54574735,latmHeight=-3075,lonmWidth=4278,fatnessMd=39) -5.4574735E7 -2.5595189E7
-5.4570457E7 -2.5598264E7
-5.4570479762721054E7 -2.559829566794168E7
-5.4574757762721054E7 -2.559522066794168E7
-5.4574735E7 -2.5595189E7


Point(minLatStart=-25598252,minLatEnd=-25598139,minLonStart=-54570449,minLonEnd=-54570674,id=1,numsegments=1,fatnessMd=0) -5.4570449E7 -2.5598252E7
-5.4570674E7 -2.5598139E7
-5.4570674E7 -2.5598139E7
-5.4570449E7 -2.5598252E7
-5.4570449E7 -2.5598252E7

PointDiamond(minLatm=-25595189,minLonm=-54574735,latmHeight=-3075,lonmWidth=4278,fatnessMd=39) -5.4574735E7 -2.5595189E7
-5.4570457E7 -2.5598264E7
-5.4570479762721054E7 -2.559829566794168E7
-5.4574757762721054E7 -2.559522066794168E7
-5.4574735E7 -2.5595189E7

PointDiamond(minLatm=-25598252,minLonm=-54570449,latmHeight=113,lonmWidth=-225,fatnessMd=0) -5.4570449E7 -2.5598252E7
-5.4570674E7 -2.5598139E7
-5.4570674E7 -2.5598139E7
-5.4570449E7 -2.5598252E7
-5.4570449E7 -2.5598252E7

-54574735,-25595189,4278,-3075,39

10/16/10

So, the issue here is that we need to assign basically a temporary maxDepth to
each panel. This is because we want to "mop up all" points that we can. But we 
can't just add the points, because they don't have an id yet. We can't set an
id

10/17/10

[Point(minLatStart=-25600618.000000,minLatEnd=-25600612.000000,minLonStart=-54573600.000000,minLonEnd=-54573596.000000,id=12,numsegments=1,fatnessMd=0.000000,depth=0) -5.45736E7 -2.5600618E7
-5.4573596E7 -2.5600612E7
-5.4573596E7 -2.5600612E7
-5.45736E7 -2.5600618E7
-5.45736E7 -2.5600618E7

, Point(minLatStart=-25600576.000000,minLatEnd=-25600592.000000,minLonStart=-54573608.000000,minLonEnd=-54573564.000000,id=16,numsegments=1,fatnessMd=0.000000,depth=0) -5.4573608E7 -2.5600576E7
-5.4573564E7 -2.5600592E7
-5.4573564E7 -2.5600592E7
-5.4573608E7 -2.5600576E7
-5.4573608E7 -2.5600576E7

]

----

D/GpsTrailer(10417): Gps loc 10 added 9
D/GpsTrailer(10417): Processing gps loc row GpsLocationRow(timeMs=   20100420 06:24:37,latm=           -25599223,lonm=           -54572552,id=11)

It's pointmapping index #2

Has two points, but they both weren't removed.

Here is the point:

PointMapping(

newPoint=Point(minLatStart=-25599230.000000,minLatEnd=-25600626.000000,minLonStart=-54572544.000000,minLonEnd=-54573588.000000,id=18,numsegments=3,fatnessMd=44.806751,depth=6)

,mpd=PointDiamond(minLatm=-25599230.000000,minLonm=-54572544.000000,latmHeight=-1395.734253,lonmWidth=-1045.717651,fatnessMd=44.806751,depth=6)

,oldPoints=[
Point(minLatStart=-25600618.000000,minLatEnd=-25600612.000000,minLonStart=-54573600.000000,minLonEnd=-54573596.000000,id=12,numsegments=1,fatnessMd=0.000000,depth=0)
, Point(minLatStart=-25600576.000000,minLatEnd=-25600592.000000,minLonStart=-54573608.000000,minLonEnd=-54573564.000000,id=16,numsegments=1,fatnessMd=0.000000,depth=0)
])

So it's id's 12 and 16.. we need to know where they came from...
Let's see, they both have a depth of 0...
So when these dudes get created, we find out the panels and...
write them down... and... 


Wait, let's make something to print out the tree for this case.

Let's see... ok we have ap's and points. Points don't even have children
or parents, so it's simple. We print out the ap, the children it contains
and then the child aps
 
Ok, so we have 

----

Ok so the bug is that when we create the child panel, we aren't grabbing 
the parents data. Also note that we create the child after we remove the
old points of the parent, and we need to get the data from the parent before
 this happens

This is because we are handling the parent which includes the child. 
Each area panel has a max depth. So the top guy will have the highest depth.
If it's depth is higher than the point being added, it means that we need to
have a panel below it that has everything in that panel minus the points
containing the biggest depth. 

10.18

Ok lets start this coversation as follows. What do we do if we scrapped all
the panels and started over each time..

Ok, so we'd need to find, first, the points with the highest level. 
Then we create a ... oh... we don't know the children. we can assume that
the parent's cover the children, lets do that for now.

So then we remove all children from consideration, and keep plotting the
highest guys everywhere. So if the top point is 12, we plot it, and then
eliminate all it's children. Then we find the next top point (which isn't
a child of 12). If items are partially overlapping... well.. what to do?
I suppose we have to display it. It's like, the parent child relationship
doesn't matter so much. We simply want to save time drawing.. of course that
ignores the visual aspects of things... but if we draw the biggest stuff last
over the smaller stuff... Anyway, so lets suppose that we do have child links
and we can determine, yes this is a child, or no it isn't.

So then, we plot the biggest parent, remove it's descendents, find the second 
biggest parent, and so on. We keep track of what the biggest parent size was,
and then for the next round, eliminate all points with the biggest parent size,
and repeat for the next level. That's what we do.

So, now we of course can't go through the entire tree for one new point. 
So the thing is that we are updating the tree, so right now we have point mappings
for what we removed and what we added to replace it in the tree.
So if we grouped points x together to form point y, then we set points x to
become point y. 

So the point is that we need a set of area panels for each depth level. Now the
other thing to consider is that the children of a panel are exactly the same size.
So even though a panel has a local "max depth", does the panel group together represent
a total "max depth?" Not really, well, the thing is a child can... hmm we don't 
got the initial thing right.

Because, for the next round, we can't just push down for everything. Like,
after we plot the 12 depth, we need to remove those points, and go down. The thing
is, let's say , for level 10, when there are 4 (or whatever) panels, that way out in 
the boondocks, there is a zero point. We don't want to create a subpanel for it if we 
don't have to (or we'd create zillions of panels).
So during the next round, we need to borrow the boondock panel for the current depth
level. So the point is, that when we plot the next level of points, we need to look
at the local max depth for the panel. So whenever we plot a point ,we look at what
panels already contain it. If there is a panel that does, and it's local max depth
is less than the current max depth (ie one less than the last max depth), then we 
skip plotting it, because we can borrow this panel for the current max depth.

So, now we just need to improve the performance of this so we don't have to go
through all the frickin data.

Ok, now the point is that all points are either modified (to be slightly bigger but
at the same depth level), or created fresh (to be parents of other points, while 
staying under the current max depth). This is another discusion of course... but
we are using areapanels to do this. Which seems crazy bizzare. Well, technically, the
area panels are really just an index to help us find the children of points.

Ok, so for this stage, when we are just adding a point, we start at the top, and
find the points that together with it, can make a new parent. 

So, I guess the point is that we mop up the points how we mop them up, and we don't
need to worry so much. No, it won't be perfect, but it should be ok, because
we are aggregating the points regardless. Yes, you could have a huge blob in the 
center with a bunch of outliers on the edge, I guess.

Hmm...

Will that happen in practice? God this is so hard. Darn.

I don't know. I feel like I'm wasting time... but at the same time, I don't want
to have a shitty product. I just wonder... what if we started with the locs.
the zero level points. So, in other words, maybe we remove some points and add
others, not just one but two... What if we strove for balance. Such as we have
a level 3 point and a level 0 point got added, so instead of not adding it to
the level 3, we see they could be at closer levels together, so we split up
the level 3 and then remerge it with the level 0. We could do this by going to
the panels below, and reproducing what, all of the points of the panel? That
shouldn't be too much of a problem, actually. 

Yea, because having a level 2 panel is much better than having a level 3 if
they both have the same number of points... Hmm...

I suppose that if they are too many points in the below panels, we can 
not do this. I mean, yes there might be 1000s of points, but only at the
lowest levels. So, in which case, we don't have to worry. Hmm.. what if we
just added to reduce the complexity, rather than try to redivide. I mean that
may work, but it may not... it's complex.

Because we could say, hey, this doesn't add to the maximum depth of the panel,
so we'd rather use it then not. Yea, that would be good, then we could keep
pretty much the same stuff.

But ok. but... yea, that would be ok, I think. And if not, we can always recombine
later. The question is, how can we find out whether it doesn't work


Ok, so all the points are modified, which does nothing and can be ignored for
panel modification purposes, or they are added fresh. So now we have these 
new points. Now think, from an area panel perspective, I wonder why we are 
splitting this up... it seems weird. 

Because if we just have a reference count in each point, and then add the
point to the area panels instead... well hold on... ok so lets say we 
do that.

We add the point to the areapanels. So lets start at the top, and
lets just say we add it, and replace a point (and we have ref counts),
so we can ignore any crap that we do as far as updating other panels.
So then, by adding this point, we are subtracting out the two that it
represented... the "old" points. Ok, lets forget that for now.
So then, lets say that we've now raised the maximum depth for the child
panel (if it exists). If it doesn't we need to do a panel smash on the
subpanels covering the changed data of the parent panel before the changes were made. 

FACT: we do point adding because if we added based on panels, we would miss points
that are in different panels. We need to see the entire gamat of points regardless
if they are in many panels that are spread out.

Ok, so this is it..

PointMappings are like layers. Layers have a max depth and cover the world and are
an abstract notion. 
So we start at the top, and create the biggest layer. Then we set the max depth one
minus that, and create another.

So lets suppose the top layer is 10, and we add one point to it. We now have a layer
of 12 when we combine, lets say. Then, we set the max to 11, and create a 10 layer
which has a new 0 point.

See... that is what we do.. We create layers all the way down. 

Then we mess with area panels. We determine which layer(s) an area panel belongs to.
If, when making the necessary changes, the area panel can no longer satisfy the 
requirements of all layers, that's when we smash. Smashing sucks.
But it's what we have to do.. now listen.

So we make a copy of the area panel. Then we modify it for the top layer it
is part of.. This is determined by whether there is a parent that has a maxdepth
at or under the layer. If not, the panel belongs to it.

Then, we keep going down until we reach the layer that has a depth that is equal
to the child (if any), or go all the way down.

Then, we look at the max depth of the *new points that overlap the panel*
if this is greater than the depth of the min layer, we've got to smash

When we add the *new points that overlap the panel*, we are talking about 1
point that transcends multiple layers, all of them in fact. Ah,
so that's it. We are talking about a new point, not new points.
And that new point covers multiple layers. Right, so a point mapping has
a max and min depth layer. And the min depth is the size of the new point...
hmmm... nah...

But yes, though, if we want, we can make pointmappings cover multiple layers.
Because they do.

10/19

Ok, so we start at the top layer. Then ,we see if we can combine points.
Then we go to the next level.

FACT: When we smash, we are only creating a new child if there wasn't one there
before, otherwise we are replacing the panel. So we smash the original
to make it work like it did before (over some smaller sub panels) and the
main one which shows through where the smaller subpanels don't exist.

10/20


Not removed point:
 Point(minLatStart=-25675776.000000,minLatEnd=-25598070.000000,minLonStart=-54505048.000000,
minLonEnd=-54577856.000000,id=20,numsegments=13,fatnessMd=6719.298828,depth=11) 
-5.4505048E7 -2.5675776E7
-5.4577856E7 -2.559807E7
-5.45729527190577E7 -2.5593475784829296E7
-5.45001447190577E7 -2.5671181784829296E7
-5.4505048E7 -2.5675776E7

D/GpsTrailer(  747): Processing gps loc row GpsLocationRow(timeMs=   20100421 03:51:03,
latm=           -25766609,lonm=           -54538617,id=18)


10/21

pmIndex == 0


TODO: the location of the center of the items is not the same as the location of the inner points


10/22

new child:

AreaPanel(id=-1,minLonm=-54586485,minLatm=-26033013,width=6561,tsLFk=-2147483648,tsRFk=-2147483648,numChildren=0,numPoints=2,maxDepth=0) -5.4586485E7 -2.6033013E7
-5.4579924E7 -2.6033013E7
-5.4579924E7 -2.6026452E7
-5.4586485E7 -2.6026452E7
-5.4586485E7 -2.6033013E7

ap:

AreaPanel(id=15,minLonm=-54599607,minLatm=-26039574,width=19683,tsLFk=-2147483648,tsRFk=-2147483648,numChildren=2,numPoints=5,maxDepth=3) -5.4599607E7 -2.6039574E7
-5.4579924E7 -2.6039574E7
-5.4579924E7 -2.6019891E7
-5.4599607E7 -2.6019891E7
-5.4599607E7 -2.6039574E7

orig child:

AreaPanel(id=17,minLonm=-54586485,minLatm=-26033013,width=6561,tsLFk=-2147483648,tsRFk=-2147483648,numChildren=0,numPoints=1,maxDepth=0) -5.4586485E7 -2.6033013E7
-5.4579924E7 -2.6033013E7
-5.4579924E7 -2.6026452E7
-5.4586485E7 -2.6026452E7
-5.4586485E7 -2.6033013E7

point diamond:

[PointDiamond(minLatm=-26027004.000000,minLonm=-54586392.000000,latmHeight=3172.242676,lonmWidth=196.120102,fatnessMd=5.538909,depth=4)  -5.4586392E7 -2.6027004E7
-5.4586196E7 -2.6023832E7
-5.4586190471645534E7 -2.602383234178388E7
-5.4586386471645534E7 -2.602700434178388E7
-5.4586392E7 -2.6027004E7

]

Strange, the point diamond depth is 4, but the ap depth is 3, and the child depth is 0... is that normal?

Id's are 15 and 17... hmm

Point that should have caused 17 to be created is id=42, depth =3

But it is way far away.. so why was 17 created?

---

Points array

tart=-25983342.000000,minLatEnd=-26023832.000000,minLonStart=-54581720.000000,minLonEnd=-54586192.000000,id=39,numsegments=1,fatnessMd=0.000000,depth=0) -5.458172E7 -2.5983342E7
-5.4586192E7 -2.6023832E7
-5.4586192E7 -2.6023832E7
-5.458172E7 -2.5983342E7
-5.458172E7 -2.5983342E7

, Point(minLatStart=-26026010.000000,minLatEnd=-26023832.000000,minLonStart=-54586332.000000,minLonEnd=-54586192.000000,id=42,numsegments=2,fatnessMd=1.271939,depth=3) -5.4586332E7 -2.602601E7
-5.4586192E7 -2.6023832E7
-5.4586190730680786E7 -2.6023832081590764E7
-5.4586330730680786E7 -2.6026010081590764E7
-5.4586332E7 -2.602601E7

, Point(minLatStart=-26026012.000000,minLatEnd=-26026420.000000,minLonStart=-54586332.000000,minLonEnd=-54586352.000000,id=45,numsegments=1,fatnessMd=0.000000,depth=0) -5.4586332E7 -2.6026012E7
-5.4586352E7 -2.602642E7
-5.4586352E7 -2.602642E7
-5.4586332E7 -2.6026012E7
-5.4586332E7 -2.6026012E7

, Point(minLatStart=-26026418.000000,minLatEnd=-26027004.000000,minLonStart=-54586352.000000,minLonEnd=-54586392.000000,id=46,numsegments=1,fatnessMd=0.000000,depth=0) -5.4586352E7 -2.6026418E7
-5.4586392E7 -2.6027004E7
-5.4586392E7 -2.6027004E7
-5.4586352E7 -2.6026418E7
-5.4586352E7 -2.6026418E7

, Point(minLatStart=-26027004.000000,minLatEnd=-26027374.000000,minLonStart=-54586392.000000,minLonEnd=-54586424.000000,id=48,numsegments=1,fatnessMd=0.000000,depth=0) -5.4586392E7 -2.6027004E7
-5.4586424E7 -2.6027374E7
-5.4586424E7 -2.6027374E7
-5.4586392E7 -2.6027004E7
-5.4586392E7 -2.6027004E7

]

original child point

[Point(minLatStart=-26027004.000000,minLatEnd=-26027374.000000,minLonStart=-54586392.000000,minLonEnd=-54586424.000000,id=48,numsegments=1,fatnessMd=0.000000,depth=0) -5.4586392E7 -2.6027004E7
-5.4586424E7 -2.6027374E7
-5.4586424E7 -2.6027374E7
-5.4586392E7 -2.6027004E7
-5.4586392E7 -2.6027004E7

]


ok, when we process id 31 is when the real issue happens. It is with the following
depth 4 pointdiamond:

-5.4586392E7 -2.6027004E7
-5.4586196E7 -2.6023832E7
-5.4586190471645534E7 -2.602383234178388E7
-5.4586386471645534E7 -2.602700434178388E7
-5.4586392E7 -2.6027004E7

It is the first point that got accepted, i=5

-5.4586485E7 -2.6033013E7
-5.4579924E7 -2.6033013E7
-5.4579924E7 -2.6026452E7
-5.4586485E7 -2.6026452E7
-5.4586485E7 -2.6033013E7

But wait... id 17 is not null? why?

It is being created as a result of this guy:

,mpd=PointDiamond(minLatm=-26027004.000000,minLonm=-54586392.000000,latmHeight=-370.000000,lonmWidth=-32.000000,fatnessMd=0.000000,depth=0)  
-5.4586392E7 -2.6027004E7
-5.4586424E7 -2.6027374E7
-5.4586424E7 -2.6027374E7
-5.4586392E7 -2.6027004E7
-5.4586392E7 -2.6027004E7

which is incorrect, because there is no level 3 depth guy to cover up at this level.

So basically in updatePanel2() we should only create a subpanel if there is 
a point with a max depth within the child area.

-----

PointMapping(newPoint=Point(minLatStart=-26238990.000000,minLatEnd=-25763554.000000,minLonStart=-54597960.000000,minLonEnd=-54579932.000000,id=53,numsegments=3,fatnessMd=41412.367188,depth=12) -5.459796E7 -2.623899E7
-5.4579932E7 -2.5763554E7
-5.453854936840636E7 -2.5765123084097933E7
-5.455657736840636E7 -2.6240559084097933E7
-5.459796E7 -2.623899E7

,mpd=PointDiamond(minLatm=-26238990.000000,minLonm=-54597960.000000,latmHeight=475435.625000,lonmWidth=18026.849609,fatnessMd=41412.367188,depth=12)  -5.459796E7 -2.623899E7
-5.4579932E7 -2.5763554E7
-5.453854936840636E7 -2.5765123084097933E7
-5.455657736840636E7 -2.6240559084097933E7
-5.459796E7 -2.623899E7

,oldPoints=[Point(minLatStart=-26027378.000000,minLatEnd=-25764772.000000,minLonStart=-54588852.000000,minLonEnd=-54551324.000000,id=33,numsegments=1535,fatnessMd=12841.058594,depth=11) -5.4588852E7 -2.6027378E7
-5.4551324E7 -2.5764772E7
-5.453861209566386E7 -2.576658866475798E7
-5.457614009566386E7 -2.602919466475798E7
-5.4588852E7 -2.6027378E7

, Point(minLatStart=-26166254.000000,minLatEnd=-26027800.000000,minLonStart=-54586864.000000,minLonEnd=-54589952.000000,id=48,numsegments=5,fatnessMd=3495.855225,depth=10) -5.4586864E7 -2.6166254E7
-5.4589952E7 -2.60278E7
-5.458645701432626E7 -2.6027722032383297E7
-5.458336901432626E7 -2.6166176032383297E7
-5.4586864E7 -2.6166254E7

])


removed points:

[Point(minLatStart=-26027378.000000,minLatEnd=-25764772.000000,minLonStart=-54588852.000000,minLonEnd=-54551324.000000,id=33,numsegments=1535,fatnessMd=12841.058594,depth=11) -5.4588852E7 -2.6027378E7
-5.4551324E7 -2.5764772E7
-5.453861209566386E7 -2.576658866475798E7
-5.457614009566386E7 -2.602919466475798E7
-5.4588852E7 -2.6027378E7

]


48 was already under P3,4,-65208744,-36510930,14348907,1,8,10,-2147483648,-2147483648

panel 4

pmIndex = 1

but it's also under panel 2, so what the heck?

---

Point(minLatStart=-26515022.000000,minLatEnd=-26823204.000000,minLonStart=-54697072.000000,minLonEnd=-55028520.000000,id=96,numsegments=5,fatnessMd=43.785618,depth=6)
disappears from 50 miles to 25 with no replacement???

AreaPanel 14 has id 96

---

10/23

Processing gps loc row GpsLocationRow(timeMs=   20100421 05:16:07,latm=           -26025533,lonm=   
        -54586301,id=26)


E/GTG     ( 4589): Missing Point: Can't find id 39, Point Point(id=39,minLatStart=-25983342.000000,minLatEnd=-26023832.00
0000,minLonStart=-54581720.000000,minLonEnd=-54586192.000000,numsegments=1,fatnessMd=0.000000,depth=0)
E/GTG     ( 4589): -5.458172E7 -2.5983342E7
E/GTG     ( 4589): -5.4586192E7 -2.6023832E7
E/GTG     ( 4589): -5.4586192E7 -2.6023832E7
E/GTG     ( 4589): -5.458172E7 -2.5983342E7
E/GTG     ( 4589): -5.458172E7 -2.5983342E7
E/GTG     ( 4589): 
E/GTG     ( 4589): Missing Point: Can't find id 41, Point Point(id=41,minLatStart=-26023832.000000,minLatEnd=-26025532.00
0000,minLonStart=-54586192.000000,minLonEnd=-54586300.000000,numsegments=1,fatnessMd=0.000000,depth=0)
E/GTG     ( 4589): -5.4586192E7 -2.6023832E7
E/GTG     ( 4589): -5.45863E7 -2.6025532E7
E/GTG     ( 4589): -5.45863E7 -2.6025532E7
E/GTG     ( 4589): -5.4586192E7 -2.6023832E7
E/GTG     ( 4589): -5.4586192E7 -2.6023832E7
E/GTG     ( 4589): 

39 was a child of Panel 6, Panel 5, and Panel 4
E/CACHEVIEWER_AP( 4589): P5,6,-55642806,-26944992,1594323,1,16,8,-2147483648,-2147483648

Here are the pointmappings

[PointMapping(newPoint=null,mpd=PointDiamond(minLatm=-26025532.000000,minLonm=-54586300.000000,latmHeight=447250.687500,lonmWidth=-1959.977417,fatnessMd=118616.718750,depth=13)  -5.45863E7 -2.6025532E7
-5.458826E7 -2.5578282E7
-5.446964441816751E7 -2.5577762193374973E7
-5.446768441816751E7 -2.6025012193374973E7
-5.45863E7 -2.6025532E7

,oldPoints=[Point(id=24,minLatStart=-25577762.000000,minLatEnd=-26023282.000000,minLonStart=-54469760.000000,minLonEnd=-54467692.000000,numsegments=767,fatnessMd=118501.117188,depth=13)
-5.446976E7 -2.5577762E7
-5.4467692E7 -2.6023282E7
-5.45861918429096E7 -2.6023832005468693E7
-5.45882598429096E7 -2.5578312005468693E7
-5.446976E7 -2.5577762E7

]), PointMapping(newPoint=null,mpd=PointDiamond(minLatm=-26025212.000000,minLonm=-54588544.000000,latmHeight=260439.265625,lonmWidth=37220.273438,fatnessMd=12841.059570,depth=11)  -5.4588544E7 -2.6025212E7
-5.4551324E7 -2.5764772E7
-5.453861209988934E7 -2.576658870143885E7
-5.457583209988934E7 -2.602702870143885E7
-5.4588544E7 -2.6025212E7

,oldPoints=[Point(id=33,minLatStart=-26023530.000000,minLatEnd=-25764772.000000,minLonStart=-54588304.000000,minLonEnd=-54551324.000000,numsegments=23,fatnessMd=12841.059570,depth=11)
-5.4588304E7 -2.602353E7
-5.4551324E7 -2.5764772E7
-5.453861210229199E7 -2.5766588717076335E7
-5.457559210229199E7 -2.6025346717076335E7
-5.4588304E7 -2.602353E7

]), PointMapping(newPoint=null,mpd=PointDiamond(minLatm=-25983342.000000,minLonm=-54581720.000000,latmHeight=-42190.000000,lonmWidth=-4580.000000,fatnessMd=76.099022,depth=7)  -5.458172E7 -2.5983342E7
-5.45863E7 -2.6025532E7
-5.4586375654550694E7 -2.602552378720428E7
-5.4581795654550694E7 -2.598333378720428E7
-5.458172E7 -2.5983342E7

,oldPoints=[Point(id=39,minLatStart=-25983342.000000,minLatEnd=-26023832.000000,minLonStart=-54581720.000000,minLonEnd=-54586192.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-5.458172E7 -2.5983342E7
-5.4586192E7 -2.6023832E7
-5.4586192E7 -2.6023832E7
-5.458172E7 -2.5983342E7
-5.458172E7 -2.5983342E7

]), PointMapping(newPoint=null,mpd=PointDiamond(minLatm=-26023832.000000,minLonm=-54586192.000000,latmHeight=-1700.000000,lonmWidth=-107.000000,fatnessMd=0.000000,depth=0)  -5.4586192E7 -2.6023832E7
-5.45863E7 -2.6025532E7
-5.45863E7 -2.6025532E7
-5.4586192E7 -2.6023832E7
-5.4586192E7 -2.6023832E7

,oldPoints=[])]

pm 2

10/23

So here is the deal... the subpanels are responsible for the layers from
their own depth to one minus the depth of the parent of the area they cover.
So if the parent is a big panel, and has depths 3, 6, and 10, and lets say
the child has a max depth of 4 and is under the parent's 6 area, then the
child covers layers 4-5. Now, if the parent changes, and the 10 creeps into
the 6 area, then the area panel must be smashed for the area where the child is
to cover ranges 6 through 9.
Then of course, the child must be smashed, and so on, through all the children
for the bumping heads between the area panels problem (a parent cannot contain
a child which si the same size as itself)

10/24

FACT: remember to view things in layesr

Ok, so each child needs to reach to 0. So a particular area can go from 0
to infinity if there is only one point in that area. So when we make a change
to an area panel, we are changing the min and max area it can handle. We must
split it up into it's child panels and investigate each separately.

So when we get a new point, we check what depths the area panel can handle
for each child panel area of the parent. So if the pointmapping produces
a new point of size 2 for example, we can apply it to all layers from inf
to 2. So if we have a particular *sub* area of a panel A that goes from inf - 0
that is covered by the 2 we need to smash that sub area out before we apply the change.

 (Note that it currently is not possible for the new point to miss an area that is 
covered by the old points. If we do in the future support this, we need to smash 
the panels covered by the old points too. This is because we must be consistent
within a layer. This also means we have to same the min and max area of each panel
because it will no longer be calculable.)

This will create a subpanel B from 0-1 so we can add the new panel. Any area
not covered by the change does not need to be smashed (this includes cases
where A has subpanel areas that are not covered by the change)

When we smash the sub area B (which is only needed for cases where there is 
a child beneath), we again have to look at each sub panel separately.

For example, panel A goes from inf - 6 (in some places... remember it changes
per sub panel). It contains a single subpanel in position
0, subpanel B that goes from 5 - 0 (and contains only level 0 points). Then lets
say we get a point mapping which replaces 6 with a 10. Then panel A becomes
A1 and A2. A1 goes from inf - 10 with the 6 replaced by the 10. A2 gets smashed
and covers A1 wherever the point 10 reaches. Now, lets say the A2 subpanel that 
was smashed has child B under where the 6 was (it'd have to). Then child B gets
smashed,

In this case, the new panel we created for the original smash becomes the "ceiling"


FACT:
Ok, so this is it:

1) We have new and old panel.. new contains the changes according to pm
2) Break into subpanels. For each subpanel:
    a) if the old panel has a lower level (and is not empty), then we need
       to make a covering subpanel (smash)
3) for each subpanel that we smashed, smash again if there is a child, using
   the current panel as new and the child as old


processing point 107

204, the newly added point is nowhere to be found.. why?

It never found a panel with a zero min depth that handles it

processing row is 67

pmIndex is 3

PointMapping(newPoint=Point(id=125,minLatStart=-27042320.000000,minLatEnd=-26510076.000000,minLonStart=-55234192.000000,minLonEnd=-54692036.000000,numsegments=2,fatnessMd=791.674866,depth=9)
-5.5234192E7 -2.704232E7
-5.4692036E7 -2.6510076E7
-5.469148138946529E7 -2.651064093917162E7
-5.523363738946529E7 -2.704288493917162E7
-5.5234192E7 -2.704232E7

,mpd=PointDiamond(minLatm=-27042320.000000,minLonm=-55234192.000000,latmHeight=532244.500000,lonmWidth=542156.625000,fatnessMd=791.674866,depth=9)  -5.5234192E7 -2.704232E7
-5.4692036E7 -2.6510076E7
-5.469148138946529E7 -2.651064093917162E7
-5.523363738946529E7 -2.704288493917162E7
-5.5234192E7 -2.704232E7

,oldPoints=[Point(id=98,minLatStart=-26510466.000000,minLatEnd=-26515020.000000,minLonStart=-54691652.000000,minLonEnd=-54697072.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-5.4691652E7 -2.6510466E7
-5.4697072E7 -2.651502E7
-5.4697072E7 -2.651502E7
-5.4691652E7 -2.6510466E7
-5.4691652E7 -2.6510466E7

])

Panel 71 has the old point id 98

E/CACHEVIEWER_AP( 3926): id,lonm1,latm1,width,numChildren,numPoints,maxDepth,timeSiblingLeftFk,timeSiblingRightFk
E/CACHEVIEWER_AP( 3926): P35,71,-54698022,-26531649,59049,0,3,0,-2147483648,-2147483648

1,2,3,4,5,6,34,35,71

We're actually dealing with panel 75 and 76. Even 76 has a child

But non of them have point 98. So where did it go?

We aren't even going to 71, we are at 73... ah

5 isn't even close to pm 2.. but it is close to pm 3

--

The pointmapping min depth is 7 and the child maxdepth is 5

---

PointMapping(newPoint=Point(id=194,minLatStart=-27718316.000000,minLatEnd=-27488908.000000,minLonStart=-55794040.000000,minLonEnd=-55868816.000000,numsegments=3,fatnessMd=4802.342773,depth=10)
-5.579404E7 -2.7718316E7
-5.5868816E7 -2.7488908E7
-5.586425008802992E7 -2.7487419731616467E7
-5.578947408802992E7 -2.7716827731616467E7
-5.579404E7 -2.7718316E7

,mpd=PointDiamond(minLatm=-27718316.000000,minLonm=-55794040.000000,latmHeight=229408.015625,lonmWidth=-74776.007812,fatnessMd=4802.342773,depth=10)  -5.579404E7 -2.7718316E7
-5.5868816E7 -2.7488908E7
-5.586425008802992E7 -2.7487419731616467E7
-5.578947408802992E7 -2.7716827731616467E7
-5.579404E7 -2.7718316E7

,oldPoints=[Point(id=180,minLatStart=-27487534.000000,minLatEnd=-27513188.000000,minLonStart=-55864216.000000,minLonEnd=-55860100.000000,numsegments=5,fatnessMd=774.742981,depth=9)
-5.5864216E7 -2.7487534E7
-5.58601E7 -2.7513188E7
-5.5860864958274044E7 -2.751331074161564E7
-5.5864980958274044E7 -2.748765674161564E7
-5.5864216E7 -2.7487534E7

, Point(id=188,minLatStart=-27513188.000000,minLatEnd=-27601432.000000,minLonStart=-55860096.000000,minLonEnd=-55827904.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-5.5860096E7 -2.7513188E7
-5.5827904E7 -2.7601432E7
-5.5827904E7 -2.7601432E7
-5.5860096E7 -2.7513188E7
-5.5860096E7 -2.7513188E7

])

180 and 188 Owned by Panel 277 
1,2,3,4,252,253,277

pmindex is 5

The points were owned by Panel 277, but not anymore. They are now both owned by
panel 278. That's strange, the other child, 286 has no points at all... I suppose
that's possible.. but odd. hmm..

So our maxdepth at child is 10, and the pm min depth is also 10, which means
we should have already handled everything the pointmapping has. However, we didn't.

So before we updatePanels(), panel 277 does indeed have 180 and 188.

So when we smash, I think that panel 277 is changing from what it was.
Which is why when we reach it, it contains different points.

277 does have a depth of 9 to start with, and later changes to a depth of 10

This is my guess.. the changing depth stuff doesn't matter.
But what i guess is happening is that the 10 panel has combined x and y,
and now the panel 9 wants to combine y and z and make a 10 panel.. only
there already is one.

So in this case, we just have to ignore the pointmapping... because we
can't make two alternate universes where we made the 10 two different ways.

Well the current parent is 253, but I think that doesn't matter, because of the
smashing.

,,,,

Ok the parent at the time of the update (277) has these points:

[Point(id=178,minLatStart=-27450966.000000,minLatEnd=-27487536.000000,minLonStart=-55866936.000000,minLonEnd=-55864216.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-5.5866936E7 -2.7450966E7
-5.5864216E7 -2.7487536E7
-5.5864216E7 -2.7487536E7
-5.5866936E7 -2.7450966E7
-5.5866936E7 -2.7450966E7

, Point(id=187,minLatStart=-27487424.000000,minLatEnd=-27601432.000000,minLonStart=-55864252.000000,minLonEnd=-55827904.000000,numsegments=2,fatnessMd=4636.932617,depth=10)
-5.5864252E7 -2.7487424E7
-5.5827904E7 -2.7601432E7
-5.583232184804549E7 -2.7602840461021177E7
-5.586866984804549E7 -2.7488832461021177E7
-5.5864252E7 -2.7487424E7

, Point(id=190,minLatStart=-27601476.000000,minLatEnd=-27356580.000000,minLonStart=-55827952.000000,minLonEnd=-55555440.000000,numsegments=2,fatnessMd=3554.811279,depth=10)
-5.5827952E7 -2.7601476E7
-5.555544E7 -2.735658E7
-5.555306391742842E7 -2.7359224033829957E7
-5.582557591742842E7 -2.7604120033829957E7
-5.5827952E7 -2.7601476E7

]

And the child has;

[Point(id=178,minLatStart=-27450966.000000,minLatEnd=-27487536.000000,minLonStart=-55866936.000000,minLonEnd=-55864216.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-5.5866936E7 -2.7450966E7
-5.5864216E7 -2.7487536E7
-5.5864216E7 -2.7487536E7
-5.5866936E7 -2.7450966E7
-5.5866936E7 -2.7450966E7

, Point(id=180,minLatStart=-27487534.000000,minLatEnd=-27513188.000000,minLonStart=-55864216.000000,minLonEnd=-55860100.000000,numsegments=5,fatnessMd=774.742981,depth=9)
-5.5864216E7 -2.7487534E7
-5.58601E7 -2.7513188E7
-5.5860864958274044E7 -2.751331074161564E7
-5.5864980958274044E7 -2.748765674161564E7
-5.5864216E7 -2.7487534E7

, Point(id=188,minLatStart=-27513188.000000,minLatEnd=-27601432.000000,minLonStart=-55860096.000000,minLonEnd=-55827904.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-5.5860096E7 -2.7513188E7
-5.5827904E7 -2.7601432E7
-5.5827904E7 -2.7601432E7
-5.5860096E7 -2.7513188E7
-5.5860096E7 -2.7513188E7

, Point(id=191,minLatStart=-27410520.000000,minLatEnd=-27603008.000000,minLonStart=-55901992.000000,minLonEnd=-55826576.000000,numsegments=2,fatnessMd=1490.863281,depth=9)
-5.5901992E7 -2.741052E7
-5.5826576E7 -2.7603008E7
-5.582796412394691E7 -2.7603551861220237E7
-5.590338012394691E7 -2.7411063861220237E7
-5.5901992E7 -2.741052E7

]

Ah, because when we invent the pointmapping, we are asked to stay under the parents
depth, which is 11. However, it gets smashed up and a 10 is created.

ok, so 253 with a max depth of 11 definitely has some 10 areas.

So by smashing it, we effectively lowered the depth. Which means the 9to1=10 mapping
is no longer valid


Alright, so the thing is that when we apply a point mapping, we do so in *layers*,
that is, across the entire world. So if we have a top area panel with a 0 point in the
boondocks, and a 10 point that becomes an 11 point, we are not applying the point mapping
to the entire area panel, but only layers 10 and 11. After that has been completed, the
remaining part of the area panel that is below 9 (way out in the boondocks) could still
be update by another point mapping.

Now the other thing to consider is that we are doing it by layers. So first we have to
apply the point mapping change to the area panels of interest, and then we need to find
all the children at once before we can generate the next point mapping.

10/25

PointMapping(newPoint=Point(id=665,minLatStart=-34610832.000000,minLatEnd=-34614592.000000,minLonStart=-58375104.000000,minLonEnd=-58383000.000000,numsegments=2,fatnessMd=581.858459,depth=8)
-5.8375104E7 -3.4610832E7
-5.8383E7 -3.4614592E7
-5.83832500163452E7 -3.461406659435596E7
-5.83753540163452E7 -3.461030659435596E7
-5.8375104E7 -3.4610832E7

,mpd=PointDiamond(minLatm=-34610832.000000,minLonm=-58375104.000000,latmHeight=-3758.111328,lonmWidth=-7897.614746,fatnessMd=581.858459,depth=8)  -5.8375104E7 -3.4610832E7
-5.8383E7 -3.4614592E7
-5.83832500163452E7 -3.461406659435596E7
-5.83753540163452E7 -3.461030659435596E7
-5.8375104E7 -3.4610832E7

,oldPoints=[Point(id=563,minLatStart=-34610696.000000,minLatEnd=-34611360.000000,minLonStart=-58375168.000000,minLonEnd=-58376208.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-5.8375168E7 -3.4610696E7
-5.8376208E7 -3.461136E7
-5.8376208E7 -3.461136E7
-5.8375168E7 -3.4610696E7
-5.8375168E7 -3.4610696E7

])

Owned by 3341 and 3353
E/CACHEVIEWER_AP( 5182): P3052,3341,-58477158,-34562313,59049,9,26,8,-2147483648,-2147483648
Max depth is 8

E/CACHEVIEWER_AP( 5182): P3424,3533,-58378743,-34614801,6561,9,6,0,-2147483648,-2147483648
Max depth is 0

But 3533 is not a parent of it

3339,3424,3533

Here are the area panels that we add the point to:

10/26

D/GpsTrailer( 2080): tapped on row Point(id=223,minLatStart=-31757462.000000,minLatEnd=-27332396.000000,minLonStart=-58350668.000000,minLonEnd=-55593484.000000,numsegments=12287,fatnessMd=45079.648438,depth=12)
D/GpsTrailer( 2080): -5.8350668E7 -3.1757462E7
D/GpsTrailer( 2080): -5.5593484E7 -2.7332396E7
D/GpsTrailer( 2080): -5.555522361300002E7 -2.735623541466131E7
D/GpsTrailer( 2080): -5.831240761300002E7 -3.178130141466131E7
D/GpsTrailer( 2080): -5.8350668E7 -3.1757462E7
D/GpsTrailer( 2080): 
D


--

PointMapping(newPoint=Point(id=906,minLatStart=-32948502.000000,minLatEnd=-32955668.000000,minLonStart=-60641704.000000,minLonEnd=-60642696.000000,numsegments=2,fatnessMd=40.993397,depth=6)
-6.0641704E7 -3.2948502E7
-6.0642696E7 -3.2955668E7
-6.064273660617036E7 -3.295566237882783E7
-6.064174460617036E7 -3.294849637882783E7
-6.0641704E7 -3.2948502E7

,mpd=PointDiamond(minLatm=-32948502.000000,minLonm=-60641704.000000,latmHeight=-7166.000000,lonmWidth=-992.000000,fatnessMd=40.993397,depth=6)  -6.0641704E7 -3.2948502E7
-6.0642696E7 -3.2955668E7
-6.064273660617036E7 -3.295566237882783E7
-6.064174460617036E7 -3.294849637882783E7
-6.0641704E7 -3.2948502E7

,oldPoints=[Point(id=864,minLatStart=-32948502.000000,minLatEnd=-32955668.000000,minLonStart=-60641704.000000,minLonEnd=-60642696.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-6.0641704E7 -3.2948502E7
-6.0642696E7 -3.2955668E7
-6.0642696E7 -3.2955668E7
-6.0641704E7 -3.2948502E7
-6.0641704E7 -3.2948502E7

])

The owners:

E/CACHEVIEWER_AP( 2830): P8891,9176,-60642288,-32952681,2187,9,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8891,9179,-60642288,-32950494,2187,9,2,0,-2147483648,-2147483648

E/CACHEVIEWER_AP( 2830): P8953,8964,-60642774,-32955840,243,9,26,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8891,9173,-60642288,-32954868,2187,9,3,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8981,9008,-60642288,-32955597,729,9,4,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8888,8981,-60642288,-32957055,2187,9,7,5,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8868,8888,-60642288,-32961429,6561,9,15,6,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8884,8971,-60644475,-32952681,2187,9,2,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8884,8968,-60644475,-32954868,2187,9,3,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8956,9264,-60642774,-32955597,243,9,15,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8938,8956,-60643017,-32955597,729,9,16,4,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8953,8965,-60642531,-32955840,243,9,13,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8953,8964,-60642774,-32955840,243,9,26,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8938,8953,-60643017,-32956326,729,9,26,4,-2147483648,-2147483648
E/CACHEVIEWER_AP( 2830): P8881,8938,-60644475,-32957055,2187,9,27,5,-2147483648,-2147483648



-----


PointMapping(newPoint=Point(id=857,minLatStart=-32948502.000000,minLatEnd=-32955806.000000,minLonStart=-60641704.000000,minLonEnd=-60642672.000000,numsegments=6,fatnessMd=42.892086,depth=3)
-6.0641704E7 -3.2948502E7
-6.0642672E7 -3.2955806E7
-6.064271452009426E7 -3.2955800363263413E7
-6.064174652009426E7 -3.2948496363263413E7
-6.0641704E7 -3.2948502E7

,mpd=PointDiamond(minLatm=-32948502.000000,minLonm=-60641704.000000,latmHeight=-7303.281738,lonmWidth=-968.169922,fatnessMd=42.892086,depth=3)  -6.0641704E7 -3.2948502E7
-6.0642672E7 -3.2955806E7
-6.064271452009426E7 -3.2955800363263413E7
-6.064174652009426E7 -3.2948496363263413E7
-6.0641704E7 -3.2948502E7

,oldPoints=[Point(id=857,minLatStart=-32948502.000000,minLatEnd=-32955806.000000,minLonStart=-60641704.000000,minLonEnd=-60642672.000000,numsegments=6,fatnessMd=42.892086,depth=3)
-6.0641704E7 -3.2948502E7
-6.0642672E7 -3.2955806E7
-6.064271452009426E7 -3.2955800363263413E7
-6.064174652009426E7 -3.2948496363263413E7
-6.0641704E7 -3.2948502E7

, Point(id=856,minLatStart=-32948502.000000,minLatEnd=-32955668.000000,minLonStart=-60641704.000000,minLonEnd=-60642696.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-6.0641704E7 -3.2948502E7
-6.0642696E7 -3.2955668E7
-6.0642696E7 -3.2955668E7
-6.0641704E7 -3.2948502E7
-6.0641704E7 -3.2948502E7

])



x = (log(360000000) - log(500)) / log(3)



-------


PointMapping(newPoint=Point(id=341,minLatStart=-32770684.000000,minLatEnd=-32955824.000000,minLonStart=-58500844.000000,minLonEnd=-58582164.000000,numsegments=2,fatnessMd=1664.564087,depth=4)
-5.8500844E7 -3.2770684E7
-5.8582164E7 -3.2955824E7
-5.858368802997129E7 -3.2955154592457213E7
-5.850236802997129E7 -3.2770014592457213E7
-5.8500844E7 -3.2770684E7

,mpd=PointDiamond(minLatm=-32770684.000000,minLonm=-58500844.000000,latmHeight=-185140.000000,lonmWidth=-81320.000000,fatnessMd=1664.564087,depth=4)  -5.8500844E7 -3.2770684E7
-5.8582164E7 -3.2955824E7
-5.858368802997129E7 -3.2955154592457213E7
-5.850236802997129E7 -3.2770014592457213E7
-5.8500844E7 -3.2770684E7

,oldPoints=[Point(id=337,minLatStart=-32770684.000000,minLatEnd=-32850254.000000,minLonStart=-58500844.000000,minLonEnd=-58537612.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-5.8500844E7 -3.2770684E7
-5.8537612E7 -3.2850254E7
-5.8537612E7 -3.2850254E7
-5.8500844E7 -3.2770684E7
-5.8500844E7 -3.2770684E7

])

-----

PointMapping(newPoint=Point(id=263,minLatStart=-30059718.000000,minLatEnd=-30064504.000000,minLonStart=-57907700.000000,minLonEnd=-57864984.000000,numsegments=3,fatnessMd=178.528137,depth=1)
-5.79077E7 -3.0059718E7
-5.7864984E7 -3.0064504E7
-5.786500387617938E7 -3.0064681418247685E7
-5.790771987617938E7 -3.0059895418247685E7
-5.79077E7 -3.0059718E7

,mpd=PointDiamond(minLatm=-30059718.000000,minLonm=-57907700.000000,latmHeight=-4785.336914,lonmWidth=42714.753906,fatnessMd=178.528137,depth=1)  -5.79077E7 -3.0059718E7
-5.7864984E7 -3.0064504E7
-5.786500387617938E7 -3.0064681418247685E7
-5.790771987617938E7 -3.0059895418247685E7
-5.79077E7 -3.0059718E7

,oldPoints=[Point(id=257,minLatStart=-30063996.000000,minLatEnd=-30062280.000000,minLonStart=-57870448.000000,minLonEnd=-57886440.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-5.7870448E7 -3.0063996E7
-5.788644E7 -3.006228E7
-5.788644E7 -3.006228E7
-5.7870448E7 -3.0063996E7
-5.7870448E7 -3.0063996E7

, Point(id=256,minLatStart=-30064634.000000,minLatEnd=-30063996.000000,minLonStart=-57865000.000000,minLonEnd=-57870444.000000,numsegments=3,fatnessMd=10.904068,depth=0)
-5.7865E7 -3.0064634E7
-5.7870444E7 -3.0063996E7
-5.787044273080295E7 -3.0063985170049056E7
-5.786499873080295E7 -3.0064623170049056E7
-5.7865E7 -3.0064634E7

])

E/CACHEVIEWER_AP( 3851): P824,874,-57907575,-30086964,54873,9,4,1,-2147483648,-2147483648

Here are the panels that are gone through:

 (java.util.ArrayList) [AreaPanel(id=898,minLonm=-57925866,minLatm=-30068673,width=18291,tsLFk=-2147483648,tsRFk=-2147483648,numChildren=9,numPoints=1,maxDepth=1) -5.7925866E7 -3.0068673E7
-5.7907575E7 -3.0068673E7
-5.7907575E7 -3.0050382E7
-5.7925866E7 -3.0050382E7
-5.7925866E7 -3.0068673E7

, AreaPanel(id=885,minLonm=-57889284,minLatm=-30086964,width=18291,tsLFk=-2147483648,tsRFk=-2147483648,numChildren=9,numPoints=2,maxDepth=1) -5.7889284E7 -3.0086964E7
-5.7870993E7 -3.0086964E7
-5.7870993E7 -3.0068673E7
-5.7889284E7 -3.0068673E7
-5.7889284E7 -3.0086964E7

, AreaPanel(id=886,minLonm=-57870993,minLatm=-30086964,width=18291,tsLFk=-2147483648,tsRFk=-2147483648,numChildren=9,numPoints=4,maxDepth=1) -5.7870993E7 -3.0086964E7
-5.7852702E7 -3.0086964E7
-5.7852702E7 -3.0068673E7
-5.7870993E7 -3.0068673E7
-5.7870993E7 -3.0086964E7

, AreaPanel(id=887,minLonm=-57907575,minLatm=-30068673,width=18291,tsLFk=-2147483648,tsRFk=-2147483648,numChildren=9,numPoints=1,maxDepth=1) -5.7907575E7 -3.0068673E7
-5.7889284E7 -3.0068673E7
-5.7889284E7 -3.0050382E7
-5.7907575E7 -3.0050382E7
-5.7907575E7 -3.0068673E7

, AreaPanel(id=888,minLonm=-57889284,minLatm=-30068673,width=18291,tsLFk=-2147483648,tsRFk=-2147483648,numChildren=9,numPoints=3,maxDepth=1) -5.7889284E7 -3.0068673E7
-5.7870993E7 -3.0068673E7
-5.7870993E7 -3.0050382E7
-5.7889284E7 -3.0050382E7
-5.7889284E7 -3.0068673E7

, AreaPanel(id=889,minLonm=-57870993,minLatm=-30068673,width=18291,tsLFk=-2147483648,tsRFk=-2147483648,numChildren=9,numPoints=4,maxDepth=1) -5.7870993E7 -3.0068673E7
-5.7852702E7 -3.0068673E7
-5.7852702E7 -3.0050382E7
-5.7870993E7 -3.0050382E7
-5.7870993E7 -3.0068673E7

]

When we process pm with depth 5, the areapanels for 885 changes.
257 is replaced by 259... 

917 now contains 257

The pointmapping is taking 257 from panel 884
Ah, but it's not close enough to bother with. Ah.

Why would that ever happen?

-5.7870448E7 -3.0063996E7
-5.788644E7 -3.006228E7
-5.788644E7 -3.006228E7
-5.7870448E7 -3.0063996E7
-5.7870448E7 -3.0063996E7

-5.7907575E7 -3.0086964E7
-5.7889284E7 -3.0086964E7
-5.7889284E7 -3.0068673E7
-5.7907575E7 -3.0068673E7
-5.7907575E7 -3.0086964E7


-5.79077E7 -3.0059718E7
-5.7864984E7 -3.0064504E7
-5.786500387617938E7 -3.0064681418247685E7
-5.790771987617938E7 -3.0059895418247685E7
-5.79077E7 -3.0059718E7

Ah, so the point was added to the panel when it shouldn't have been.

So we need to find out when 257 was added to 884

D/GpsTrailer( 4195): Processing gps loc row GpsLocationRow(timeMs=   20100421 02:37:08,latm=           -30061598,lonm=           -57891260,id=156)

Here is where 884 is created


-5.7907575E7 -3.0086964E7
-5.7889284E7 -3.0086964E7
-5.7889284E7 -3.0068673E7
-5.7907575E7 -3.0068673E7
-5.7907575E7 -3.0086964E7

-5.7870448E7 -3.0063996E7
-5.788644E7 -3.006228E7
-5.788644E7 -3.006228E7
-5.7870448E7 -3.0063996E7
-5.7870448E7 -3.0063996E7

Here is the parent panel when it gets created:

AreaPanel(id=874,minLonm=-57907575,minLatm=-30086964,width=54873,tsLFk=-2147483648,tsRFk=-2147483648,numChildren=1,numPoints=4,maxDepth=0) -5.7907575E7 -3.0086964E7
-5.7852702E7 -3.0086964E7
-5.7852702E7 -3.0032091E7
-5.7907575E7 -3.0032091E7
-5.7907575E7 -3.0086964E7

i=1

maxDepth is 5

how bad:

lon1	-57907575	
lat1	-30086964	
width	18291	
pd	PointDiamond  (id=830103184264)	
	fatnessMd2	0.0	
	latmHeight2	1717.0	
	lineLength	16082.91565606187	
	lonmWidth2	-15991.0	
	minLatm2	-30063996	
	minLonm2	-57870448	
	rlatm	0.0	
	rlonm	0.0	
maxDepth	0	
edgeIndex	0	



---

maxDepth is 4

PointMapping(newPoint=Point(id=341,minLatStart=-32770684.000000,minLatEnd=-32955824.000000,minLonStart=-58500844.000000,minLonEnd=-58582164.000000,numsegments=2,fatnessMd=1664.564087,depth=4)
-5.8500844E7 -3.2770684E7
-5.8582164E7 -3.2955824E7
-5.858368802997129E7 -3.2955154592457213E7
-5.850236802997129E7 -3.2770014592457213E7
-5.8500844E7 -3.2770684E7

,mpd=PointDiamond(minLatm=-32770684.000000,minLonm=-58500844.000000,latmHeight=-185140.000000,lonmWidth=-81320.000000,fatnessMd=1664.564087,depth=4)  -5.8500844E7 -3.2770684E7
-5.8582164E7 -3.2955824E7
-5.858368802997129E7 -3.2955154592457213E7
-5.850236802997129E7 -3.2770014592457213E7
-5.8500844E7 -3.2770684E7

,oldPoints=[Point(id=337,minLatStart=-32770684.000000,minLatEnd=-32850254.000000,minLonStart=-58500844.000000,minLonEnd=-58537612.000000,numsegments=1,fatnessMd=0.000000,depth=0)
-5.8500844E7 -3.2770684E7
-5.8537612E7 -3.2850254E7
-5.8537612E7 -3.2850254E7
-5.8500844E7 -3.2770684E7
-5.8500844E7 -3.2770684E7

])

Here are the parents:

E/CACHEVIEWER_AP( 4493): P940,955,-58675797,-32995233,164619,9,2,3,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P991,1041,-58547760,-32867196,18291,0,2,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P991,1042,-58529469,-32867196,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P991,1044,-58547760,-32848905,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P991,1045,-58529469,-32848905,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P943,958,-58675797,-32830614,164619,9,2,3,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P994,1047,-58547760,-32830614,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P994,1048,-58529469,-32830614,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P994,1050,-58547760,-32812323,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P994,1051,-58529469,-32812323,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P994,1053,-58547760,-32794032,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P994,1054,-58529469,-32794032,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P958,997,-58566051,-32775741,54873,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P597,920,-58511178,-33324471,493857,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P923,965,-58511178,-32830614,164619,9,4,3,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P1001,1055,-58511178,-32830614,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P1001,1058,-58511178,-32812323,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P1001,1061,-58511178,-32794032,18291,0,1,0,-2147483648,-2147483648
E/CACHEVIEWER_AP( 4493): P1004,1064,-58511178,-32775741,18291,0,2,0,-2147483648,-2147483648

maxDepth is 4

Here is the ap that contains it

AreaPanel(id=920,minLonm=-58511178,minLatm=-33324471,width=493857,tsLFk=-2147483648,tsRFk=-2147483648,numChildren=0,numPoints=1,maxDepth=0) -5.8511178E7 -3.3324471E7
-5.8017321E7 -3.3324471E7
-5.8017321E7 -3.2830614E7
-5.8511178E7 -3.2830614E7
-5.8511178E7 -3.3324471E7

-5.8511178E7 -3.3324471E7
-5.8017321E7 -3.3324471E7
-5.8017321E7 -3.2830614E7
-5.8511178E7 -3.2830614E7
-5.8511178E7 -3.3324471E7

-5.8500844E7 -3.2770684E7
-5.8537612E7 -3.2850254E7
-5.8537612E7 -3.2850254E7
-5.8500844E7 -3.2770684E7
-5.8500844E7 -3.2770684E7

-5.8500844E7 -3.2770684E7
-5.8582164E7 -3.2955824E7
-5.858368802997129E7 -3.2955154592457213E7
-5.850236802997129E7 -3.2770014592457213E7
-5.8500844E7 -3.2770684E7

Well, well, same problem... 


---

-3.2770684E7 5.479444724236366E7	
-4.2770684E7 5.479444724236366E7	

-3.2770684E7 5.458729093793411E7	
-4.2770684E7 5.458729093793411E7	

-3.2770684E7 5.413898188812287E7	
-4.2770684E7 5.413898188812287E7	

-3.2770684E7 5.434613819255242E7	
-4.2770684E7 5.434613819255242E7	


-3.2770684E7 5.428740071468907E7	
-4.2770684E7 5.428740071468907E7	

-3.2770684E7 5.4375054983221754E7	
-4.2770684E7 5.4375054983221754E7	

-3.2770684E7 5.4375054983221754E7	
-4.2770684E7 5.4375054983221754E7	

-3.2770684E7 5.428740071468907E7	
-4.2770684E7 5.428740071468907E7	

minDp	5.413898188812287E7	
maxDp	5.479444724236366E7	
minPdp	5.428740071468907E7	
maxPdp	5.4375054983221754E7	

----

Here is the monster point

Point(id=1171,minLatStart=-34619648.000000,minLatEnd=41733640.000000,minLonStart=-58382896.000000,minLonEnd=12274080.000000,numsegments=1,fatnessMd=0.000000,depth=0)

TimePanel(id=1171,START_TIME=1273251895934,END_TIME=1273494479803,LEFT_FK=-2147483648,RIGHT_FK=-2147483648)

Really long point (from BA to Italy) is 777